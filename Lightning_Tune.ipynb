{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lightning_Tune.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "WK9GeW6miiXr",
        "ASOiXXn-36Wf",
        "FbXpnU7Y4Cr9"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/souravraha/galaxy/blob/experimental/Lightning_Tune.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDzU0Yty6Qsw"
      },
      "source": [
        "# Install packages and import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMwknUIDcTW1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7d0cc05-5f4d-4fac-8beb-134f450614a9"
      },
      "source": [
        "# If you are running on Google Colab, uncomment below to install the necessary dependencies \n",
        "# before beginning the exercise.\n",
        "\n",
        "print('Setting up colab environment')\n",
        "!pip uninstall -y -q pyarrow\n",
        "!pip install -q ray[debug] lightning-bolts\n",
        "!pip install -U -q ray[tune]\n",
        "# !pip install cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.8-cp37-cp37m-linux_x86_64.whl\n",
        "\n",
        "# # A hack to force the runtime to restart, needed to include the above dependencies.\n",
        "# print('Done installing! Restarting via forced crash (this is not an issue).')\n",
        "# import os\n",
        "# os._exit(0)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting up colab environment\n",
            "\u001b[K     |████████████████████████████████| 51.6MB 61kB/s \n",
            "\u001b[33m  WARNING: ray 1.4.1 does not provide the extra 'debug'\u001b[0m\n",
            "\u001b[K     |████████████████████████████████| 256kB 42.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 81kB 10.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.1MB 41.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 133kB 64.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 10.1MB 27.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 10.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.3MB 35.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 81kB 12.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 819kB 26.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 235kB 52.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 92kB 12.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 296kB 52.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 143kB 54.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 10.6MB 31.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 122kB 58.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 829kB 30.0MB/s \n",
            "\u001b[?25h  Building wheel for gpustat (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: tensorflow 2.5.0 has requirement tensorboard~=2.5, but you'll have tensorboard 2.4.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: pytorch-lightning 1.3.8 has requirement PyYAML<=5.4.1,>=5.1, but you'll have pyyaml 3.13 which is incompatible.\u001b[0m\n",
            "\u001b[K     |████████████████████████████████| 133kB 4.0MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNMd9sh02fG9"
      },
      "source": [
        "# If you are running on Google Colab, please install TensorFlow 2.0 by uncommenting below..\n",
        "\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cR6G_K6bWVqd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea78c173-1168-44e9-cb1b-c426a086b93b"
      },
      "source": [
        "# __import_lightning_begin__\n",
        "import math\n",
        "# import shutil\n",
        "import numpy as np              #\n",
        "from matplotlib import pyplot as plt\n",
        "from itertools import cycle\n",
        "import torch\n",
        "from torch import nn\n",
        "import pytorch_lightning as pl\n",
        "# from filelock import FileLock\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torch.nn import functional as F\n",
        "import torchvision.datasets as datasets\n",
        "from torchvision.models import resnet18\n",
        "from pl_bolts.models.self_supervised.resnets import BasicBlock                  # problem with resnet18\n",
        "from pl_bolts.models.gans import DCGAN\n",
        "from pl_bolts.models.gans.dcgan.components import DCGANDiscriminator, DCGANGenerator\n",
        "import torchmetrics as tm\n",
        "from torchvision import transforms\n",
        "import os\n",
        "from os.path import basename\n",
        "# __import_lightning_end__\n",
        "\n",
        "# __import_tune_begin__\n",
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "from pytorch_lightning.utilities.cloud_io import load as pl_load\n",
        "from ray import tune\n",
        "from ray.tune import CLIReporter, JupyterNotebookReporter\n",
        "from ray.tune.schedulers import ASHAScheduler, PopulationBasedTraining\n",
        "from ray.tune.integration.pytorch_lightning import TuneReportCallback, \\\n",
        "    TuneReportCheckpointCallback\n",
        "# __import_tune_end__"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ray/autoscaler/_private/cli_logger.py:61: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
            "  \"update your install command.\", FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WK9GeW6miiXr"
      },
      "source": [
        "# Download and extract data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGZRwWHbXqLu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df9ae5c5-48f0-47e8-d999-8a239dffbc9c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZUIrgOfbwPe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1a9f368-caaf-4e0a-cc6d-24a4ac57364d"
      },
      "source": [
        "# 'a': 1Cjcw2EWorhdhJSGoWOdxsEUDxvl943dt, 'b': 15yXXC4h5VsytP3Ak1jfUSjQhdgP2s23K, 'c': 1vuQ-pLzoKT4Hd_V7949r9eND9E2fB_u_,\n",
        "# 'd': , 'e': 1wFuasvb7PthxXtMUlsD13uzYHWlWt06H, 'f': 17l6H61tLAu26zGuei38r_T5ssjbYUeaJ, \n",
        "# 'g': 1SxQVosWeEjY3Pyn8LRXA11rLnZ9HK_7B, 'h': 1Atau0RH4oyLAiYReW-G9a8l9pUNltglF, 'i': 15lEgsR1p00KSHieaT9a1nkbJ86pDxwgp, \n",
        "# 'j': 1m0EQUbqZZeyl76XsQIKWU5Qd7jGmmWhB, 'k': , 'l': 1meTDi4aeWfdChOiXeLtUOGhjVDVu000e\n",
        "\n",
        "# !rm -rf images\n",
        "!gdown --id 17l6H61tLAu26zGuei38r_T5ssjbYUeaJ\n",
        "!tar zxf ./model_f.tgz\n",
        "\n",
        "# def prepare_data(data_dir: str = '/content'):\n",
        "#     gdown.download('https://drive.google.com/uc?id=17l6H61tLAu26zGuei38r_T5ssjbYUeaJ', data_dir+'/model_f.tgz', quiet=True)\n",
        "    \n",
        "#     temp = tarfile.open(data_dir+'/model_f.tgz', 'r|gz')\n",
        "#     temp.extractall()\n",
        "#     temp.close()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=17l6H61tLAu26zGuei38r_T5ssjbYUeaJ\n",
            "To: /content/model_f.tgz\n",
            "2.34GB [00:56, 41.3MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Snbv_zoNiWfW"
      },
      "source": [
        "# DataModule\n",
        "This creates dataloaders which need to be supplied to train, validate or test the module we have."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yItuGxXmXzGr"
      },
      "source": [
        "class NpyDataModule(pl.LightningDataModule):\n",
        "\n",
        "    def __init__(self, config, img_width: int = 150, data_dir: str = '/content/images/'):\n",
        "        super().__init__()\n",
        "        # This method is not implemented\n",
        "        # self.save_hyperparameters()\n",
        "        self.batch_size = config['batch_size']\n",
        "        self.data_dir = os.path.expanduser(data_dir)\n",
        "        \n",
        "        GLOBAL = np.load('/content/drive/MyDrive/git_repos/forging_new_worlds/GLOBAL_VALS_F.npz')\n",
        "        self.transform = transforms.Compose([\n",
        "            # transforms.ConvertImageDtype(torch.float32),\n",
        "            # Can't use this, divides values by dtype.max, use float() in npyloader instead\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.RandomVerticalFlip(),\n",
        "            transforms.Normalize(mean=(GLOBAL['VALS'][0],), std=(GLOBAL['VALS'][1],)),\n",
        "            # this shift-scales the pixel values, N(mu, sigma) -> N(0, 1)\n",
        "            transforms.Resize(img_width, transforms.InterpolationMode.NEAREST),\n",
        "        ])\n",
        "    \n",
        "    @staticmethod\n",
        "    def npy_loader(path):\n",
        "        # s=np.load(path).astype('float',copy=False)\n",
        "        return torch.from_numpy(np.load(path)).unsqueeze(0).float()\n",
        "        # Convert to tenssor first, and then to float, otherwise final dtype \n",
        "        # would be float64, which would raise errors in conv layers      ###### type as\n",
        "\n",
        "    def setup(self, stage: str = None):\n",
        "        if stage in ('fit', None):\n",
        "            self.full_set = datasets.DatasetFolder(os.path.join(self.data_dir,'train'),\n",
        "                                                   self.npy_loader, \n",
        "                                                   ('.npy'), \n",
        "                                                   self.transform,\n",
        "                                                   )\n",
        "            self.train_set, self.val_set = random_split(self.full_set, [60000, 15000])            \n",
        "            # self.val_set = datasets.DatasetFolder(os.path.join(self.data_dir,'val'), \n",
        "            #                                        self.npy_loader, \n",
        "            #                                        ('.npy'), \n",
        "            #                                        self.transform,\n",
        "            #                                        )\n",
        "            self.dims = tuple(self.train_set[0][0].shape)\n",
        "\n",
        "        if stage in ('test', None):\n",
        "            self.test_set = datasets.DatasetFolder(os.path.join(self.data_dir,'val'), \n",
        "                                                   self.npy_loader, \n",
        "                                                   ('.npy'), \n",
        "                                                   self.transform,\n",
        "                                                   )\n",
        "            self.dims = getattr(self, 'dims', self.test_set[0][0].shape)\n",
        "    \n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(self.train_set, self.batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(self.val_set, self.batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(self.test_set, self.batch_size, shuffle=True, num_workers=2)\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0bm1afc11hN"
      },
      "source": [
        "# ResNet:\n",
        "We modify a ResNet slightly for our purpose."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ojZ0yT4z168p"
      },
      "source": [
        "class LensResnet(pl.LightningModule):\n",
        "\n",
        "    def __init__(self, config, image_channels: int = 1, num_classes: int = 3, **kwargs):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters(ignore=config)\n",
        "        self.learning_rate = config['learning_rate']\n",
        "\n",
        "        # init a pretrained resnet\n",
        "        self.backbone = resnet18(num_classes = self.hparams.num_classes)\n",
        "        self.backbone.conv1 = nn.Conv2d(self.hparams.image_channels, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
        "        #  can't merely change the in_channels since weights have to changed as well\n",
        "        self.backbone.fc = nn.Sequential(\n",
        "            nn.Dropout(0.5),\n",
        "            self.backbone.fc\n",
        "        )\n",
        "        # self.backbone.\n",
        "        # metrics = tm.MetricCollection([\n",
        "        #     # tm.AUROC(self.hparams.num_classes, average='weighted'),\n",
        "        #     # tm.ROC(self.hparams.num_classes),\n",
        "        # #     tm.PrecisionRecallCurve(self.hparams.num_classes),\n",
        "        # ])\n",
        "        # self.train_metrics = metrics.clone(prefix='ResNet/train/')\n",
        "        # self.val_metrics = metrics.clone(prefix='ResNet/val/')\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.Adam(self.backbone.parameters(), self.learning_rate)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return F.softmax(self.backbone(x), 1)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        imgs, labels = batch\n",
        "        self.log('ResNet/train/auroc', tm.functional.auroc(self(imgs),labels, average='weighted', num_classes=self.hparams.num_classes))\n",
        "        loss = F.cross_entropy(self.backbone(imgs), labels)\n",
        "        self.log('ResNet/train/loss', loss)\n",
        "        #  keep only scalars here, for no errors\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        imgs, labels = batch\n",
        "        self.log('ResNet/val/loss', F.cross_entropy(self.backbone(imgs), labels))\n",
        "        #  keep only scalars here, for no errors\n",
        "        return {'pred': self(imgs), 'target': labels}\n",
        "\n",
        "    def validation_epoch_end(self, Listofdicts):\n",
        "        prediction, target = torch.cat([x['pred'] for x in Listofdicts]), torch.cat([x['target'] for x in Listofdicts])\n",
        "        aurocTensor = tm.functional.auroc(prediction, target, num_classes=self.hparams.num_classes, average=None)\n",
        "        self.log('ResNet/val/auroc', aurocTensor.min())\n",
        "        fprList, tprList, _ = tm.functional.roc(prediction, target, num_classes=self.hparams.num_classes)\n",
        "        \n",
        "        f = plt.figure()\n",
        "        colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n",
        "        for i, color in zip(range(self.hparams.num_classes), colors):\n",
        "            plt.plot(fprList[i].cpu(), tprList[i].cpu(), color=color,\n",
        "                    label='ROC curve of class {0} (area = {1:0.2f})'\n",
        "                    ''.format(i, aurocTensor[i].cpu()))\n",
        "        plt.plot([0, 1], [0, 1], 'k--')\n",
        "        plt.xlim([0.0, 1.0])\n",
        "        plt.ylim([0.0, 1.05])\n",
        "        plt.xlabel('False Positive Rate')\n",
        "        plt.ylabel('True Positive Rate')\n",
        "        plt.title('Multi-class ROC')\n",
        "        plt.legend(loc='lower right')\n",
        "\n",
        "        self.logger.experiment.add_figure('ResNet/val/ROC', f)\n",
        "        f.savefig(str(tune.get_trial_dir())+'ROC_epoch_'+str(self.current_epoch)+'.pdf')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASOiXXn-36Wf"
      },
      "source": [
        "Trying out Auto Tuning of learning rate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FtDpDdAADLi"
      },
      "source": [
        "# Can't work with multiple optimizers\n",
        "config = {\n",
        "    'learning_rate': 1e-4, 'batch_size': 128, 'feature_maps': 64,\n",
        "}\n",
        "dm = NpyDataModule(config)\n",
        "generator = StackGAN(config)\n",
        "\n",
        "trainer = pl.Trainer(\n",
        "    # logger=,\n",
        "    # checkpoint_callback=,\n",
        "    default_root_dir='./drive/MyDrive/Logs/', \n",
        "    gpus=1,\n",
        "    auto_select_gpus=True, \n",
        "    # tpu_cores=\n",
        "    progress_bar_refresh_rate=1,\n",
        "    # fast_dev_run=,\n",
        "    max_epochs=5,\n",
        "    # max_time=,\n",
        "    # limit_train_batches=,\n",
        "    # flush_logs_every_n_steps=,\n",
        "    # log_every_n_steps=,\n",
        "    # resume_from_checkpoint='./drive/MyDrive/Logs/lr_find_temp_model.ckpt',\n",
        "    auto_lr_find = True,\n",
        "    # auto_scale_batch_size=True,\n",
        "    # prepare_data_per_node=,\n",
        "    )\n",
        "\n",
        "# Run learning rate finder\n",
        "lr_finder = trainer.tuner.lr_find(generator, dm)\n",
        "\n",
        "# # Results can be found in\n",
        "# # lr_finder.results\n",
        "\n",
        "# Plot with\n",
        "fig = lr_finder.plot(suggest=True)\n",
        "fig.show()\n",
        "\n",
        "# Pick point based on plot, or get suggestion\n",
        "new_lr = lr_finder.suggestion()\n",
        "\n",
        "# # update hparams of the model\n",
        "# model.hparams.lr = new_lr\n",
        "\n",
        "# # Fit model\n",
        "# trainer.fit(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbXpnU7Y4Cr9"
      },
      "source": [
        "# Tune ResNet hyperparameters:\n",
        "Here we tune hyperparameters as we train our modified ResNet."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33OXrLjWn0II"
      },
      "source": [
        "# __tune_train_checkpoint_begin\n",
        "def train_LensResnet_tune_checkpoint(config,\n",
        "                                    checkpoint_dir=None,\n",
        "                                    num_epochs=10,\n",
        "                                    num_gpus=1):\n",
        "    data_dir = os.path.expanduser('/content/images/')\n",
        "\n",
        "    trainer = pl.Trainer(\n",
        "        max_epochs=num_epochs,\n",
        "        prepare_data_per_node = False,\n",
        "        num_sanity_val_steps=0,\n",
        "        # If fractional GPUs passed in, convert to int.\n",
        "        gpus=math.ceil(num_gpus),\n",
        "        # tpu_cores = 8,\n",
        "        logger=TensorBoardLogger(\n",
        "            save_dir=tune.get_trial_dir(), name='', version='.'),\n",
        "        # progress_bar_refresh_rate=1,\n",
        "        callbacks=[\n",
        "            TuneReportCheckpointCallback(\n",
        "                metrics={\n",
        "                    'loss': 'ResNet/val/loss',\n",
        "                    'auroc': 'ResNet/val/auroc',\n",
        "                },\n",
        "                filename='checkpoint',\n",
        "                # on='validation_end'\n",
        "            )\n",
        "        ],\n",
        "        stochastic_weight_avg=True,\n",
        "    )\n",
        "\n",
        "    dm = NpyDataModule(config, data_dir)\n",
        "    \n",
        "    if checkpoint_dir:\n",
        "        # Currently, this leads to errors:\n",
        "        # model = LensResnet.load_from_checkpoint(\n",
        "        #     os.path.join(checkpoint, 'checkpoint'))\n",
        "        # Workaround:\n",
        "        ckpt = pl_load(\n",
        "            os.path.join(checkpoint_dir, 'checkpoint'),\n",
        "            map_location=lambda storage, loc: storage)\n",
        "        model = LensResnet._load_model_state(\n",
        "            ckpt, config=config, \n",
        "            # data_dir=data_dir\n",
        "            )\n",
        "        trainer.current_epoch = ckpt['epoch']\n",
        "    else:\n",
        "        model = LensResnet(config, \n",
        "                        #  data_dir\n",
        "                         )\n",
        "\n",
        "    trainer.fit(model, dm)\n",
        "\n",
        "# __tune_train_checkpoint_end__\n",
        "\n",
        "\n",
        "# __tune_asha_begin__\n",
        "def tune_LensResnet_asha(num_samples=10, num_epochs=10, gpus_per_trial=1):\n",
        "    # config = {\n",
        "    #     'learning_rate': tune.choice([1e-5, 1e-4, 1e-3, 1e-2]),\n",
        "    #     'batch_size': tune.choice([128, 64, 32]),\n",
        "    # }\n",
        "\n",
        "    best = {'batch_size': 128, 'learning_rate': 0.0001}    \n",
        "\n",
        "    scheduler = ASHAScheduler(\n",
        "        max_t=num_epochs,\n",
        "        grace_period=1,\n",
        "        reduction_factor=2)\n",
        "\n",
        "    reporter = CLIReporter(\n",
        "        # overwrite=True,\n",
        "        parameter_columns=['learning_rate', 'batch_size'],\n",
        "        metric_columns=['loss', 'auroc', 'training_iteration'])\n",
        "\n",
        "    analysis = tune.run(\n",
        "        tune.with_parameters(\n",
        "            train_LensResnet_tune_checkpoint,\n",
        "            num_epochs=num_epochs,\n",
        "            num_gpus=gpus_per_trial),\n",
        "        name='LensResNet_J',                                                    # Change with dataset change\n",
        "        metric='auroc',\n",
        "        mode='max',\n",
        "        config=best,\n",
        "        resources_per_trial={\n",
        "            'cpu': 2,\n",
        "            'gpu': gpus_per_trial,\n",
        "            # 'tpu': 8,\n",
        "        },\n",
        "        # num_samples=num_samples,\n",
        "        local_dir='./drive/MyDrive/Logs',\n",
        "        scheduler=scheduler,\n",
        "        progress_reporter=reporter,\n",
        "        fail_fast = True,\n",
        "        restore = '/content/drive/MyDrive/Logs/LensResNet_J/train_LensResnet_tune_checkpoint_c74d9_00003_3_batch_size=128,learning_rate=0.0001_2021-07-08_01-03-43/checkpoint_epoch=2-step=1406',\n",
        "        # '/content/drive/MyDrive/Logs/tune_LensResnet_asha_model_f/train_LensResnet_tune_checkpoint_e32ba_00000_0_batch_size=64,learning_rate=0.0001_2021-07-06_03-33-10/checkpoint_epoch=14-step=4689',\n",
        "        # resume=True,\n",
        "        )\n",
        "\n",
        "    print('Best hyperparameters found were: ', analysis.best_config)\n",
        "# __tune_asha_end__\n",
        "\n",
        "\n",
        "# __tune_pbt_begin__\n",
        "def tune_LensResnet_pbt(num_samples=10, num_epochs=10, gpus_per_trial=1):\n",
        "    config = {\n",
        "        'learning_rate': 1e-3,\n",
        "        'batch_size': 64,\n",
        "    }\n",
        "\n",
        "    scheduler = PopulationBasedTraining(\n",
        "        perturbation_interval=4,\n",
        "        hyperparam_mutations={\n",
        "            'learning_rate': [1e-5, 1e-4, 1e-3, 1e-2],\n",
        "            'batch_size': [32, 64, 128]\n",
        "        })\n",
        "\n",
        "    reporter = CLIReporter(\n",
        "        # overwrite=True,\n",
        "        parameter_columns=['learning_rate', 'batch_size'],\n",
        "        metric_columns=['loss', 'auroc', 'training_iteration'])\n",
        "\n",
        "    analysis = tune.run(\n",
        "        # resume=True,\n",
        "        tune.with_parameters(\n",
        "            train_LensResnet_tune_checkpoint,\n",
        "            num_epochs=num_epochs,\n",
        "            num_gpus=gpus_per_trial),\n",
        "        metric='auroc',\n",
        "        mode='max',\n",
        "        resources_per_trial={\n",
        "            'cpu': 2,\n",
        "            'gpu': gpus_per_trial,\n",
        "            # 'tpu': 8,\n",
        "        },\n",
        "        fail_fast = True,\n",
        "        config=config,\n",
        "        num_samples=num_samples,\n",
        "        scheduler=scheduler,\n",
        "        progress_reporter=reporter,\n",
        "        local_dir='./drive/MyDrive/Logs' ,\n",
        "        name='tune_LensResnet_pbt')\n",
        "\n",
        "    print('Best hyperparameters found were: ', analysis.best_config)\n",
        "\n",
        "# __tune_pbt_end__\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    import argparse\n",
        "\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\n",
        "        '--smoke-test', action='store_true', help='Finish quickly for testing')\n",
        "    args, _ = parser.parse_known_args()\n",
        "\n",
        "    if args.smoke_test:\n",
        "        tune_LensResnet_asha(num_samples=1, num_epochs=6, gpus_per_trial=1)\n",
        "        tune_LensResnet_pbt(num_samples=1, num_epochs=6, gpus_per_trial=1)\n",
        "    else:\n",
        "        # ASHA scheduler\n",
        "        tune_LensResnet_asha(num_samples=12, num_epochs=20, gpus_per_trial=1)\n",
        "        # Population based training\n",
        "        # tune_LensResnet_pbt(num_samples=10, num_epochs=10, gpus_per_trial=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zc0oRARcKBv3"
      },
      "source": [
        "# Experimental"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQNQT3Fr_LND"
      },
      "source": [
        "Check if we can load checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wKxksXYFFXC5"
      },
      "source": [
        "def pretrained_LensResNets():\n",
        "    ckptf = pl_load(os.path.join(\n",
        "        '/content/drive/MyDrive/Logs/LensResNet_F/train_LensResnet_tune_checkpoint_efb38_00000_0_2021-07-09_04-20-04/checkpoint_epoch=9-step=16407',\n",
        "        'checkpoint'), map_location=lambda storage, loc: storage)\n",
        "    f = LensResnet._load_model_state(ckptf, config={'batch_size': 32, 'learning_rate': 0.0001})\n",
        "    ckptj = pl_load(os.path.join(\n",
        "        '/content/drive/MyDrive/Logs/LensResNet_J/train_LensResnet_tune_checkpoint_21355_00000_0_2021-07-09_16-17-17/checkpoint_epoch=27-step=4687',\n",
        "        'checkpoint'), map_location=lambda storage, loc: storage)\n",
        "    j = LensResnet._load_model_state(ckptj, config={'batch_size': 128, 'learning_rate': 0.0001})\n",
        "    return f, j\n",
        "\n",
        "def post_plotting(ax):\n",
        "    ax.plot([0, 1], [0, 1], 'k--')\n",
        "    ax.legend(loc='lower right')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kclAeom914wK"
      },
      "source": [
        "class Stage1(DCGAN):\n",
        "    def __init__(self, config, num_classes: int = 3, **kwargs):\n",
        "        super().__init__(feature_maps_gen=config['ngf'], feature_maps_disc=config['ndf'], learning_rate=config['learning_rate'])\n",
        "        self.save_hyperparameters(ignore=config)\n",
        "\n",
        "        self.generator.add_module('emb', nn.Embedding(self.hparams.num_classes, self.hparams.latent_dim))\n",
        "\n",
        "        self.modelF, self.modelJ = pretrained_LensResNets()\n",
        "\n",
        "    def forward(self, noise, labels = None):\n",
        "        if labels is None:\n",
        "            labels = getattr(self, 'labels', \n",
        "                             torch.randint(self.hparams.num_classes, noise.shape[:-1], device=self.device))  # last dimension is the hidden dimension\n",
        "        inp = noise.mul(self.generator.emb(labels))\n",
        "        return self.generator(inp.view(-1, inp.shape[-1], 1, 1))\n",
        "\n",
        "    def training_step(self, batch, batch_idx, optimizer_idx):\n",
        "        real, self.labels = batch\n",
        "\n",
        "        # Train discriminator\n",
        "        result = None\n",
        "        if optimizer_idx == 0:\n",
        "            result = self._disc_step(real)\n",
        "\n",
        "        # Train generator\n",
        "        if optimizer_idx == 1:\n",
        "            result = self._gen_step(real)\n",
        "\n",
        "        return result\n",
        "\n",
        "    def _disc_step(self, real: torch.Tensor) -> torch.Tensor:\n",
        "        disc_loss = self._get_disc_loss(real)\n",
        "        self.log('Stage1/D/train/loss', disc_loss, on_epoch=True)\n",
        "        return disc_loss\n",
        "\n",
        "    def _gen_step(self, real: torch.Tensor) -> torch.Tensor:\n",
        "        gen_loss = self._get_gen_loss(real)\n",
        "        self.log('Stage1/G/train/loss', gen_loss, on_epoch=True)\n",
        "        return gen_loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        imgs, labels = batch\n",
        "        out_64 = self(torch.randn(labels.shape[0], self.hparams.latent_dim).type_as(imgs), labels)\n",
        "        out = F.interpolate(out_64, 150)\n",
        "        return {'predF': self.modelF(out), 'predJ': self.modelJ(out), 'target': labels}\n",
        "\n",
        "    def validation_epoch_end(self, listofDicts):\n",
        "        target = torch.cat([x['target'] for x in listofDicts])\n",
        "        f, ax = plt.subplots(1,2, subplot_kw={'xlim': [0,1], 'xlabel': 'False Positive Rate', \n",
        "                                              'ylim': [0,1.05], 'ylabel': 'True Positive Rate'},\n",
        "                             figsize=[11, 5])\n",
        "        letters = ['F', 'J']\n",
        "        for l in range(2):\n",
        "            prediction = torch.cat([x['pred' + str(letters[l])] for x in listofDicts])\n",
        "            aurocTensor = tm.functional.auroc(prediction, target, num_classes=self.hparams.num_classes, average=None)\n",
        "            self.log('Stage1/ResNet(' + str(letters[l]) + ')/val/auroc', aurocTensor.min())\n",
        "            fprList, tprList, _ = tm.functional.roc(prediction, target, num_classes=self.hparams.num_classes)\n",
        "            \n",
        "            colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n",
        "            for i, color in zip(range(self.hparams.num_classes), colors):\n",
        "                ax[l].plot(fprList[i].cpu(), tprList[i].cpu(), color=color,\n",
        "                        label='ROC curve of class {0} (area = {1:0.2f})'\n",
        "                        ''.format(i, aurocTensor[i].cpu()))\n",
        "            post_plotting(ax[l])\n",
        "            ax[l].set_title('Multi-class ROC (' + str(letters[l]) + ')')\n",
        "        \n",
        "        f.tight_layout()\n",
        "        self.logger.experiment.add_figure('Stage1/ResNet/val/ROC', f)\n",
        "        f.savefig(str(tune.get_trial_dir()) + 'ROC_epoch_' + str(self.current_epoch) + '.pdf')"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dTigZtbCrI0",
        "outputId": "35a79ba9-5048-4914-863f-be81be7e663d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        }
      },
      "source": [
        "dummy = Stage1.load_from_checkpoint(os.path.join('/content/drive/MyDrive/Logs/Stage1_pbt_F/train_Stage1_tune_checkpoint_228f8_00001_1_ndf=128,ngf=128_2021-07-11_05-30-43/checkpoint_epoch=0-step=937', 'checkpoint'))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-a03aba31db22>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdummy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStage1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_from_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/Logs/Stage1_pbt_F/train_Stage1_tune_checkpoint_228f8_00001_1_ndf=128,ngf=128_2021-07-11_05-30-43/checkpoint_epoch=0-step=937'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'checkpoint'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/core/saving.py\u001b[0m in \u001b[0;36mload_from_checkpoint\u001b[0;34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCHECKPOINT_HYPER_PARAMS_KEY\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_model_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/core/saving.py\u001b[0m in \u001b[0;36m_load_model_state\u001b[0;34m(cls, checkpoint, strict, **cls_kwargs_new)\u001b[0m\n\u001b[1;32m    197\u001b[0m             \u001b[0m_cls_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_cls_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcls_init_args_name\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0m_cls_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0;31m# give model a chance to load something\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-980f4f44cd29>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config, num_classes, **kwargs)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mStage1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDCGAN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_maps_gen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ngf'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_maps_disc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ndf'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'learning_rate'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_hyperparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mignore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: __init__() got multiple values for keyword argument 'feature_maps_gen'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lB-mN-v4lAb6"
      },
      "source": [
        "del dummy"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPxU2-AggCrI"
      },
      "source": [
        "Let's train Stage1 GAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3OOr5f_bgLoM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0d471660-bc8b-44b0-c3c3-bed7af3de1dc"
      },
      "source": [
        "# __tune_train_checkpoint_begin\n",
        "def train_Stage1_tune_checkpoint(config, checkpoint_dir=None, num_epochs=10, num_gpus=1):\n",
        "    # data_dir = os.path.expanduser('/content/images/')\n",
        "    trainer = pl.Trainer(\n",
        "        # accumulate_grad_batches=2,\n",
        "        # limit_train_batches=0.20,\n",
        "        # limit_val_batches=0.20,\n",
        "        # num_sanity_val_steps=-1,\n",
        "        max_epochs=num_epochs,\n",
        "        prepare_data_per_node = False,\n",
        "        # If fractional GPUs passed in, convert to int.\n",
        "        gpus=math.ceil(num_gpus),\n",
        "        # tpu_cores = 8,\n",
        "        logger=TensorBoardLogger(save_dir=tune.get_trial_dir(), name='', version='.'),\n",
        "        # progress_bar_refresh_rate=1,\n",
        "        callbacks=[\n",
        "                   TuneReportCheckpointCallback(\n",
        "                       {'lossG': 'Stage1/G/train/loss', \n",
        "                        'lossD': 'Stage1/D/train/loss', \n",
        "                        'auroc': 'Stage1/ResNet(F)/val/auroc', \n",
        "                        'auroc_cross': 'Stage1/ResNet(J)/val/auroc',\n",
        "                        },\n",
        "                   ),\n",
        "        ],\n",
        "        # stochastic_weight_avg=True,\n",
        "        # works with only one optimizer\n",
        "        # benchmark=True,\n",
        "    )\n",
        "    dm = NpyDataModule(config, 64)                                              # Specify image width here\n",
        "    if checkpoint_dir:\n",
        "        # Currently, this leads to errors:\n",
        "        # model = Stage1.load_from_checkpoint(\n",
        "        #     os.path.join(checkpoint, 'checkpoint'))\n",
        "        # Workaround:\n",
        "        ckpt = pl_load(os.path.join(checkpoint_dir, 'checkpoint'),\n",
        "                       map_location=lambda storage, loc: storage)\n",
        "        model = Stage1._load_model_state(ckpt, config=config)\n",
        "        trainer.current_epoch = ckpt['epoch']\n",
        "    else:\n",
        "        model = Stage1(config)\n",
        "\n",
        "    trainer.fit(model, dm)\n",
        "# __tune_train_checkpoint_end__\n",
        "\n",
        "\n",
        "# __tune_asha_begin__\n",
        "def tune_Stage1_asha(num_samples=10, num_epochs=10, gpus_per_trial=1):\n",
        "    analysis = tune.run(\n",
        "        tune.with_parameters(train_Stage1_tune_checkpoint,\n",
        "                             num_epochs=num_epochs,\n",
        "                             num_gpus=gpus_per_trial),\n",
        "        name='Stage1_F',\n",
        "        metric='auroc',\n",
        "        mode='max',\n",
        "        config={'learning_rate': tune.choice([1e-5, 1e-4, 1e-3]),\n",
        "                'ngf': tune.choice([128, 64, 32]),\n",
        "                'ndf': tune.choice([128, 64, 32]),\n",
        "                'batch_size': tune.choice([128, 64, 32]),\n",
        "                },\n",
        "        resources_per_trial={'cpu': 2,\n",
        "                             'gpu': gpus_per_trial,\n",
        "                             },\n",
        "        local_dir='./drive/MyDrive/Logs/',\n",
        "        scheduler = ASHAScheduler(max_t=num_epochs, grace_period=1,  reduction_factor=2),\n",
        "        progress_reporter=JupyterNotebookReporter(\n",
        "            overwrite=True,\n",
        "            parameter_columns=['learning_rate', 'ngf', 'ndf', 'batch_size'],\n",
        "            metric_columns=['lossG', 'lossD', 'auroc', 'auroc_cross', 'training_iteration'],\n",
        "            ),\n",
        "        fail_fast = True,\n",
        "        # num_samples=num_samples,\n",
        "        resume='PROMPT',\n",
        "        )\n",
        "\n",
        "    print('Best hyperparameters found were: ', analysis.best_config)\n",
        "\n",
        "# __tune_asha_end__\n",
        "\n",
        "\n",
        "# __tune_pbt_begin__\n",
        "def tune_Stage1_pbt(num_samples=10, num_epochs=10, gpus_per_trial=1):\n",
        "    analysis = tune.run(\n",
        "        tune.with_parameters(train_Stage1_tune_checkpoint,\n",
        "                             num_epochs=num_epochs,\n",
        "                             num_gpus=gpus_per_trial),\n",
        "        name='Stage1_pbt_F',\n",
        "        metric='auroc',\n",
        "        mode='max',\n",
        "        config={'learning_rate': 1e-4,\n",
        "                'ngf': tune.choice([128, 64, 32]),\n",
        "                'ndf': tune.choice([128, 64, 32]),\n",
        "                'batch_size': 64,\n",
        "                },\n",
        "        resources_per_trial={'cpu': 2,\n",
        "                             'gpu': gpus_per_trial,\n",
        "                             },\n",
        "        local_dir='./drive/MyDrive/Logs',\n",
        "        scheduler = PopulationBasedTraining(perturbation_interval=1,\n",
        "                                        hyperparam_mutations={\n",
        "                                            'learning_rate': tune.loguniform(1e-6, 1e-2),\n",
        "                                            'batch_size': [128, 64, 32],\n",
        "                                            },\n",
        "                                        ),\n",
        "        progress_reporter=JupyterNotebookReporter(\n",
        "            overwrite=True,\n",
        "            parameter_columns=['learning_rate', 'ngf', 'ndf', 'batch_size'],\n",
        "            metric_columns=['lossG', 'lossD', 'auroc', 'auroc_cross', 'training_iteration'],\n",
        "            ),\n",
        "        fail_fast = True,\n",
        "        num_samples=num_samples,\n",
        "        # resume='PROMPT',\n",
        "        )\n",
        "\n",
        "    print('Best hyperparameters found were: ', analysis.best_config)\n",
        "\n",
        "# __tune_pbt_end__\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    import argparse\n",
        "\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\n",
        "        '--smoke-test', action='store_true', help='Finish quickly for testing')\n",
        "    args, _ = parser.parse_known_args()\n",
        "\n",
        "    if args.smoke_test:\n",
        "        tune_Stage1_asha(num_samples=1, num_epochs=6, gpus_per_trial=1)\n",
        "        tune_Stage1_pbt(num_samples=1, num_epochs=6, gpus_per_trial=1)\n",
        "    else:\n",
        "        # ASHA scheduler\n",
        "        # tune_Stage1_asha(num_samples=12, num_epochs=5, gpus_per_trial=1)\n",
        "        # Population based training\n",
        "        tune_Stage1_pbt(num_samples=10, num_epochs=10, gpus_per_trial=1)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "== Status ==\n",
            "Memory usage on this node: 1.7/12.7 GiB\n",
            "PopulationBasedTraining: 0 checkpoints, 0 perturbs\n",
            "Resources requested: 0/2 CPUs, 0/1 GPUs, 0.0/7.51 GiB heap, 0.0/3.75 GiB objects (0.0/1.0 accelerator_type:T4)\n",
            "Result logdir: /content/drive/MyDrive/Logs/Stage1_pbt_F\n",
            "Number of trials: 10/10 (10 PENDING)\n",
            "+------------------------------------------+----------+-------+-----------------+-------+-------+--------------+\n",
            "| Trial name                               | status   | loc   |   learning_rate |   ngf |   ndf |   batch_size |\n",
            "|------------------------------------------+----------+-------+-----------------+-------+-------+--------------|\n",
            "| train_Stage1_tune_checkpoint_228f8_00000 | PENDING  |       |          0.0001 |   128 |    32 |           64 |\n",
            "| train_Stage1_tune_checkpoint_228f8_00001 | PENDING  |       |          0.0001 |   128 |   128 |           64 |\n",
            "| train_Stage1_tune_checkpoint_228f8_00002 | PENDING  |       |          0.0001 |    32 |    64 |           64 |\n",
            "| train_Stage1_tune_checkpoint_228f8_00003 | PENDING  |       |          0.0001 |    32 |    64 |           64 |\n",
            "| train_Stage1_tune_checkpoint_228f8_00004 | PENDING  |       |          0.0001 |    32 |    64 |           64 |\n",
            "| train_Stage1_tune_checkpoint_228f8_00005 | PENDING  |       |          0.0001 |   128 |    64 |           64 |\n",
            "| train_Stage1_tune_checkpoint_228f8_00006 | PENDING  |       |          0.0001 |    32 |    64 |           64 |\n",
            "| train_Stage1_tune_checkpoint_228f8_00007 | PENDING  |       |          0.0001 |   128 |    64 |           64 |\n",
            "| train_Stage1_tune_checkpoint_228f8_00008 | PENDING  |       |          0.0001 |    32 |    64 |           64 |\n",
            "| train_Stage1_tune_checkpoint_228f8_00009 | PENDING  |       |          0.0001 |   128 |    32 |           64 |\n",
            "+------------------------------------------+----------+-------+-----------------+-------+-------+--------------+\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[33m(raylet)\u001b[0m /usr/local/lib/python3.7/dist-packages/ray/autoscaler/_private/cli_logger.py:61: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
            "\u001b[2m\u001b[33m(raylet)\u001b[0m   \"update your install command.\", FutureWarning)\n",
            "\u001b[2m\u001b[33m(raylet)\u001b[0m /usr/local/lib/python3.7/dist-packages/ray/autoscaler/_private/cli_logger.py:61: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
            "\u001b[2m\u001b[33m(raylet)\u001b[0m   \"update your install command.\", FutureWarning)\n",
            "\u001b[2m\u001b[36m(pid=1266)\u001b[0m GPU available: True, used: True\n",
            "\u001b[2m\u001b[36m(pid=1266)\u001b[0m TPU available: False, using: 0 TPU cores\n",
            "\u001b[2m\u001b[36m(pid=1266)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "== Status ==\n",
            "Memory usage on this node: 2.3/12.7 GiB\n",
            "PopulationBasedTraining: 0 checkpoints, 0 perturbs\n",
            "Resources requested: 2.0/2 CPUs, 1.0/1 GPUs, 0.0/7.51 GiB heap, 0.0/3.75 GiB objects (0.0/1.0 accelerator_type:T4)\n",
            "Result logdir: /content/drive/MyDrive/Logs/Stage1_pbt_F\n",
            "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
            "+------------------------------------------+----------+-------+-----------------+-------+-------+--------------+\n",
            "| Trial name                               | status   | loc   |   learning_rate |   ngf |   ndf |   batch_size |\n",
            "|------------------------------------------+----------+-------+-----------------+-------+-------+--------------|\n",
            "| train_Stage1_tune_checkpoint_228f8_00000 | RUNNING  |       |          0.0001 |   128 |    32 |           64 |\n",
            "| train_Stage1_tune_checkpoint_228f8_00001 | PENDING  |       |          0.0001 |   128 |   128 |           64 |\n",
            "| train_Stage1_tune_checkpoint_228f8_00002 | PENDING  |       |          0.0001 |    32 |    64 |           64 |\n",
            "| train_Stage1_tune_checkpoint_228f8_00003 | PENDING  |       |          0.0001 |    32 |    64 |           64 |\n",
            "| train_Stage1_tune_checkpoint_228f8_00004 | PENDING  |       |          0.0001 |    32 |    64 |           64 |\n",
            "| train_Stage1_tune_checkpoint_228f8_00005 | PENDING  |       |          0.0001 |   128 |    64 |           64 |\n",
            "| train_Stage1_tune_checkpoint_228f8_00006 | PENDING  |       |          0.0001 |    32 |    64 |           64 |\n",
            "| train_Stage1_tune_checkpoint_228f8_00007 | PENDING  |       |          0.0001 |   128 |    64 |           64 |\n",
            "| train_Stage1_tune_checkpoint_228f8_00008 | PENDING  |       |          0.0001 |    32 |    64 |           64 |\n",
            "| train_Stage1_tune_checkpoint_228f8_00009 | PENDING  |       |          0.0001 |   128 |    32 |           64 |\n",
            "+------------------------------------------+----------+-------+-----------------+-------+-------+--------------+\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1266)\u001b[0m 2021-07-11 05:30:51.229461: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "\u001b[2m\u001b[36m(pid=1266)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1266)\u001b[0m   | Name          | Type               | Params\n",
            "\u001b[2m\u001b[36m(pid=1266)\u001b[0m -----------------------------------------------------\n",
            "\u001b[2m\u001b[36m(pid=1266)\u001b[0m 0 | generator     | DCGANGenerator     | 12.7 M\n",
            "\u001b[2m\u001b[36m(pid=1266)\u001b[0m 1 | discriminator | DCGANDiscriminator | 693 K \n",
            "\u001b[2m\u001b[36m(pid=1266)\u001b[0m 2 | criterion     | BCELoss            | 0     \n",
            "\u001b[2m\u001b[36m(pid=1266)\u001b[0m 3 | modelF        | LensResnet         | 11.2 M\n",
            "\u001b[2m\u001b[36m(pid=1266)\u001b[0m 4 | modelJ        | LensResnet         | 11.2 M\n",
            "\u001b[2m\u001b[36m(pid=1266)\u001b[0m -----------------------------------------------------\n",
            "\u001b[2m\u001b[36m(pid=1266)\u001b[0m 35.7 M    Trainable params\n",
            "\u001b[2m\u001b[36m(pid=1266)\u001b[0m 0         Non-trainable params\n",
            "\u001b[2m\u001b[36m(pid=1266)\u001b[0m 35.7 M    Total params\n",
            "\u001b[2m\u001b[36m(pid=1266)\u001b[0m 142.767   Total estimated model params size (MB)\n",
            "\u001b[2m\u001b[36m(pid=1266)\u001b[0m /usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/data_loading.py:349: UserWarning: Your val_dataloader has `shuffle=True`, it is best practice to turn this off for val/test/predict dataloaders.\n",
            "\u001b[2m\u001b[36m(pid=1266)\u001b[0m   f'Your {mode}_dataloader has `shuffle=True`, it is best practice to turn'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1266)\u001b[0m \rValidation sanity check: 0it [00:00, ?it/s]\rValidation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "== Status ==\n",
            "Memory usage on this node: 3.4/12.7 GiB\n",
            "PopulationBasedTraining: 0 checkpoints, 0 perturbs\n",
            "Resources requested: 2.0/2 CPUs, 1.0/1 GPUs, 0.0/7.51 GiB heap, 0.0/3.75 GiB objects (0.0/1.0 GPU_group_0_ecd3f63ef35a3c653f2d69781ac9fe69, 0.0/2.0 CPU_group_ecd3f63ef35a3c653f2d69781ac9fe69, 0.0/1.0 accelerator_type:T4, 0.0/1.0 GPU_group_ecd3f63ef35a3c653f2d69781ac9fe69, 0.0/2.0 CPU_group_0_ecd3f63ef35a3c653f2d69781ac9fe69)\n",
            "Result logdir: /content/drive/MyDrive/Logs/Stage1_pbt_F\n",
            "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
            "+------------------------------------------+----------+-------+-----------------+-------+-------+--------------+\n",
            "| Trial name                               | status   | loc   |   learning_rate |   ngf |   ndf |   batch_size |\n",
            "|------------------------------------------+----------+-------+-----------------+-------+-------+--------------|\n",
            "| train_Stage1_tune_checkpoint_228f8_00000 | RUNNING  |       |          0.0001 |   128 |    32 |           64 |\n",
            "| train_Stage1_tune_checkpoint_228f8_00001 | PENDING  |       |          0.0001 |   128 |   128 |           64 |\n",
            "| train_Stage1_tune_checkpoint_228f8_00002 | PENDING  |       |          0.0001 |    32 |    64 |           64 |\n",
            "| train_Stage1_tune_checkpoint_228f8_00003 | PENDING  |       |          0.0001 |    32 |    64 |           64 |\n",
            "| train_Stage1_tune_checkpoint_228f8_00004 | PENDING  |       |          0.0001 |    32 |    64 |           64 |\n",
            "| train_Stage1_tune_checkpoint_228f8_00005 | PENDING  |       |          0.0001 |   128 |    64 |           64 |\n",
            "| train_Stage1_tune_checkpoint_228f8_00006 | PENDING  |       |          0.0001 |    32 |    64 |           64 |\n",
            "| train_Stage1_tune_checkpoint_228f8_00007 | PENDING  |       |          0.0001 |   128 |    64 |           64 |\n",
            "| train_Stage1_tune_checkpoint_228f8_00008 | PENDING  |       |          0.0001 |    32 |    64 |           64 |\n",
            "| train_Stage1_tune_checkpoint_228f8_00009 | PENDING  |       |          0.0001 |   128 |    32 |           64 |\n",
            "+------------------------------------------+----------+-------+-----------------+-------+-------+--------------+\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1266)\u001b[0m /usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "\u001b[2m\u001b[36m(pid=1266)\u001b[0m   return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
            "\u001b[2m\u001b[36m(pid=1266)\u001b[0m /usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/deprecated_api.py:152: LightningDeprecationWarning: `Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\n",
            "\u001b[2m\u001b[36m(pid=1266)\u001b[0m   \"`Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0:   0%|          | 0/1173 [00:00<?, ?it/s] \n",
            "Epoch 0:   2%|▏         | 20/1173 [00:07<06:54,  2.78it/s, loss=1.69, v_num=.]\n",
            "Epoch 0:   3%|▎         | 40/1173 [00:13<06:24,  2.94it/s, loss=1.95, v_num=.]\n",
            "Epoch 0:   5%|▌         | 60/1173 [00:19<06:08,  3.02it/s, loss=2.09, v_num=.]\n",
            "Epoch 0:   7%|▋         | 80/1173 [00:26<05:57,  3.06it/s, loss=2.13, v_num=.]\n",
            "Epoch 0:   9%|▊         | 100/1173 [00:34<06:05,  2.94it/s, loss=2.22, v_num=.]\n",
            "Epoch 0:  10%|█         | 120/1173 [00:44<06:33,  2.67it/s, loss=2.3, v_num=.] \n",
            "Epoch 0:  12%|█▏        | 140/1173 [00:55<06:48,  2.53it/s, loss=2.24, v_num=.]\n",
            "Epoch 0:  14%|█▎        | 160/1173 [01:05<06:55,  2.44it/s, loss=2.31, v_num=.]\n",
            "Epoch 0:  15%|█▌        | 180/1173 [01:15<06:57,  2.38it/s, loss=2.27, v_num=.]\n",
            "Epoch 0:  17%|█▋        | 200/1173 [01:25<06:56,  2.34it/s, loss=2.38, v_num=.]\n",
            "Epoch 0:  19%|█▉        | 220/1173 [01:35<06:53,  2.30it/s, loss=2.32, v_num=.]\n",
            "Epoch 0:  20%|██        | 240/1173 [01:45<06:50,  2.27it/s, loss=2.38, v_num=.]\n",
            "Epoch 0:  22%|██▏       | 260/1173 [01:55<06:45,  2.25it/s, loss=2.43, v_num=.]\n",
            "Epoch 0:  24%|██▍       | 280/1173 [02:05<06:38,  2.24it/s, loss=2.43, v_num=.]\n",
            "Epoch 0:  26%|██▌       | 300/1173 [02:14<06:31,  2.23it/s, loss=2.49, v_num=.]\n",
            "Epoch 0:  27%|██▋       | 320/1173 [02:24<06:24,  2.22it/s, loss=2.45, v_num=.]\n",
            "Epoch 0:  29%|██▉       | 340/1173 [02:33<06:17,  2.21it/s, loss=2.47, v_num=.]\n",
            "Epoch 0:  31%|███       | 360/1173 [02:43<06:09,  2.20it/s, loss=2.52, v_num=.]\n",
            "Epoch 0:  32%|███▏      | 380/1173 [02:52<06:00,  2.20it/s, loss=2.52, v_num=.]\n",
            "Epoch 0:  34%|███▍      | 400/1173 [03:02<05:52,  2.19it/s, loss=2.53, v_num=.]\n",
            "Epoch 0:  36%|███▌      | 420/1173 [03:11<05:44,  2.19it/s, loss=2.63, v_num=.]\n",
            "Epoch 0:  38%|███▊      | 440/1173 [03:21<05:36,  2.18it/s, loss=2.68, v_num=.]\n",
            "Epoch 0:  39%|███▉      | 460/1173 [03:31<05:27,  2.18it/s, loss=2.67, v_num=.]\n",
            "Epoch 0:  41%|████      | 480/1173 [03:40<05:18,  2.17it/s, loss=2.7, v_num=.] \n",
            "Epoch 0:  43%|████▎     | 500/1173 [03:50<05:10,  2.17it/s, loss=2.75, v_num=.]\n",
            "Epoch 0:  44%|████▍     | 520/1173 [04:00<05:01,  2.16it/s, loss=2.79, v_num=.]\n",
            "Epoch 0:  46%|████▌     | 540/1173 [04:09<04:52,  2.16it/s, loss=2.72, v_num=.]\n",
            "Epoch 0:  48%|████▊     | 560/1173 [04:19<04:44,  2.16it/s, loss=2.79, v_num=.]\n",
            "Epoch 0:  49%|████▉     | 580/1173 [04:29<04:35,  2.15it/s, loss=2.81, v_num=.]\n",
            "Epoch 0:  51%|█████     | 600/1173 [04:38<04:26,  2.15it/s, loss=2.84, v_num=.]\n",
            "Epoch 0:  53%|█████▎    | 620/1173 [04:48<04:17,  2.15it/s, loss=2.91, v_num=.]\n",
            "Epoch 0:  55%|█████▍    | 640/1173 [04:57<04:08,  2.15it/s, loss=2.9, v_num=.] \n",
            "Epoch 0:  56%|█████▋    | 660/1173 [05:07<03:58,  2.15it/s, loss=2.95, v_num=.]\n",
            "Epoch 0:  58%|█████▊    | 680/1173 [05:16<03:49,  2.15it/s, loss=2.91, v_num=.]\n",
            "Epoch 0:  60%|█████▉    | 700/1173 [05:26<03:40,  2.14it/s, loss=2.92, v_num=.]\n",
            "Epoch 0:  61%|██████▏   | 720/1173 [05:36<03:31,  2.14it/s, loss=2.99, v_num=.]\n",
            "Epoch 0:  63%|██████▎   | 740/1173 [05:45<03:22,  2.14it/s, loss=3.06, v_num=.]\n",
            "Epoch 0:  65%|██████▍   | 760/1173 [05:54<03:12,  2.14it/s, loss=3.11, v_num=.]\n",
            "Epoch 0:  66%|██████▋   | 780/1173 [06:04<03:03,  2.14it/s, loss=3.09, v_num=.]\n",
            "Epoch 0:  68%|██████▊   | 800/1173 [06:13<02:54,  2.14it/s, loss=3.06, v_num=.]\n",
            "Epoch 0:  70%|██████▉   | 820/1173 [06:23<02:44,  2.14it/s, loss=2.9, v_num=.] \n",
            "Epoch 0:  72%|███████▏  | 840/1173 [06:32<02:35,  2.14it/s, loss=1.57, v_num=.]\n",
            "Epoch 0:  73%|███████▎  | 860/1173 [06:42<02:26,  2.14it/s, loss=2.48, v_num=.]\n",
            "Epoch 0:  75%|███████▌  | 880/1173 [06:51<02:16,  2.14it/s, loss=2.89, v_num=.]\n",
            "Epoch 0:  77%|███████▋  | 900/1173 [07:00<02:07,  2.14it/s, loss=2.8, v_num=.] \n",
            "Epoch 0:  78%|███████▊  | 920/1173 [07:10<01:58,  2.14it/s, loss=2.23, v_num=.]\n",
            "Epoch 0:  80%|████████  | 940/1173 [07:18<01:48,  2.14it/s, loss=2.23, v_num=.]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/235 [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[2m\u001b[36m(pid=1266)\u001b[0m \n",
            "Epoch 0:  82%|████████▏ | 960/1173 [07:27<01:39,  2.15it/s, loss=2.23, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1266)\u001b[0m \n",
            "Epoch 0:  84%|████████▎ | 980/1173 [07:36<01:29,  2.15it/s, loss=2.23, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1266)\u001b[0m \n",
            "Epoch 0:  85%|████████▌ | 1000/1173 [07:46<01:20,  2.15it/s, loss=2.23, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1266)\u001b[0m \n",
            "Epoch 0:  87%|████████▋ | 1020/1173 [07:55<01:11,  2.15it/s, loss=2.23, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1266)\u001b[0m \n",
            "Epoch 0:  89%|████████▊ | 1040/1173 [08:04<01:01,  2.15it/s, loss=2.23, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1266)\u001b[0m \n",
            "Epoch 0:  90%|█████████ | 1060/1173 [08:14<00:52,  2.14it/s, loss=2.23, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1266)\u001b[0m \n",
            "Epoch 0:  92%|█████████▏| 1080/1173 [08:23<00:43,  2.14it/s, loss=2.23, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1266)\u001b[0m \n",
            "Epoch 0:  94%|█████████▍| 1100/1173 [08:32<00:34,  2.14it/s, loss=2.23, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1266)\u001b[0m \n",
            "Epoch 0:  95%|█████████▌| 1120/1173 [08:42<00:24,  2.14it/s, loss=2.23, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1266)\u001b[0m \n",
            "Epoch 0:  97%|█████████▋| 1140/1173 [08:51<00:15,  2.14it/s, loss=2.23, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1266)\u001b[0m \n",
            "Epoch 0:  99%|█████████▉| 1160/1173 [09:01<00:06,  2.14it/s, loss=2.23, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1266)\u001b[0m \n",
            "Validating: 100%|██████████| 235/235 [01:49<00:00,  2.16it/s]\u001b[A\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1266)\u001b[0m /usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/deprecated_api.py:152: LightningDeprecationWarning: `Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\n",
            "\u001b[2m\u001b[36m(pid=1266)\u001b[0m   \"`Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for train_Stage1_tune_checkpoint_228f8_00000:\n",
            "  auroc: 0.4618435502052307\n",
            "  auroc_cross: 0.47367948293685913\n",
            "  date: 2021-07-11_05-40-06\n",
            "  done: false\n",
            "  experiment_id: 14d26a48183a4a18b4f18beb9fafaf34\n",
            "  hostname: bd0789e721af\n",
            "  iterations_since_restore: 1\n",
            "  lossD: 0.05285283178091049\n",
            "  lossG: 5.041823387145996\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 1266\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 560.2282562255859\n",
            "  time_this_iter_s: 560.2282562255859\n",
            "  time_total_s: 560.2282562255859\n",
            "  timestamp: 1625982006\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 1\n",
            "  trial_id: 228f8_00000\n",
            "  \n",
            "== Status ==\n",
            "Memory usage on this node: 4.5/12.7 GiB\n",
            "PopulationBasedTraining: 0 checkpoints, 0 perturbs\n",
            "Resources requested: 2.0/2 CPUs, 1.0/1 GPUs, 0.0/7.51 GiB heap, 0.0/3.75 GiB objects (0.0/1.0 GPU_group_0_ecd3f63ef35a3c653f2d69781ac9fe69, 0.0/2.0 CPU_group_ecd3f63ef35a3c653f2d69781ac9fe69, 0.0/1.0 accelerator_type:T4, 0.0/1.0 GPU_group_ecd3f63ef35a3c653f2d69781ac9fe69, 0.0/2.0 CPU_group_0_ecd3f63ef35a3c653f2d69781ac9fe69)\n",
            "Current best trial: 228f8_00000 with auroc=0.4618435502052307 and parameters={'learning_rate': 0.0001, 'ngf': 128, 'ndf': 32, 'batch_size': 64}\n",
            "Result logdir: /content/drive/MyDrive/Logs/Stage1_pbt_F\n",
            "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
            "+------------------------------------------+----------+-----------------+-----------------+-------+-------+--------------+---------+-----------+----------+---------------+----------------------+\n",
            "| Trial name                               | status   | loc             |   learning_rate |   ngf |   ndf |   batch_size |   lossG |     lossD |    auroc |   auroc_cross |   training_iteration |\n",
            "|------------------------------------------+----------+-----------------+-----------------+-------+-------+--------------+---------+-----------+----------+---------------+----------------------|\n",
            "| train_Stage1_tune_checkpoint_228f8_00000 | RUNNING  | 172.28.0.2:1266 |          0.0001 |   128 |    32 |           64 | 5.04182 | 0.0528528 | 0.461844 |      0.473679 |                    1 |\n",
            "| train_Stage1_tune_checkpoint_228f8_00001 | PENDING  |                 |          0.0001 |   128 |   128 |           64 |         |           |          |               |                      |\n",
            "| train_Stage1_tune_checkpoint_228f8_00002 | PENDING  |                 |          0.0001 |    32 |    64 |           64 |         |           |          |               |                      |\n",
            "| train_Stage1_tune_checkpoint_228f8_00003 | PENDING  |                 |          0.0001 |    32 |    64 |           64 |         |           |          |               |                      |\n",
            "| train_Stage1_tune_checkpoint_228f8_00004 | PENDING  |                 |          0.0001 |    32 |    64 |           64 |         |           |          |               |                      |\n",
            "| train_Stage1_tune_checkpoint_228f8_00005 | PENDING  |                 |          0.0001 |   128 |    64 |           64 |         |           |          |               |                      |\n",
            "| train_Stage1_tune_checkpoint_228f8_00006 | PENDING  |                 |          0.0001 |    32 |    64 |           64 |         |           |          |               |                      |\n",
            "| train_Stage1_tune_checkpoint_228f8_00007 | PENDING  |                 |          0.0001 |   128 |    64 |           64 |         |           |          |               |                      |\n",
            "| train_Stage1_tune_checkpoint_228f8_00008 | PENDING  |                 |          0.0001 |    32 |    64 |           64 |         |           |          |               |                      |\n",
            "| train_Stage1_tune_checkpoint_228f8_00009 | PENDING  |                 |          0.0001 |   128 |    32 |           64 |         |           |          |               |                      |\n",
            "+------------------------------------------+----------+-----------------+-----------------+-------+-------+--------------+---------+-----------+----------+---------------+----------------------+\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1266)\u001b[0m 2021-07-11 05:40:06,786\tINFO trainable.py:76 -- Checkpoint size is 249768194 bytes\n",
            "\u001b[2m\u001b[36m(pid=1265)\u001b[0m GPU available: True, used: True\n",
            "\u001b[2m\u001b[36m(pid=1265)\u001b[0m TPU available: False, using: 0 TPU cores\n",
            "\u001b[2m\u001b[36m(pid=1265)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\u001b[2m\u001b[36m(pid=1265)\u001b[0m 2021-07-11 05:40:24.082666: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "\u001b[2m\u001b[36m(pid=1265)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1265)\u001b[0m   | Name          | Type               | Params\n",
            "\u001b[2m\u001b[36m(pid=1265)\u001b[0m -----------------------------------------------------\n",
            "\u001b[2m\u001b[36m(pid=1265)\u001b[0m 0 | generator     | DCGANGenerator     | 12.7 M\n",
            "\u001b[2m\u001b[36m(pid=1265)\u001b[0m 1 | discriminator | DCGANDiscriminator | 11.0 M\n",
            "\u001b[2m\u001b[36m(pid=1265)\u001b[0m 2 | criterion     | BCELoss            | 0     \n",
            "\u001b[2m\u001b[36m(pid=1265)\u001b[0m 3 | modelF        | LensResnet         | 11.2 M\n",
            "\u001b[2m\u001b[36m(pid=1265)\u001b[0m 4 | modelJ        | LensResnet         | 11.2 M\n",
            "\u001b[2m\u001b[36m(pid=1265)\u001b[0m -----------------------------------------------------\n",
            "\u001b[2m\u001b[36m(pid=1265)\u001b[0m 46.0 M    Trainable params\n",
            "\u001b[2m\u001b[36m(pid=1265)\u001b[0m 0         Non-trainable params\n",
            "\u001b[2m\u001b[36m(pid=1265)\u001b[0m 46.0 M    Total params\n",
            "\u001b[2m\u001b[36m(pid=1265)\u001b[0m 184.121   Total estimated model params size (MB)\n",
            "\u001b[2m\u001b[36m(pid=1265)\u001b[0m /usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/data_loading.py:349: UserWarning: Your val_dataloader has `shuffle=True`, it is best practice to turn this off for val/test/predict dataloaders.\n",
            "\u001b[2m\u001b[36m(pid=1265)\u001b[0m   f'Your {mode}_dataloader has `shuffle=True`, it is best practice to turn'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1265)\u001b[0m \rValidation sanity check: 0it [00:00, ?it/s]\rValidation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1265)\u001b[0m /usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "\u001b[2m\u001b[36m(pid=1265)\u001b[0m   return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
            "\u001b[2m\u001b[36m(pid=1265)\u001b[0m /usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/deprecated_api.py:152: LightningDeprecationWarning: `Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\n",
            "\u001b[2m\u001b[36m(pid=1265)\u001b[0m   \"`Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0:   0%|          | 0/1173 [00:00<?, ?it/s] \n",
            "Epoch 0:   2%|▏         | 20/1173 [00:08<08:08,  2.36it/s, loss=6.41, v_num=.]\n",
            "Epoch 0:   3%|▎         | 40/1173 [00:16<07:47,  2.42it/s, loss=5.16, v_num=.]\n",
            "Epoch 0:   5%|▌         | 60/1173 [00:24<07:39,  2.42it/s, loss=6.23, v_num=.]\n",
            "Epoch 0:   7%|▋         | 80/1173 [00:33<07:34,  2.41it/s, loss=5.27, v_num=.]\n",
            "Epoch 0:   9%|▊         | 100/1173 [00:41<07:28,  2.39it/s, loss=5.31, v_num=.]\n",
            "Epoch 0:  10%|█         | 120/1173 [00:50<07:24,  2.37it/s, loss=5.6, v_num=.] \n",
            "Epoch 0:  12%|█▏        | 140/1173 [00:59<07:21,  2.34it/s, loss=5.17, v_num=.]\n",
            "Epoch 0:  14%|█▎        | 160/1173 [01:09<07:18,  2.31it/s, loss=5.39, v_num=.]\n",
            "Epoch 0:  15%|█▌        | 180/1173 [01:18<07:11,  2.30it/s, loss=75.1, v_num=.]\n",
            "Epoch 0:  17%|█▋        | 200/1173 [01:26<07:02,  2.30it/s, loss=88.8, v_num=.]\n",
            "Epoch 0:  19%|█▉        | 220/1173 [01:35<06:52,  2.31it/s, loss=89.1, v_num=.]\n",
            "Epoch 0:  20%|██        | 240/1173 [01:43<06:42,  2.32it/s, loss=89.2, v_num=.]\n",
            "Epoch 0:  22%|██▏       | 260/1173 [01:51<06:32,  2.32it/s, loss=89.3, v_num=.]\n",
            "Epoch 0:  24%|██▍       | 280/1173 [02:01<06:27,  2.31it/s, loss=89, v_num=.]  \n",
            "Epoch 0:  26%|██▌       | 300/1173 [02:10<06:20,  2.29it/s, loss=89.2, v_num=.]\n",
            "Epoch 0:  27%|██▋       | 320/1173 [02:20<06:14,  2.28it/s, loss=89, v_num=.]  \n",
            "Epoch 0:  29%|██▉       | 340/1173 [02:30<06:07,  2.27it/s, loss=89.3, v_num=.]\n",
            "Epoch 0:  31%|███       | 360/1173 [02:39<05:59,  2.26it/s, loss=89.1, v_num=.]\n",
            "Epoch 0:  32%|███▏      | 380/1173 [02:48<05:51,  2.25it/s, loss=89.2, v_num=.]\n",
            "Epoch 0:  34%|███▍      | 400/1173 [02:57<05:43,  2.25it/s, loss=89.2, v_num=.]\n",
            "Epoch 0:  36%|███▌      | 420/1173 [03:07<05:35,  2.24it/s, loss=89.2, v_num=.]\n",
            "Epoch 0:  38%|███▊      | 440/1173 [03:16<05:27,  2.24it/s, loss=89, v_num=.]  \n",
            "Epoch 0:  39%|███▉      | 460/1173 [03:26<05:19,  2.23it/s, loss=89.1, v_num=.]\n",
            "Epoch 0:  41%|████      | 480/1173 [03:35<05:11,  2.23it/s, loss=89.2, v_num=.]\n",
            "Epoch 0:  43%|████▎     | 500/1173 [03:44<05:02,  2.22it/s, loss=89.1, v_num=.]\n",
            "Epoch 0:  44%|████▍     | 520/1173 [03:54<04:54,  2.22it/s, loss=89.1, v_num=.]\n",
            "Epoch 0:  46%|████▌     | 540/1173 [04:03<04:46,  2.21it/s, loss=89.2, v_num=.]\n",
            "Epoch 0:  48%|████▊     | 560/1173 [04:13<04:37,  2.21it/s, loss=89, v_num=.]  \n",
            "Epoch 0:  49%|████▉     | 580/1173 [04:22<04:28,  2.21it/s, loss=88.9, v_num=.]\n",
            "Epoch 0:  51%|█████     | 600/1173 [04:32<04:19,  2.20it/s, loss=89.1, v_num=.]\n",
            "Epoch 0:  53%|█████▎    | 620/1173 [04:41<04:10,  2.20it/s, loss=89, v_num=.]  \n",
            "Epoch 0:  55%|█████▍    | 640/1173 [04:50<04:02,  2.20it/s, loss=89.1, v_num=.]\n",
            "Epoch 0:  56%|█████▋    | 660/1173 [04:59<03:53,  2.20it/s, loss=89, v_num=.]  \n",
            "Epoch 0:  58%|█████▊    | 680/1173 [05:09<03:44,  2.20it/s, loss=89.1, v_num=.]\n",
            "Epoch 0:  60%|█████▉    | 700/1173 [05:18<03:35,  2.20it/s, loss=88.9, v_num=.]\n",
            "Epoch 0:  61%|██████▏   | 720/1173 [05:28<03:26,  2.20it/s, loss=89.1, v_num=.]\n",
            "Epoch 0:  63%|██████▎   | 740/1173 [05:37<03:17,  2.19it/s, loss=89.2, v_num=.]\n",
            "Epoch 0:  65%|██████▍   | 760/1173 [05:46<03:08,  2.19it/s, loss=89.1, v_num=.]\n",
            "Epoch 0:  66%|██████▋   | 780/1173 [05:56<02:59,  2.19it/s, loss=89.1, v_num=.]\n",
            "Epoch 0:  68%|██████▊   | 800/1173 [06:05<02:50,  2.19it/s, loss=89.1, v_num=.]\n",
            "Epoch 0:  70%|██████▉   | 820/1173 [06:14<02:41,  2.19it/s, loss=89.1, v_num=.]\n",
            "Epoch 0:  72%|███████▏  | 840/1173 [06:24<02:32,  2.19it/s, loss=89.1, v_num=.]\n",
            "Epoch 0:  73%|███████▎  | 860/1173 [06:33<02:23,  2.18it/s, loss=89.1, v_num=.]\n",
            "Epoch 0:  75%|███████▌  | 880/1173 [06:43<02:14,  2.18it/s, loss=89.1, v_num=.]\n",
            "Epoch 0:  77%|███████▋  | 900/1173 [06:52<02:05,  2.18it/s, loss=89.2, v_num=.]\n",
            "Epoch 0:  78%|███████▊  | 920/1173 [07:01<01:55,  2.18it/s, loss=89.1, v_num=.]\n",
            "Epoch 0:  80%|████████  | 940/1173 [07:09<01:46,  2.19it/s, loss=89.1, v_num=.]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/235 [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[2m\u001b[36m(pid=1265)\u001b[0m \n",
            "Epoch 0:  82%|████████▏ | 960/1173 [07:18<01:37,  2.19it/s, loss=89.1, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1265)\u001b[0m \n",
            "Epoch 0:  84%|████████▎ | 980/1173 [07:27<01:28,  2.19it/s, loss=89.1, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1265)\u001b[0m \n",
            "Epoch 0:  85%|████████▌ | 1000/1173 [07:36<01:19,  2.19it/s, loss=89.1, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1265)\u001b[0m \n",
            "Epoch 0:  87%|████████▋ | 1020/1173 [07:46<01:09,  2.19it/s, loss=89.1, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1265)\u001b[0m \n",
            "Epoch 0:  89%|████████▊ | 1040/1173 [07:55<01:00,  2.19it/s, loss=89.1, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1265)\u001b[0m \n",
            "Epoch 0:  90%|█████████ | 1060/1173 [08:04<00:51,  2.19it/s, loss=89.1, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1265)\u001b[0m \n",
            "Epoch 0:  92%|█████████▏| 1080/1173 [08:14<00:42,  2.19it/s, loss=89.1, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1265)\u001b[0m \n",
            "Epoch 0:  94%|█████████▍| 1100/1173 [08:23<00:33,  2.19it/s, loss=89.1, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1265)\u001b[0m \n",
            "Epoch 0:  95%|█████████▌| 1120/1173 [08:32<00:24,  2.19it/s, loss=89.1, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1265)\u001b[0m \n",
            "Epoch 0:  97%|█████████▋| 1140/1173 [08:41<00:15,  2.19it/s, loss=89.1, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1265)\u001b[0m \n",
            "Epoch 0:  99%|█████████▉| 1160/1173 [08:50<00:05,  2.19it/s, loss=89.1, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1265)\u001b[0m \n",
            "Validating: 100%|██████████| 235/235 [01:47<00:00,  2.21it/s]\u001b[A\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1265)\u001b[0m /usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/deprecated_api.py:152: LightningDeprecationWarning: `Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\n",
            "\u001b[2m\u001b[36m(pid=1265)\u001b[0m   \"`Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\"\n",
            "2021-07-11 05:49:29,734\tINFO pbt.py:490 -- [pbt]: no checkpoint for trial. Skip exploit for Trial train_Stage1_tune_checkpoint_228f8_00001\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for train_Stage1_tune_checkpoint_228f8_00001:\n",
            "  auroc: 0.3095346987247467\n",
            "  auroc_cross: 0.381914347410202\n",
            "  date: 2021-07-11_05-49-29\n",
            "  done: false\n",
            "  experiment_id: f111bebca8014281875ff9bfc982c872\n",
            "  hostname: bd0789e721af\n",
            "  iterations_since_restore: 1\n",
            "  lossD: 79.15614318847656\n",
            "  lossG: 100.0\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 1265\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 558.8177676200867\n",
            "  time_this_iter_s: 558.8177676200867\n",
            "  time_total_s: 558.8177676200867\n",
            "  timestamp: 1625982569\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 1\n",
            "  trial_id: 228f8_00001\n",
            "  \n",
            "== Status ==\n",
            "Memory usage on this node: 4.6/12.7 GiB\n",
            "PopulationBasedTraining: 0 checkpoints, 0 perturbs\n",
            "Resources requested: 2.0/2 CPUs, 1.0/1 GPUs, 0.0/7.51 GiB heap, 0.0/3.75 GiB objects (0.0/1.0 GPU_group_0_ecd3f63ef35a3c653f2d69781ac9fe69, 0.0/1.0 accelerator_type:T4, 0.0/1.0 GPU_group_ecd3f63ef35a3c653f2d69781ac9fe69, 0.0/2.0 CPU_group_ecd3f63ef35a3c653f2d69781ac9fe69, 0.0/2.0 CPU_group_0_ecd3f63ef35a3c653f2d69781ac9fe69)\n",
            "Current best trial: 228f8_00000 with auroc=0.4618435502052307 and parameters={'learning_rate': 0.0001, 'ngf': 128, 'ndf': 32, 'batch_size': 64}\n",
            "Result logdir: /content/drive/MyDrive/Logs/Stage1_pbt_F\n",
            "Number of trials: 10/10 (1 PAUSED, 8 PENDING, 1 RUNNING)\n",
            "+------------------------------------------+----------+-----------------+-----------------+-------+-------+--------------+-----------+------------+----------+---------------+----------------------+\n",
            "| Trial name                               | status   | loc             |   learning_rate |   ngf |   ndf |   batch_size |     lossG |      lossD |    auroc |   auroc_cross |   training_iteration |\n",
            "|------------------------------------------+----------+-----------------+-----------------+-------+-------+--------------+-----------+------------+----------+---------------+----------------------|\n",
            "| train_Stage1_tune_checkpoint_228f8_00001 | RUNNING  | 172.28.0.2:1265 |          0.0001 |   128 |   128 |           64 | 100       | 79.1561    | 0.309535 |      0.381914 |                    1 |\n",
            "| train_Stage1_tune_checkpoint_228f8_00000 | PAUSED   |                 |          0.0001 |   128 |    32 |           64 |   5.04182 |  0.0528528 | 0.461844 |      0.473679 |                    1 |\n",
            "| train_Stage1_tune_checkpoint_228f8_00002 | PENDING  |                 |          0.0001 |    32 |    64 |           64 |           |            |          |               |                      |\n",
            "| train_Stage1_tune_checkpoint_228f8_00003 | PENDING  |                 |          0.0001 |    32 |    64 |           64 |           |            |          |               |                      |\n",
            "| train_Stage1_tune_checkpoint_228f8_00004 | PENDING  |                 |          0.0001 |    32 |    64 |           64 |           |            |          |               |                      |\n",
            "| train_Stage1_tune_checkpoint_228f8_00005 | PENDING  |                 |          0.0001 |   128 |    64 |           64 |           |            |          |               |                      |\n",
            "| train_Stage1_tune_checkpoint_228f8_00006 | PENDING  |                 |          0.0001 |    32 |    64 |           64 |           |            |          |               |                      |\n",
            "| train_Stage1_tune_checkpoint_228f8_00007 | PENDING  |                 |          0.0001 |   128 |    64 |           64 |           |            |          |               |                      |\n",
            "| train_Stage1_tune_checkpoint_228f8_00008 | PENDING  |                 |          0.0001 |    32 |    64 |           64 |           |            |          |               |                      |\n",
            "| train_Stage1_tune_checkpoint_228f8_00009 | PENDING  |                 |          0.0001 |   128 |    32 |           64 |           |            |          |               |                      |\n",
            "+------------------------------------------+----------+-----------------+-----------------+-------+-------+--------------+-----------+------------+----------+---------------+----------------------+\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1265)\u001b[0m 2021-07-11 05:49:30,867\tINFO trainable.py:76 -- Checkpoint size is 373840194 bytes\n",
            "\u001b[2m\u001b[33m(raylet)\u001b[0m /usr/local/lib/python3.7/dist-packages/ray/autoscaler/_private/cli_logger.py:61: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
            "\u001b[2m\u001b[33m(raylet)\u001b[0m   \"update your install command.\", FutureWarning)\n",
            "\u001b[2m\u001b[36m(pid=1643)\u001b[0m GPU available: True, used: True\n",
            "\u001b[2m\u001b[36m(pid=1643)\u001b[0m TPU available: False, using: 0 TPU cores\n",
            "\u001b[2m\u001b[36m(pid=1643)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\u001b[2m\u001b[36m(pid=1643)\u001b[0m 2021-07-11 05:49:52.996036: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "\u001b[2m\u001b[36m(pid=1643)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1643)\u001b[0m   | Name          | Type               | Params\n",
            "\u001b[2m\u001b[36m(pid=1643)\u001b[0m -----------------------------------------------------\n",
            "\u001b[2m\u001b[36m(pid=1643)\u001b[0m 0 | generator     | DCGANGenerator     | 1.1 M \n",
            "\u001b[2m\u001b[36m(pid=1643)\u001b[0m 1 | discriminator | DCGANDiscriminator | 2.8 M \n",
            "\u001b[2m\u001b[36m(pid=1643)\u001b[0m 2 | criterion     | BCELoss            | 0     \n",
            "\u001b[2m\u001b[36m(pid=1643)\u001b[0m 3 | modelF        | LensResnet         | 11.2 M\n",
            "\u001b[2m\u001b[36m(pid=1643)\u001b[0m 4 | modelJ        | LensResnet         | 11.2 M\n",
            "\u001b[2m\u001b[36m(pid=1643)\u001b[0m -----------------------------------------------------\n",
            "\u001b[2m\u001b[36m(pid=1643)\u001b[0m 26.2 M    Trainable params\n",
            "\u001b[2m\u001b[36m(pid=1643)\u001b[0m 0         Non-trainable params\n",
            "\u001b[2m\u001b[36m(pid=1643)\u001b[0m 26.2 M    Total params\n",
            "\u001b[2m\u001b[36m(pid=1643)\u001b[0m 104.826   Total estimated model params size (MB)\n",
            "\u001b[2m\u001b[36m(pid=1643)\u001b[0m /usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/data_loading.py:349: UserWarning: Your val_dataloader has `shuffle=True`, it is best practice to turn this off for val/test/predict dataloaders.\n",
            "\u001b[2m\u001b[36m(pid=1643)\u001b[0m   f'Your {mode}_dataloader has `shuffle=True`, it is best practice to turn'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1643)\u001b[0m \rValidation sanity check: 0it [00:00, ?it/s]\rValidation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1643)\u001b[0m /usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "\u001b[2m\u001b[36m(pid=1643)\u001b[0m   return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
            "\u001b[2m\u001b[36m(pid=1643)\u001b[0m /usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/deprecated_api.py:152: LightningDeprecationWarning: `Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\n",
            "\u001b[2m\u001b[36m(pid=1643)\u001b[0m   \"`Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0:   0%|          | 0/1173 [00:00<?, ?it/s] \n",
            "Epoch 0:   2%|▏         | 20/1173 [00:06<06:03,  3.17it/s, loss=2.51, v_num=.]\n",
            "Epoch 0:   3%|▎         | 40/1173 [00:12<05:50,  3.23it/s, loss=3.05, v_num=.]\n",
            "Epoch 0:   5%|▌         | 60/1173 [00:18<05:40,  3.27it/s, loss=3.38, v_num=.]\n",
            "Epoch 0:   7%|▋         | 80/1173 [00:27<06:15,  2.91it/s, loss=3.62, v_num=.]\n",
            "Epoch 0:   9%|▊         | 100/1173 [00:37<06:37,  2.70it/s, loss=3.72, v_num=.]\n",
            "Epoch 0:  10%|█         | 120/1173 [00:47<06:53,  2.55it/s, loss=3.86, v_num=.]\n",
            "Epoch 0:  12%|█▏        | 140/1173 [00:57<07:02,  2.44it/s, loss=3.88, v_num=.]\n",
            "Epoch 0:  14%|█▎        | 160/1173 [01:07<07:07,  2.37it/s, loss=4.11, v_num=.]\n",
            "Epoch 0:  15%|█▌        | 180/1173 [01:17<07:08,  2.32it/s, loss=3.3, v_num=.] \n",
            "Epoch 0:  17%|█▋        | 200/1173 [01:27<07:07,  2.28it/s, loss=2.94, v_num=.]\n",
            "Epoch 0:  19%|█▉        | 220/1173 [01:37<07:02,  2.25it/s, loss=3.15, v_num=.]\n",
            "Epoch 0:  20%|██        | 240/1173 [01:47<06:57,  2.23it/s, loss=3.23, v_num=.]\n",
            "Epoch 0:  22%|██▏       | 260/1173 [01:57<06:52,  2.22it/s, loss=3.45, v_num=.]\n",
            "Epoch 0:  24%|██▍       | 280/1173 [02:07<06:45,  2.20it/s, loss=3.7, v_num=.] \n",
            "Epoch 0:  26%|██▌       | 300/1173 [02:16<06:37,  2.20it/s, loss=3.75, v_num=.]\n",
            "Epoch 0:  27%|██▋       | 320/1173 [02:26<06:30,  2.18it/s, loss=3.76, v_num=.]\n",
            "Epoch 0:  29%|██▉       | 340/1173 [02:36<06:22,  2.18it/s, loss=3.76, v_num=.]\n",
            "Epoch 0:  31%|███       | 360/1173 [02:45<06:14,  2.17it/s, loss=3.79, v_num=.]\n",
            "Epoch 0:  32%|███▏      | 380/1173 [02:55<06:05,  2.17it/s, loss=3.77, v_num=.]\n",
            "Epoch 0:  34%|███▍      | 400/1173 [03:04<05:56,  2.17it/s, loss=3.76, v_num=.]\n",
            "Epoch 0:  36%|███▌      | 420/1173 [03:13<05:47,  2.17it/s, loss=3.77, v_num=.]\n",
            "Epoch 0:  38%|███▊      | 440/1173 [03:23<05:38,  2.17it/s, loss=3.79, v_num=.]\n",
            "Epoch 0:  39%|███▉      | 460/1173 [03:32<05:30,  2.16it/s, loss=3.77, v_num=.]\n",
            "Epoch 0:  41%|████      | 480/1173 [03:42<05:21,  2.16it/s, loss=3.84, v_num=.]\n",
            "Epoch 0:  43%|████▎     | 500/1173 [03:52<05:12,  2.15it/s, loss=3.86, v_num=.]\n",
            "Epoch 0:  44%|████▍     | 520/1173 [04:01<05:03,  2.15it/s, loss=3.86, v_num=.]\n",
            "Epoch 0:  46%|████▌     | 540/1173 [04:11<04:54,  2.15it/s, loss=3.87, v_num=.]\n",
            "Epoch 0:  48%|████▊     | 560/1173 [04:20<04:45,  2.15it/s, loss=3.88, v_num=.]\n",
            "Epoch 0:  49%|████▉     | 580/1173 [04:30<04:36,  2.14it/s, loss=3.91, v_num=.]\n",
            "Epoch 0:  51%|█████     | 600/1173 [04:40<04:27,  2.14it/s, loss=3.93, v_num=.]\n",
            "Epoch 0:  53%|█████▎    | 620/1173 [04:49<04:18,  2.14it/s, loss=4.01, v_num=.]\n",
            "Epoch 0:  55%|█████▍    | 640/1173 [04:58<04:08,  2.14it/s, loss=4.06, v_num=.]\n",
            "Epoch 0:  56%|█████▋    | 660/1173 [05:08<03:59,  2.14it/s, loss=4.07, v_num=.]\n",
            "Epoch 0:  58%|█████▊    | 680/1173 [05:17<03:50,  2.14it/s, loss=4.04, v_num=.]\n",
            "Epoch 0:  60%|█████▉    | 700/1173 [05:27<03:41,  2.14it/s, loss=4.07, v_num=.]\n",
            "Epoch 0:  61%|██████▏   | 720/1173 [05:37<03:32,  2.14it/s, loss=4.03, v_num=.]\n",
            "Epoch 0:  63%|██████▎   | 740/1173 [05:46<03:22,  2.14it/s, loss=4.07, v_num=.]\n",
            "Epoch 0:  65%|██████▍   | 760/1173 [05:55<03:13,  2.14it/s, loss=4.09, v_num=.]\n",
            "Epoch 0:  66%|██████▋   | 780/1173 [06:05<03:04,  2.13it/s, loss=4.11, v_num=.]\n",
            "Epoch 0:  68%|██████▊   | 800/1173 [06:14<02:54,  2.13it/s, loss=4.11, v_num=.]\n",
            "Epoch 0:  70%|██████▉   | 820/1173 [06:24<02:45,  2.14it/s, loss=4.08, v_num=.]\n",
            "Epoch 0:  72%|███████▏  | 840/1173 [06:33<02:36,  2.13it/s, loss=4.12, v_num=.]\n",
            "Epoch 0:  73%|███████▎  | 860/1173 [06:43<02:26,  2.13it/s, loss=4.13, v_num=.]\n",
            "Epoch 0:  75%|███████▌  | 880/1173 [06:52<02:17,  2.13it/s, loss=4.14, v_num=.]\n",
            "Epoch 0:  77%|███████▋  | 900/1173 [07:01<02:07,  2.13it/s, loss=4.17, v_num=.]\n",
            "Epoch 0:  78%|███████▊  | 920/1173 [07:11<01:58,  2.13it/s, loss=4.26, v_num=.]\n",
            "Epoch 0:  80%|████████  | 940/1173 [07:19<01:48,  2.14it/s, loss=4.26, v_num=.]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/235 [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[2m\u001b[36m(pid=1643)\u001b[0m \n",
            "Epoch 0:  82%|████████▏ | 960/1173 [07:28<01:39,  2.14it/s, loss=4.26, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1643)\u001b[0m \n",
            "Epoch 0:  84%|████████▎ | 980/1173 [07:37<01:30,  2.14it/s, loss=4.26, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1643)\u001b[0m \n",
            "Epoch 0:  85%|████████▌ | 1000/1173 [07:47<01:20,  2.14it/s, loss=4.26, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1643)\u001b[0m \n",
            "Epoch 0:  87%|████████▋ | 1020/1173 [07:56<01:11,  2.14it/s, loss=4.26, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1643)\u001b[0m \n",
            "Epoch 0:  89%|████████▊ | 1040/1173 [08:06<01:02,  2.14it/s, loss=4.26, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1643)\u001b[0m \n",
            "Epoch 0:  90%|█████████ | 1060/1173 [08:15<00:52,  2.14it/s, loss=4.26, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1643)\u001b[0m \n",
            "Epoch 0:  92%|█████████▏| 1080/1173 [08:25<00:43,  2.14it/s, loss=4.26, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1643)\u001b[0m \n",
            "Epoch 0:  94%|█████████▍| 1100/1173 [08:34<00:34,  2.14it/s, loss=4.26, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1643)\u001b[0m \n",
            "Epoch 0:  95%|█████████▌| 1120/1173 [08:44<00:24,  2.14it/s, loss=4.26, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1643)\u001b[0m \n",
            "Epoch 0:  97%|█████████▋| 1140/1173 [08:53<00:15,  2.14it/s, loss=4.26, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1643)\u001b[0m \n",
            "Epoch 0:  99%|█████████▉| 1160/1173 [09:03<00:06,  2.13it/s, loss=4.26, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1643)\u001b[0m \n",
            "Validating: 100%|██████████| 235/235 [01:50<00:00,  2.18it/s]\u001b[A\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1643)\u001b[0m /usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/deprecated_api.py:152: LightningDeprecationWarning: `Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\n",
            "\u001b[2m\u001b[36m(pid=1643)\u001b[0m   \"`Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for train_Stage1_tune_checkpoint_228f8_00002:\n",
            "  auroc: 0.47464263439178467\n",
            "  auroc_cross: 0.4897451400756836\n",
            "  date: 2021-07-11_05-59-09\n",
            "  done: false\n",
            "  experiment_id: 21ed1fd3506342328606b025c798018c\n",
            "  hostname: bd0789e721af\n",
            "  iterations_since_restore: 1\n",
            "  lossD: 0.000612126721534878\n",
            "  lossG: 8.432461738586426\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 1643\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 569.0690321922302\n",
            "  time_this_iter_s: 569.0690321922302\n",
            "  time_total_s: 569.0690321922302\n",
            "  timestamp: 1625983149\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 1\n",
            "  trial_id: 228f8_00002\n",
            "  \n",
            "== Status ==\n",
            "Memory usage on this node: 4.9/12.7 GiB\n",
            "PopulationBasedTraining: 1 checkpoints, 0 perturbs\n",
            "Resources requested: 2.0/2 CPUs, 1.0/1 GPUs, 0.0/7.51 GiB heap, 0.0/3.75 GiB objects (0.0/1.0 accelerator_type:T4, 0.0/1.0 GPU_group_ecd3f63ef35a3c653f2d69781ac9fe69, 0.0/2.0 CPU_group_0_ecd3f63ef35a3c653f2d69781ac9fe69, 0.0/1.0 GPU_group_0_ecd3f63ef35a3c653f2d69781ac9fe69, 0.0/2.0 CPU_group_ecd3f63ef35a3c653f2d69781ac9fe69)\n",
            "Current best trial: 228f8_00002 with auroc=0.47464263439178467 and parameters={'learning_rate': 0.0001, 'ngf': 32, 'ndf': 64, 'batch_size': 64}\n",
            "Result logdir: /content/drive/MyDrive/Logs/Stage1_pbt_F\n",
            "Number of trials: 10/10 (2 PAUSED, 7 PENDING, 1 RUNNING)\n",
            "+------------------------------------------+----------+-----------------+-----------------+-------+-------+--------------+-----------+--------------+----------+---------------+----------------------+\n",
            "| Trial name                               | status   | loc             |   learning_rate |   ngf |   ndf |   batch_size |     lossG |        lossD |    auroc |   auroc_cross |   training_iteration |\n",
            "|------------------------------------------+----------+-----------------+-----------------+-------+-------+--------------+-----------+--------------+----------+---------------+----------------------|\n",
            "| train_Stage1_tune_checkpoint_228f8_00002 | RUNNING  | 172.28.0.2:1643 |          0.0001 |    32 |    64 |           64 |   8.43246 |  0.000612127 | 0.474643 |      0.489745 |                    1 |\n",
            "| train_Stage1_tune_checkpoint_228f8_00000 | PAUSED   |                 |          0.0001 |   128 |    32 |           64 |   5.04182 |  0.0528528   | 0.461844 |      0.473679 |                    1 |\n",
            "| train_Stage1_tune_checkpoint_228f8_00001 | PAUSED   |                 |          0.0001 |   128 |   128 |           64 | 100       | 79.1561      | 0.309535 |      0.381914 |                    1 |\n",
            "| train_Stage1_tune_checkpoint_228f8_00003 | PENDING  |                 |          0.0001 |    32 |    64 |           64 |           |              |          |               |                      |\n",
            "| train_Stage1_tune_checkpoint_228f8_00004 | PENDING  |                 |          0.0001 |    32 |    64 |           64 |           |              |          |               |                      |\n",
            "| train_Stage1_tune_checkpoint_228f8_00005 | PENDING  |                 |          0.0001 |   128 |    64 |           64 |           |              |          |               |                      |\n",
            "| train_Stage1_tune_checkpoint_228f8_00006 | PENDING  |                 |          0.0001 |    32 |    64 |           64 |           |              |          |               |                      |\n",
            "| train_Stage1_tune_checkpoint_228f8_00007 | PENDING  |                 |          0.0001 |   128 |    64 |           64 |           |              |          |               |                      |\n",
            "| train_Stage1_tune_checkpoint_228f8_00008 | PENDING  |                 |          0.0001 |    32 |    64 |           64 |           |              |          |               |                      |\n",
            "| train_Stage1_tune_checkpoint_228f8_00009 | PENDING  |                 |          0.0001 |   128 |    32 |           64 |           |              |          |               |                      |\n",
            "+------------------------------------------+----------+-----------------+-----------------+-------+-------+--------------+-----------+--------------+----------+---------------+----------------------+\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1643)\u001b[0m 2021-07-11 05:59:09,700\tINFO trainable.py:76 -- Checkpoint size is 135937282 bytes\n",
            "\u001b[2m\u001b[36m(pid=1643)\u001b[0m 2021-07-11 05:59:10,239\tINFO trainable.py:76 -- Checkpoint size is 135937282 bytes\n",
            "\u001b[2m\u001b[33m(raylet)\u001b[0m /usr/local/lib/python3.7/dist-packages/ray/autoscaler/_private/cli_logger.py:61: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
            "\u001b[2m\u001b[33m(raylet)\u001b[0m   \"update your install command.\", FutureWarning)\n",
            "\u001b[2m\u001b[36m(pid=1852)\u001b[0m GPU available: True, used: True\n",
            "\u001b[2m\u001b[36m(pid=1852)\u001b[0m TPU available: False, using: 0 TPU cores\n",
            "\u001b[2m\u001b[36m(pid=1852)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\u001b[2m\u001b[36m(pid=1852)\u001b[0m 2021-07-11 05:59:30.239435: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "\u001b[2m\u001b[36m(pid=1852)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1852)\u001b[0m   | Name          | Type               | Params\n",
            "\u001b[2m\u001b[36m(pid=1852)\u001b[0m -----------------------------------------------------\n",
            "\u001b[2m\u001b[36m(pid=1852)\u001b[0m 0 | generator     | DCGANGenerator     | 1.1 M \n",
            "\u001b[2m\u001b[36m(pid=1852)\u001b[0m 1 | discriminator | DCGANDiscriminator | 2.8 M \n",
            "\u001b[2m\u001b[36m(pid=1852)\u001b[0m 2 | criterion     | BCELoss            | 0     \n",
            "\u001b[2m\u001b[36m(pid=1852)\u001b[0m 3 | modelF        | LensResnet         | 11.2 M\n",
            "\u001b[2m\u001b[36m(pid=1852)\u001b[0m 4 | modelJ        | LensResnet         | 11.2 M\n",
            "\u001b[2m\u001b[36m(pid=1852)\u001b[0m -----------------------------------------------------\n",
            "\u001b[2m\u001b[36m(pid=1852)\u001b[0m 26.2 M    Trainable params\n",
            "\u001b[2m\u001b[36m(pid=1852)\u001b[0m 0         Non-trainable params\n",
            "\u001b[2m\u001b[36m(pid=1852)\u001b[0m 26.2 M    Total params\n",
            "\u001b[2m\u001b[36m(pid=1852)\u001b[0m 104.826   Total estimated model params size (MB)\n",
            "\u001b[2m\u001b[36m(pid=1852)\u001b[0m /usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/data_loading.py:349: UserWarning: Your val_dataloader has `shuffle=True`, it is best practice to turn this off for val/test/predict dataloaders.\n",
            "\u001b[2m\u001b[36m(pid=1852)\u001b[0m   f'Your {mode}_dataloader has `shuffle=True`, it is best practice to turn'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1852)\u001b[0m \rValidation sanity check: 0it [00:00, ?it/s]\n",
            "\u001b[2m\u001b[36m(pid=1852)\u001b[0m \rValidation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1852)\u001b[0m /usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "\u001b[2m\u001b[36m(pid=1852)\u001b[0m   return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
            "\u001b[2m\u001b[36m(pid=1852)\u001b[0m /usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/deprecated_api.py:152: LightningDeprecationWarning: `Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\n",
            "\u001b[2m\u001b[36m(pid=1852)\u001b[0m   \"`Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1852)\u001b[0m \r                                                              \r\n",
            "Epoch 0:   0%|          | 0/1173 [00:00<?, ?it/s] \n",
            "Epoch 0:   2%|▏         | 20/1173 [00:06<05:59,  3.20it/s, loss=2.61, v_num=.]\n",
            "Epoch 0:   3%|▎         | 40/1173 [00:11<05:35,  3.38it/s, loss=3.03, v_num=.]\n",
            "Epoch 0:   5%|▌         | 60/1173 [00:17<05:28,  3.38it/s, loss=3.34, v_num=.]\n",
            "Epoch 0:   7%|▋         | 80/1173 [00:26<06:06,  2.98it/s, loss=3.55, v_num=.]\n",
            "Epoch 0:   9%|▊         | 100/1173 [00:36<06:27,  2.77it/s, loss=3.8, v_num=.] \n",
            "Epoch 0:  10%|█         | 120/1173 [00:45<06:40,  2.63it/s, loss=3.87, v_num=.]\n",
            "Epoch 0:  12%|█▏        | 140/1173 [00:55<06:48,  2.53it/s, loss=4, v_num=.]   \n",
            "Epoch 0:  14%|█▎        | 160/1173 [01:05<06:53,  2.45it/s, loss=3.99, v_num=.]\n",
            "Epoch 0:  15%|█▌        | 180/1173 [01:15<06:55,  2.39it/s, loss=4.05, v_num=.]\n",
            "Epoch 0:  17%|█▋        | 200/1173 [01:25<06:54,  2.35it/s, loss=4.11, v_num=.]\n",
            "Epoch 0:  19%|█▉        | 220/1173 [01:34<06:51,  2.32it/s, loss=4.16, v_num=.]\n",
            "Epoch 0:  20%|██        | 240/1173 [01:44<06:48,  2.29it/s, loss=4.21, v_num=.]\n",
            "Epoch 0:  22%|██▏       | 260/1173 [01:54<06:42,  2.27it/s, loss=4.22, v_num=.]\n",
            "Epoch 0:  24%|██▍       | 280/1173 [02:04<06:36,  2.25it/s, loss=4.23, v_num=.]\n",
            "Epoch 0:  26%|██▌       | 300/1173 [02:14<06:30,  2.24it/s, loss=4.26, v_num=.]\n",
            "Epoch 0:  27%|██▋       | 320/1173 [02:24<06:24,  2.22it/s, loss=4.25, v_num=.]\n",
            "Epoch 0:  29%|██▉       | 340/1173 [02:33<06:16,  2.21it/s, loss=4.31, v_num=.]\n",
            "Epoch 0:  31%|███       | 360/1173 [02:43<06:08,  2.21it/s, loss=4.28, v_num=.]\n",
            "Epoch 0:  32%|███▏      | 380/1173 [02:52<06:00,  2.20it/s, loss=4.33, v_num=.]\n",
            "Epoch 0:  34%|███▍      | 400/1173 [03:02<05:51,  2.20it/s, loss=4.37, v_num=.]\n",
            "Epoch 0:  36%|███▌      | 420/1173 [03:11<05:43,  2.19it/s, loss=4.36, v_num=.]\n",
            "Epoch 0:  38%|███▊      | 440/1173 [03:21<05:35,  2.19it/s, loss=4.43, v_num=.]\n",
            "Epoch 0:  39%|███▉      | 460/1173 [03:31<05:27,  2.18it/s, loss=4.4, v_num=.] \n",
            "Epoch 0:  41%|████      | 480/1173 [03:40<05:18,  2.17it/s, loss=4.37, v_num=.]\n",
            "Epoch 0:  43%|████▎     | 500/1173 [03:50<05:10,  2.17it/s, loss=4.41, v_num=.]\n",
            "Epoch 0:  44%|████▍     | 520/1173 [03:59<05:01,  2.17it/s, loss=4.47, v_num=.]\n",
            "Epoch 0:  46%|████▌     | 540/1173 [04:09<04:52,  2.16it/s, loss=4.46, v_num=.]\n",
            "Epoch 0:  48%|████▊     | 560/1173 [04:19<04:43,  2.16it/s, loss=4.43, v_num=.]\n",
            "Epoch 0:  49%|████▉     | 580/1173 [04:28<04:34,  2.16it/s, loss=4.45, v_num=.]\n",
            "Epoch 0:  51%|█████     | 600/1173 [04:38<04:25,  2.15it/s, loss=4.53, v_num=.]\n",
            "Epoch 0:  53%|█████▎    | 620/1173 [04:48<04:17,  2.15it/s, loss=4.54, v_num=.]\n",
            "Epoch 0:  55%|█████▍    | 640/1173 [04:57<04:08,  2.15it/s, loss=4.55, v_num=.]\n",
            "Epoch 0:  56%|█████▋    | 660/1173 [05:07<03:58,  2.15it/s, loss=4.52, v_num=.]\n",
            "Epoch 0:  58%|█████▊    | 680/1173 [05:17<03:49,  2.14it/s, loss=4.58, v_num=.]\n",
            "Epoch 0:  60%|█████▉    | 700/1173 [05:26<03:40,  2.14it/s, loss=4.61, v_num=.]\n",
            "Epoch 0:  61%|██████▏   | 720/1173 [05:36<03:31,  2.14it/s, loss=4.63, v_num=.]\n",
            "Epoch 0:  63%|██████▎   | 740/1173 [05:46<03:22,  2.14it/s, loss=4.63, v_num=.]\n",
            "Epoch 0:  65%|██████▍   | 760/1173 [05:55<03:13,  2.14it/s, loss=4.62, v_num=.]\n",
            "Epoch 0:  66%|██████▋   | 780/1173 [06:05<03:04,  2.14it/s, loss=4.64, v_num=.]\n",
            "Epoch 0:  68%|██████▊   | 800/1173 [06:14<02:54,  2.13it/s, loss=4.64, v_num=.]\n",
            "Epoch 0:  70%|██████▉   | 820/1173 [06:24<02:45,  2.13it/s, loss=4.65, v_num=.]\n",
            "Epoch 0:  72%|███████▏  | 840/1173 [06:33<02:36,  2.13it/s, loss=4.67, v_num=.]\n",
            "Epoch 0:  73%|███████▎  | 860/1173 [06:43<02:26,  2.13it/s, loss=4.67, v_num=.]\n",
            "Epoch 0:  75%|███████▌  | 880/1173 [06:53<02:17,  2.13it/s, loss=4.7, v_num=.] \n",
            "Epoch 0:  77%|███████▋  | 900/1173 [07:02<02:08,  2.13it/s, loss=4.62, v_num=.]\n",
            "Epoch 0:  78%|███████▊  | 920/1173 [07:12<01:58,  2.13it/s, loss=4.68, v_num=.]\n",
            "Epoch 0:  80%|████████  | 940/1173 [07:20<01:49,  2.13it/s, loss=4.68, v_num=.]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/235 [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[2m\u001b[36m(pid=1852)\u001b[0m \n",
            "Epoch 0:  82%|████████▏ | 960/1173 [07:29<01:39,  2.13it/s, loss=4.68, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1852)\u001b[0m \n",
            "Epoch 0:  84%|████████▎ | 980/1173 [07:39<01:30,  2.13it/s, loss=4.68, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1852)\u001b[0m \n",
            "Epoch 0:  85%|████████▌ | 1000/1173 [07:48<01:21,  2.13it/s, loss=4.68, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1852)\u001b[0m \n",
            "Epoch 0:  87%|████████▋ | 1020/1173 [07:58<01:11,  2.13it/s, loss=4.68, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1852)\u001b[0m \n",
            "Epoch 0:  89%|████████▊ | 1040/1173 [08:08<01:02,  2.13it/s, loss=4.68, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1852)\u001b[0m \n",
            "Epoch 0:  90%|█████████ | 1060/1173 [08:17<00:53,  2.13it/s, loss=4.68, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1852)\u001b[0m \n",
            "Epoch 0:  92%|█████████▏| 1080/1173 [08:27<00:43,  2.13it/s, loss=4.68, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1852)\u001b[0m \n",
            "Epoch 0:  94%|█████████▍| 1100/1173 [08:37<00:34,  2.13it/s, loss=4.68, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1852)\u001b[0m \n",
            "Epoch 0:  95%|█████████▌| 1120/1173 [08:46<00:24,  2.13it/s, loss=4.68, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1852)\u001b[0m \n",
            "Epoch 0:  97%|█████████▋| 1140/1173 [08:56<00:15,  2.13it/s, loss=4.68, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1852)\u001b[0m \n",
            "Epoch 0:  99%|█████████▉| 1160/1173 [09:06<00:06,  2.12it/s, loss=4.68, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1852)\u001b[0m \n",
            "Validating: 100%|██████████| 235/235 [01:52<00:00,  2.11it/s]\u001b[A\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1852)\u001b[0m /usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/deprecated_api.py:152: LightningDeprecationWarning: `Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\n",
            "\u001b[2m\u001b[36m(pid=1852)\u001b[0m   \"`Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for train_Stage1_tune_checkpoint_228f8_00003:\n",
            "  auroc: 0.500049889087677\n",
            "  auroc_cross: 0.49964848160743713\n",
            "  date: 2021-07-11_06-08-49\n",
            "  done: false\n",
            "  experiment_id: 85b52e42878e431c9780c84a9f24f97c\n",
            "  hostname: bd0789e721af\n",
            "  iterations_since_restore: 1\n",
            "  lossD: 0.00028767611365765333\n",
            "  lossG: 9.288405418395996\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 1852\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 571.7321963310242\n",
            "  time_this_iter_s: 571.7321963310242\n",
            "  time_total_s: 571.7321963310242\n",
            "  timestamp: 1625983729\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 1\n",
            "  trial_id: 228f8_00003\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2021-07-11 06:08:50,449\tWARNING util.py:162 -- The `callbacks.on_trial_result` operation took 0.722 s, which may be a performance bottleneck.\n",
            "2021-07-11 06:08:50,454\tWARNING util.py:162 -- The `process_trial_result` operation took 0.887 s, which may be a performance bottleneck.\n",
            "2021-07-11 06:08:50,460\tWARNING util.py:162 -- Processing trial results took 0.892 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
            "2021-07-11 06:08:50,464\tWARNING util.py:162 -- The `process_trial` operation took 0.897 s, which may be a performance bottleneck.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "== Status ==\n",
            "Memory usage on this node: 5.2/12.7 GiB\n",
            "PopulationBasedTraining: 2 checkpoints, 0 perturbs\n",
            "Resources requested: 2.0/2 CPUs, 1.0/1 GPUs, 0.0/7.51 GiB heap, 0.0/3.75 GiB objects (0.0/1.0 accelerator_type:T4, 0.0/2.0 CPU_group_ecd3f63ef35a3c653f2d69781ac9fe69, 0.0/2.0 CPU_group_0_ecd3f63ef35a3c653f2d69781ac9fe69, 0.0/1.0 GPU_group_0_ecd3f63ef35a3c653f2d69781ac9fe69, 0.0/1.0 GPU_group_ecd3f63ef35a3c653f2d69781ac9fe69)\n",
            "Current best trial: 228f8_00003 with auroc=0.500049889087677 and parameters={'learning_rate': 0.0001, 'ngf': 32, 'ndf': 64, 'batch_size': 64}\n",
            "Result logdir: /content/drive/MyDrive/Logs/Stage1_pbt_F\n",
            "Number of trials: 10/10 (3 PAUSED, 6 PENDING, 1 RUNNING)\n",
            "+------------------------------------------+----------+-----------------+-----------------+-------+-------+--------------+-----------+--------------+----------+---------------+----------------------+\n",
            "| Trial name                               | status   | loc             |   learning_rate |   ngf |   ndf |   batch_size |     lossG |        lossD |    auroc |   auroc_cross |   training_iteration |\n",
            "|------------------------------------------+----------+-----------------+-----------------+-------+-------+--------------+-----------+--------------+----------+---------------+----------------------|\n",
            "| train_Stage1_tune_checkpoint_228f8_00003 | RUNNING  | 172.28.0.2:1852 |          0.0001 |    32 |    64 |           64 |   9.28841 |  0.000287676 | 0.50005  |      0.499648 |                    1 |\n",
            "| train_Stage1_tune_checkpoint_228f8_00000 | PAUSED   |                 |          0.0001 |   128 |    32 |           64 |   5.04182 |  0.0528528   | 0.461844 |      0.473679 |                    1 |\n",
            "| train_Stage1_tune_checkpoint_228f8_00001 | PAUSED   |                 |          0.0001 |   128 |   128 |           64 | 100       | 79.1561      | 0.309535 |      0.381914 |                    1 |\n",
            "| train_Stage1_tune_checkpoint_228f8_00002 | PAUSED   |                 |          0.0001 |    32 |    64 |           64 |   8.43246 |  0.000612127 | 0.474643 |      0.489745 |                    1 |\n",
            "| train_Stage1_tune_checkpoint_228f8_00004 | PENDING  |                 |          0.0001 |    32 |    64 |           64 |           |              |          |               |                      |\n",
            "| train_Stage1_tune_checkpoint_228f8_00005 | PENDING  |                 |          0.0001 |   128 |    64 |           64 |           |              |          |               |                      |\n",
            "| train_Stage1_tune_checkpoint_228f8_00006 | PENDING  |                 |          0.0001 |    32 |    64 |           64 |           |              |          |               |                      |\n",
            "| train_Stage1_tune_checkpoint_228f8_00007 | PENDING  |                 |          0.0001 |   128 |    64 |           64 |           |              |          |               |                      |\n",
            "| train_Stage1_tune_checkpoint_228f8_00008 | PENDING  |                 |          0.0001 |    32 |    64 |           64 |           |              |          |               |                      |\n",
            "| train_Stage1_tune_checkpoint_228f8_00009 | PENDING  |                 |          0.0001 |   128 |    32 |           64 |           |              |          |               |                      |\n",
            "+------------------------------------------+----------+-----------------+-----------------+-------+-------+--------------+-----------+--------------+----------+---------------+----------------------+\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1852)\u001b[0m 2021-07-11 06:08:50,774\tINFO trainable.py:76 -- Checkpoint size is 135937282 bytes\n",
            "\u001b[2m\u001b[36m(pid=1852)\u001b[0m 2021-07-11 06:08:51,315\tINFO trainable.py:76 -- Checkpoint size is 135937282 bytes\n",
            "\u001b[2m\u001b[33m(raylet)\u001b[0m /usr/local/lib/python3.7/dist-packages/ray/autoscaler/_private/cli_logger.py:61: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
            "\u001b[2m\u001b[33m(raylet)\u001b[0m   \"update your install command.\", FutureWarning)\n",
            "\u001b[2m\u001b[36m(pid=2051)\u001b[0m GPU available: True, used: True\n",
            "\u001b[2m\u001b[36m(pid=2051)\u001b[0m TPU available: False, using: 0 TPU cores\n",
            "\u001b[2m\u001b[36m(pid=2051)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\u001b[2m\u001b[36m(pid=2051)\u001b[0m 2021-07-11 06:09:12.329882: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "\u001b[2m\u001b[36m(pid=2051)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=2051)\u001b[0m   | Name          | Type               | Params\n",
            "\u001b[2m\u001b[36m(pid=2051)\u001b[0m -----------------------------------------------------\n",
            "\u001b[2m\u001b[36m(pid=2051)\u001b[0m 0 | generator     | DCGANGenerator     | 1.1 M \n",
            "\u001b[2m\u001b[36m(pid=2051)\u001b[0m 1 | discriminator | DCGANDiscriminator | 2.8 M \n",
            "\u001b[2m\u001b[36m(pid=2051)\u001b[0m 2 | criterion     | BCELoss            | 0     \n",
            "\u001b[2m\u001b[36m(pid=2051)\u001b[0m 3 | modelF        | LensResnet         | 11.2 M\n",
            "\u001b[2m\u001b[36m(pid=2051)\u001b[0m 4 | modelJ        | LensResnet         | 11.2 M\n",
            "\u001b[2m\u001b[36m(pid=2051)\u001b[0m -----------------------------------------------------\n",
            "\u001b[2m\u001b[36m(pid=2051)\u001b[0m 26.2 M    Trainable params\n",
            "\u001b[2m\u001b[36m(pid=2051)\u001b[0m 0         Non-trainable params\n",
            "\u001b[2m\u001b[36m(pid=2051)\u001b[0m 26.2 M    Total params\n",
            "\u001b[2m\u001b[36m(pid=2051)\u001b[0m 104.826   Total estimated model params size (MB)\n",
            "\u001b[2m\u001b[36m(pid=2051)\u001b[0m /usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/data_loading.py:349: UserWarning: Your val_dataloader has `shuffle=True`, it is best practice to turn this off for val/test/predict dataloaders.\n",
            "\u001b[2m\u001b[36m(pid=2051)\u001b[0m   f'Your {mode}_dataloader has `shuffle=True`, it is best practice to turn'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=2051)\u001b[0m \rValidation sanity check: 0it [00:00, ?it/s]\rValidation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=2051)\u001b[0m /usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "\u001b[2m\u001b[36m(pid=2051)\u001b[0m   return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
            "\u001b[2m\u001b[36m(pid=2051)\u001b[0m /usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/deprecated_api.py:152: LightningDeprecationWarning: `Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\n",
            "\u001b[2m\u001b[36m(pid=2051)\u001b[0m   \"`Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=2051)\u001b[0m \r                                                              \r\n",
            "Epoch 0:   0%|          | 0/1173 [00:00<?, ?it/s] \n",
            "Epoch 0:   2%|▏         | 20/1173 [00:06<06:05,  3.15it/s, loss=2.71, v_num=.]\n",
            "Epoch 0:   3%|▎         | 40/1173 [00:12<05:44,  3.29it/s, loss=3.19, v_num=.]\n",
            "Epoch 0:   5%|▌         | 60/1173 [00:19<05:59,  3.09it/s, loss=3.47, v_num=.]\n",
            "Epoch 0:   7%|▋         | 80/1173 [00:28<06:33,  2.78it/s, loss=3.73, v_num=.]\n",
            "Epoch 0:   9%|▊         | 100/1173 [00:38<06:52,  2.60it/s, loss=3.96, v_num=.]\n",
            "Epoch 0:  10%|█         | 120/1173 [00:48<07:03,  2.49it/s, loss=3.96, v_num=.]\n",
            "Epoch 0:  12%|█▏        | 140/1173 [00:58<07:09,  2.40it/s, loss=4.07, v_num=.]\n",
            "Epoch 0:  14%|█▎        | 160/1173 [01:08<07:12,  2.34it/s, loss=4.13, v_num=.]\n",
            "Epoch 0:  15%|█▌        | 180/1173 [01:18<07:12,  2.30it/s, loss=4.21, v_num=.]\n",
            "Epoch 0:  17%|█▋        | 200/1173 [01:28<07:10,  2.26it/s, loss=4.23, v_num=.]\n",
            "Epoch 0:  19%|█▉        | 220/1173 [01:38<07:06,  2.24it/s, loss=4.38, v_num=.]\n",
            "Epoch 0:  20%|██        | 240/1173 [01:48<07:00,  2.22it/s, loss=4.33, v_num=.]\n",
            "Epoch 0:  22%|██▏       | 260/1173 [01:58<06:54,  2.20it/s, loss=4.37, v_num=.]\n",
            "Epoch 0:  24%|██▍       | 280/1173 [02:07<06:48,  2.19it/s, loss=4.42, v_num=.]\n",
            "Epoch 0:  26%|██▌       | 300/1173 [02:17<06:40,  2.18it/s, loss=4.38, v_num=.]\n",
            "Epoch 0:  27%|██▋       | 320/1173 [02:27<06:32,  2.17it/s, loss=4.41, v_num=.]\n",
            "Epoch 0:  29%|██▉       | 340/1173 [02:37<06:24,  2.16it/s, loss=4.4, v_num=.] \n",
            "Epoch 0:  31%|███       | 360/1173 [02:46<06:16,  2.16it/s, loss=4.45, v_num=.]\n",
            "Epoch 0:  32%|███▏      | 380/1173 [02:56<06:07,  2.16it/s, loss=4.49, v_num=.]\n",
            "Epoch 0:  34%|███▍      | 400/1173 [03:06<05:59,  2.15it/s, loss=4.49, v_num=.]\n",
            "Epoch 0:  36%|███▌      | 420/1173 [03:15<05:51,  2.14it/s, loss=4.49, v_num=.]\n",
            "Epoch 0:  38%|███▊      | 440/1173 [03:25<05:42,  2.14it/s, loss=4.52, v_num=.]\n",
            "Epoch 0:  39%|███▉      | 460/1173 [03:35<05:34,  2.13it/s, loss=4.55, v_num=.]\n",
            "Epoch 0:  41%|████      | 480/1173 [03:45<05:25,  2.13it/s, loss=4.55, v_num=.]\n",
            "Epoch 0:  43%|████▎     | 500/1173 [03:55<05:16,  2.13it/s, loss=4.53, v_num=.]\n",
            "Epoch 0:  44%|████▍     | 520/1173 [04:05<05:07,  2.12it/s, loss=4.57, v_num=.]\n",
            "Epoch 0:  46%|████▌     | 540/1173 [04:14<04:58,  2.12it/s, loss=4.61, v_num=.]\n",
            "Epoch 0:  48%|████▊     | 560/1173 [04:24<04:49,  2.12it/s, loss=4.61, v_num=.]\n",
            "Epoch 0:  49%|████▉     | 580/1173 [04:34<04:40,  2.11it/s, loss=4.63, v_num=.]\n",
            "Epoch 0:  51%|█████     | 600/1173 [04:43<04:31,  2.11it/s, loss=4.61, v_num=.]\n",
            "Epoch 0:  53%|█████▎    | 620/1173 [04:53<04:22,  2.11it/s, loss=4.65, v_num=.]\n",
            "Epoch 0:  55%|█████▍    | 640/1173 [05:04<04:13,  2.10it/s, loss=4.69, v_num=.]\n",
            "Epoch 0:  56%|█████▋    | 660/1173 [05:14<04:04,  2.10it/s, loss=4.66, v_num=.]\n",
            "Epoch 0:  58%|█████▊    | 680/1173 [05:24<03:54,  2.10it/s, loss=4.68, v_num=.]\n",
            "Epoch 0:  60%|█████▉    | 700/1173 [05:33<03:45,  2.10it/s, loss=4.75, v_num=.]\n",
            "Epoch 0:  61%|██████▏   | 720/1173 [05:43<03:36,  2.10it/s, loss=4.75, v_num=.]\n",
            "Epoch 0:  63%|██████▎   | 740/1173 [05:53<03:26,  2.10it/s, loss=4.79, v_num=.]\n",
            "Epoch 0:  65%|██████▍   | 760/1173 [06:02<03:17,  2.09it/s, loss=4.8, v_num=.] \n",
            "Epoch 0:  66%|██████▋   | 780/1173 [06:12<03:07,  2.09it/s, loss=4.8, v_num=.]\n",
            "Epoch 0:  68%|██████▊   | 800/1173 [06:22<02:58,  2.09it/s, loss=4.79, v_num=.]\n",
            "Epoch 0:  70%|██████▉   | 820/1173 [06:31<02:48,  2.09it/s, loss=4.84, v_num=.]\n",
            "Epoch 0:  72%|███████▏  | 840/1173 [06:41<02:39,  2.09it/s, loss=4.88, v_num=.]\n",
            "Epoch 0:  73%|███████▎  | 860/1173 [06:51<02:29,  2.09it/s, loss=4.91, v_num=.]\n",
            "Epoch 0:  75%|███████▌  | 880/1173 [07:00<02:20,  2.09it/s, loss=4.89, v_num=.]\n",
            "Epoch 0:  77%|███████▋  | 900/1173 [07:10<02:10,  2.09it/s, loss=4.85, v_num=.]\n",
            "Epoch 0:  78%|███████▊  | 920/1173 [07:20<02:01,  2.09it/s, loss=4.87, v_num=.]\n",
            "Epoch 0:  80%|████████  | 940/1173 [07:28<01:51,  2.10it/s, loss=4.87, v_num=.]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/235 [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[2m\u001b[36m(pid=2051)\u001b[0m \n",
            "Epoch 0:  82%|████████▏ | 960/1173 [07:38<01:41,  2.09it/s, loss=4.87, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=2051)\u001b[0m \n",
            "Epoch 0:  84%|████████▎ | 980/1173 [07:48<01:32,  2.09it/s, loss=4.87, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=2051)\u001b[0m \n",
            "Epoch 0:  85%|████████▌ | 1000/1173 [07:57<01:22,  2.09it/s, loss=4.87, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=2051)\u001b[0m \n",
            "Epoch 0:  87%|████████▋ | 1020/1173 [08:07<01:13,  2.09it/s, loss=4.87, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=2051)\u001b[0m \n",
            "Epoch 0:  89%|████████▊ | 1040/1173 [08:17<01:03,  2.09it/s, loss=4.87, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=2051)\u001b[0m \n",
            "Epoch 0:  90%|█████████ | 1060/1173 [08:26<00:54,  2.09it/s, loss=4.87, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=2051)\u001b[0m \n",
            "Epoch 0:  92%|█████████▏| 1080/1173 [08:36<00:44,  2.09it/s, loss=4.87, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=2051)\u001b[0m \n",
            "Epoch 0:  94%|█████████▍| 1100/1173 [08:46<00:34,  2.09it/s, loss=4.87, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=2051)\u001b[0m \n",
            "Epoch 0:  95%|█████████▌| 1120/1173 [08:55<00:25,  2.09it/s, loss=4.87, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=2051)\u001b[0m \n",
            "Epoch 0:  97%|█████████▋| 1140/1173 [09:05<00:15,  2.09it/s, loss=4.87, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=2051)\u001b[0m \n",
            "Epoch 0:  99%|█████████▉| 1160/1173 [09:15<00:06,  2.09it/s, loss=4.87, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=2051)\u001b[0m \n",
            "Validating: 100%|██████████| 235/235 [01:53<00:00,  2.10it/s]\u001b[A\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=2051)\u001b[0m /usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/deprecated_api.py:152: LightningDeprecationWarning: `Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\n",
            "\u001b[2m\u001b[36m(pid=2051)\u001b[0m   \"`Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for train_Stage1_tune_checkpoint_228f8_00004:\n",
            "  auroc: 0.4998002350330353\n",
            "  auroc_cross: 0.4958365857601166\n",
            "  date: 2021-07-11_06-18-41\n",
            "  done: false\n",
            "  experiment_id: b1f9e7363ff94f8686e051c875426844\n",
            "  hostname: bd0789e721af\n",
            "  iterations_since_restore: 1\n",
            "  lossD: 0.00012563096242956817\n",
            "  lossG: 9.67261028289795\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 2051\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 583.1923429965973\n",
            "  time_this_iter_s: 583.1923429965973\n",
            "  time_total_s: 583.1923429965973\n",
            "  timestamp: 1625984321\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 1\n",
            "  trial_id: 228f8_00004\n",
            "  \n",
            "== Status ==\n",
            "Memory usage on this node: 5.4/12.7 GiB\n",
            "PopulationBasedTraining: 3 checkpoints, 0 perturbs\n",
            "Resources requested: 2.0/2 CPUs, 1.0/1 GPUs, 0.0/7.51 GiB heap, 0.0/3.75 GiB objects (0.0/1.0 GPU_group_0_ecd3f63ef35a3c653f2d69781ac9fe69, 0.0/1.0 accelerator_type:T4, 0.0/2.0 CPU_group_ecd3f63ef35a3c653f2d69781ac9fe69, 0.0/1.0 GPU_group_ecd3f63ef35a3c653f2d69781ac9fe69, 0.0/2.0 CPU_group_0_ecd3f63ef35a3c653f2d69781ac9fe69)\n",
            "Current best trial: 228f8_00003 with auroc=0.500049889087677 and parameters={'learning_rate': 0.0001, 'ngf': 32, 'ndf': 64, 'batch_size': 64}\n",
            "Result logdir: /content/drive/MyDrive/Logs/Stage1_pbt_F\n",
            "Number of trials: 10/10 (4 PAUSED, 5 PENDING, 1 RUNNING)\n",
            "+------------------------------------------+----------+-----------------+-----------------+-------+-------+--------------+-----------+--------------+----------+---------------+----------------------+\n",
            "| Trial name                               | status   | loc             |   learning_rate |   ngf |   ndf |   batch_size |     lossG |        lossD |    auroc |   auroc_cross |   training_iteration |\n",
            "|------------------------------------------+----------+-----------------+-----------------+-------+-------+--------------+-----------+--------------+----------+---------------+----------------------|\n",
            "| train_Stage1_tune_checkpoint_228f8_00004 | RUNNING  | 172.28.0.2:2051 |          0.0001 |    32 |    64 |           64 |   9.67261 |  0.000125631 | 0.4998   |      0.495837 |                    1 |\n",
            "| train_Stage1_tune_checkpoint_228f8_00000 | PAUSED   |                 |          0.0001 |   128 |    32 |           64 |   5.04182 |  0.0528528   | 0.461844 |      0.473679 |                    1 |\n",
            "| train_Stage1_tune_checkpoint_228f8_00001 | PAUSED   |                 |          0.0001 |   128 |   128 |           64 | 100       | 79.1561      | 0.309535 |      0.381914 |                    1 |\n",
            "| train_Stage1_tune_checkpoint_228f8_00002 | PAUSED   |                 |          0.0001 |    32 |    64 |           64 |   8.43246 |  0.000612127 | 0.474643 |      0.489745 |                    1 |\n",
            "| train_Stage1_tune_checkpoint_228f8_00003 | PAUSED   |                 |          0.0001 |    32 |    64 |           64 |   9.28841 |  0.000287676 | 0.50005  |      0.499648 |                    1 |\n",
            "| train_Stage1_tune_checkpoint_228f8_00005 | PENDING  |                 |          0.0001 |   128 |    64 |           64 |           |              |          |               |                      |\n",
            "| train_Stage1_tune_checkpoint_228f8_00006 | PENDING  |                 |          0.0001 |    32 |    64 |           64 |           |              |          |               |                      |\n",
            "| train_Stage1_tune_checkpoint_228f8_00007 | PENDING  |                 |          0.0001 |   128 |    64 |           64 |           |              |          |               |                      |\n",
            "| train_Stage1_tune_checkpoint_228f8_00008 | PENDING  |                 |          0.0001 |    32 |    64 |           64 |           |              |          |               |                      |\n",
            "| train_Stage1_tune_checkpoint_228f8_00009 | PENDING  |                 |          0.0001 |   128 |    32 |           64 |           |              |          |               |                      |\n",
            "+------------------------------------------+----------+-----------------+-----------------+-------+-------+--------------+-----------+--------------+----------+---------------+----------------------+\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=2051)\u001b[0m 2021-07-11 06:18:42,361\tINFO trainable.py:76 -- Checkpoint size is 135937282 bytes\n",
            "\u001b[2m\u001b[36m(pid=2051)\u001b[0m 2021-07-11 06:18:42,860\tINFO trainable.py:76 -- Checkpoint size is 135937282 bytes\n",
            "\u001b[2m\u001b[33m(raylet)\u001b[0m /usr/local/lib/python3.7/dist-packages/ray/autoscaler/_private/cli_logger.py:61: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
            "\u001b[2m\u001b[33m(raylet)\u001b[0m   \"update your install command.\", FutureWarning)\n",
            "\u001b[2m\u001b[33m(raylet)\u001b[0m /usr/local/lib/python3.7/dist-packages/ray/autoscaler/_private/cli_logger.py:61: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
            "\u001b[2m\u001b[33m(raylet)\u001b[0m   \"update your install command.\", FutureWarning)\n",
            "\u001b[2m\u001b[36m(pid=2188)\u001b[0m GPU available: True, used: True\n",
            "\u001b[2m\u001b[36m(pid=2188)\u001b[0m TPU available: False, using: 0 TPU cores\n",
            "\u001b[2m\u001b[36m(pid=2188)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\u001b[2m\u001b[36m(pid=2188)\u001b[0m 2021-07-11 06:19:04.404932: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "\u001b[2m\u001b[36m(pid=2188)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=2188)\u001b[0m   | Name          | Type               | Params\n",
            "\u001b[2m\u001b[36m(pid=2188)\u001b[0m -----------------------------------------------------\n",
            "\u001b[2m\u001b[36m(pid=2188)\u001b[0m 0 | generator     | DCGANGenerator     | 12.7 M\n",
            "\u001b[2m\u001b[36m(pid=2188)\u001b[0m 1 | discriminator | DCGANDiscriminator | 2.8 M \n",
            "\u001b[2m\u001b[36m(pid=2188)\u001b[0m 2 | criterion     | BCELoss            | 0     \n",
            "\u001b[2m\u001b[36m(pid=2188)\u001b[0m 3 | modelF        | LensResnet         | 11.2 M\n",
            "\u001b[2m\u001b[36m(pid=2188)\u001b[0m 4 | modelJ        | LensResnet         | 11.2 M\n",
            "\u001b[2m\u001b[36m(pid=2188)\u001b[0m -----------------------------------------------------\n",
            "\u001b[2m\u001b[36m(pid=2188)\u001b[0m 37.8 M    Trainable params\n",
            "\u001b[2m\u001b[36m(pid=2188)\u001b[0m 0         Non-trainable params\n",
            "\u001b[2m\u001b[36m(pid=2188)\u001b[0m 37.8 M    Total params\n",
            "\u001b[2m\u001b[36m(pid=2188)\u001b[0m 151.047   Total estimated model params size (MB)\n",
            "\u001b[2m\u001b[36m(pid=2188)\u001b[0m /usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/data_loading.py:349: UserWarning: Your val_dataloader has `shuffle=True`, it is best practice to turn this off for val/test/predict dataloaders.\n",
            "\u001b[2m\u001b[36m(pid=2188)\u001b[0m   f'Your {mode}_dataloader has `shuffle=True`, it is best practice to turn'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=2188)\u001b[0m \rValidation sanity check: 0it [00:00, ?it/s]\rValidation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=2188)\u001b[0m /usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "\u001b[2m\u001b[36m(pid=2188)\u001b[0m   return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
            "\u001b[2m\u001b[36m(pid=2188)\u001b[0m /usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/deprecated_api.py:152: LightningDeprecationWarning: `Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\n",
            "\u001b[2m\u001b[36m(pid=2188)\u001b[0m   \"`Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0:   0%|          | 0/1173 [00:00<?, ?it/s] \n",
            "Epoch 0:   2%|▏         | 20/1173 [00:06<06:30,  2.95it/s, loss=2.85, v_num=.]\n",
            "Epoch 0:   3%|▎         | 40/1173 [00:12<06:00,  3.14it/s, loss=3.39, v_num=.]\n",
            "Epoch 0:   5%|▌         | 60/1173 [00:19<05:59,  3.09it/s, loss=3.62, v_num=.]\n",
            "Epoch 0:   7%|▋         | 80/1173 [00:28<06:31,  2.79it/s, loss=3.62, v_num=.]\n",
            "Epoch 0:   9%|▊         | 100/1173 [00:38<06:50,  2.61it/s, loss=3.65, v_num=.]\n",
            "Epoch 0:  10%|█         | 120/1173 [00:47<07:00,  2.50it/s, loss=3.56, v_num=.]\n",
            "Epoch 0:  12%|█▏        | 140/1173 [00:57<07:06,  2.42it/s, loss=3.65, v_num=.]\n",
            "Epoch 0:  14%|█▎        | 160/1173 [01:07<07:08,  2.37it/s, loss=5.15, v_num=.]\n",
            "Epoch 0:  15%|█▌        | 180/1173 [01:17<07:07,  2.32it/s, loss=3.55, v_num=.]\n",
            "Epoch 0:  17%|█▋        | 200/1173 [01:27<07:05,  2.29it/s, loss=3.43, v_num=.]\n",
            "Epoch 0:  19%|█▉        | 220/1173 [01:37<07:02,  2.26it/s, loss=2.87, v_num=.]\n",
            "Epoch 0:  20%|██        | 240/1173 [01:47<06:58,  2.23it/s, loss=2.89, v_num=.]\n",
            "Epoch 0:  22%|██▏       | 260/1173 [01:57<06:53,  2.21it/s, loss=3.04, v_num=.]\n",
            "Epoch 0:  24%|██▍       | 280/1173 [02:07<06:46,  2.19it/s, loss=3.15, v_num=.]\n",
            "Epoch 0:  26%|██▌       | 300/1173 [02:17<06:40,  2.18it/s, loss=3.23, v_num=.]\n",
            "Epoch 0:  27%|██▋       | 320/1173 [02:27<06:32,  2.17it/s, loss=3.49, v_num=.]\n",
            "Epoch 0:  29%|██▉       | 340/1173 [02:36<06:24,  2.17it/s, loss=3.5, v_num=.] \n",
            "Epoch 0:  31%|███       | 360/1173 [02:46<06:16,  2.16it/s, loss=3.38, v_num=.]\n",
            "Epoch 0:  32%|███▏      | 380/1173 [02:56<06:08,  2.15it/s, loss=3.48, v_num=.]\n",
            "Epoch 0:  34%|███▍      | 400/1173 [03:06<06:00,  2.14it/s, loss=3.5, v_num=.] \n",
            "Epoch 0:  36%|███▌      | 420/1173 [03:16<05:52,  2.14it/s, loss=3.43, v_num=.]\n",
            "Epoch 0:  38%|███▊      | 440/1173 [03:26<05:44,  2.13it/s, loss=3.5, v_num=.] \n",
            "Epoch 0:  39%|███▉      | 460/1173 [03:36<05:35,  2.12it/s, loss=3.47, v_num=.]\n",
            "Epoch 0:  41%|████      | 480/1173 [03:46<05:27,  2.12it/s, loss=3.48, v_num=.]\n",
            "Epoch 0:  43%|████▎     | 500/1173 [03:56<05:18,  2.11it/s, loss=3.43, v_num=.]\n",
            "Epoch 0:  44%|████▍     | 520/1173 [04:06<05:09,  2.11it/s, loss=3.51, v_num=.]\n",
            "Epoch 0:  46%|████▌     | 540/1173 [04:16<05:00,  2.11it/s, loss=3.43, v_num=.]\n",
            "Epoch 0:  48%|████▊     | 560/1173 [04:25<04:51,  2.11it/s, loss=3.42, v_num=.]\n",
            "Epoch 0:  49%|████▉     | 580/1173 [04:35<04:41,  2.10it/s, loss=3.41, v_num=.]\n",
            "Epoch 0:  51%|█████     | 600/1173 [04:45<04:32,  2.10it/s, loss=3.49, v_num=.]\n",
            "Epoch 0:  53%|█████▎    | 620/1173 [04:55<04:23,  2.10it/s, loss=3.5, v_num=.] \n",
            "Epoch 0:  55%|█████▍    | 640/1173 [05:05<04:14,  2.10it/s, loss=3.48, v_num=.]\n",
            "Epoch 0:  56%|█████▋    | 660/1173 [05:15<04:05,  2.09it/s, loss=3.35, v_num=.]\n",
            "Epoch 0:  58%|█████▊    | 680/1173 [05:24<03:55,  2.09it/s, loss=3.28, v_num=.]\n",
            "Epoch 0:  60%|█████▉    | 700/1173 [05:34<03:46,  2.09it/s, loss=50, v_num=.]  \n",
            "Epoch 0:  61%|██████▏   | 720/1173 [05:44<03:36,  2.09it/s, loss=50, v_num=.]\n",
            "Epoch 0:  63%|██████▎   | 740/1173 [05:54<03:27,  2.09it/s, loss=50, v_num=.]\n",
            "Epoch 0:  65%|██████▍   | 760/1173 [06:04<03:17,  2.09it/s, loss=50, v_num=.]\n",
            "Epoch 0:  66%|██████▋   | 780/1173 [06:14<03:08,  2.09it/s, loss=50, v_num=.]\n",
            "Epoch 0:  68%|██████▊   | 800/1173 [06:23<02:58,  2.08it/s, loss=50, v_num=.]\n",
            "Epoch 0:  70%|██████▉   | 820/1173 [06:33<02:49,  2.08it/s, loss=50, v_num=.]\n",
            "Epoch 0:  72%|███████▏  | 840/1173 [06:43<02:40,  2.08it/s, loss=50, v_num=.]\n",
            "Epoch 0:  73%|███████▎  | 860/1173 [06:53<02:30,  2.08it/s, loss=50, v_num=.]\n",
            "Epoch 0:  75%|███████▌  | 880/1173 [07:02<02:20,  2.08it/s, loss=50, v_num=.]\n",
            "Epoch 0:  77%|███████▋  | 900/1173 [07:12<02:11,  2.08it/s, loss=50, v_num=.]\n",
            "Epoch 0:  78%|███████▊  | 920/1173 [07:22<02:01,  2.08it/s, loss=50, v_num=.]\n",
            "Epoch 0:  80%|████████  | 940/1173 [07:31<01:51,  2.08it/s, loss=50, v_num=.]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/235 [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[2m\u001b[36m(pid=2188)\u001b[0m \n",
            "Epoch 0:  82%|████████▏ | 960/1173 [07:40<01:42,  2.08it/s, loss=50, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=2188)\u001b[0m \n",
            "Epoch 0:  84%|████████▎ | 980/1173 [07:50<01:32,  2.08it/s, loss=50, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=2188)\u001b[0m \n",
            "Epoch 0:  85%|████████▌ | 1000/1173 [08:00<01:23,  2.08it/s, loss=50, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=2188)\u001b[0m \n",
            "Epoch 0:  87%|████████▋ | 1020/1173 [08:10<01:13,  2.08it/s, loss=50, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=2188)\u001b[0m \n",
            "Epoch 0:  89%|████████▊ | 1040/1173 [08:19<01:03,  2.08it/s, loss=50, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=2188)\u001b[0m \n",
            "Epoch 0:  90%|█████████ | 1060/1173 [08:29<00:54,  2.08it/s, loss=50, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=2188)\u001b[0m \n",
            "Epoch 0:  92%|█████████▏| 1080/1173 [08:39<00:44,  2.08it/s, loss=50, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=2188)\u001b[0m \n",
            "Epoch 0:  94%|█████████▍| 1100/1173 [08:49<00:35,  2.08it/s, loss=50, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=2188)\u001b[0m \n",
            "Epoch 0:  95%|█████████▌| 1120/1173 [08:58<00:25,  2.08it/s, loss=50, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=2188)\u001b[0m \n",
            "Epoch 0:  97%|█████████▋| 1140/1173 [09:08<00:15,  2.08it/s, loss=50, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=2188)\u001b[0m \n",
            "Epoch 0:  99%|█████████▉| 1160/1173 [09:18<00:06,  2.08it/s, loss=50, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=2188)\u001b[0m \n",
            "Validating: 100%|██████████| 235/235 [01:54<00:00,  2.08it/s]\u001b[A\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=2188)\u001b[0m /usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/deprecated_api.py:152: LightningDeprecationWarning: `Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\n",
            "\u001b[2m\u001b[36m(pid=2188)\u001b[0m   \"`Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for train_Stage1_tune_checkpoint_228f8_00005:\n",
            "  auroc: 0.502286970615387\n",
            "  auroc_cross: 0.4919411540031433\n",
            "  date: 2021-07-11_06-28-39\n",
            "  done: false\n",
            "  experiment_id: 97f940ca42e54c6d8664a3c16e8ffffa\n",
            "  hostname: bd0789e721af\n",
            "  iterations_since_restore: 1\n",
            "  lossD: 100.0\n",
            "  lossG: 0.0\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 2188\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 589.414802312851\n",
            "  time_this_iter_s: 589.414802312851\n",
            "  time_total_s: 589.414802312851\n",
            "  timestamp: 1625984919\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 1\n",
            "  trial_id: 228f8_00005\n",
            "  \n",
            "== Status ==\n",
            "Memory usage on this node: 5.7/12.7 GiB\n",
            "PopulationBasedTraining: 4 checkpoints, 0 perturbs\n",
            "Resources requested: 2.0/2 CPUs, 1.0/1 GPUs, 0.0/7.51 GiB heap, 0.0/3.75 GiB objects (0.0/1.0 GPU_group_0_ecd3f63ef35a3c653f2d69781ac9fe69, 0.0/1.0 accelerator_type:T4, 0.0/1.0 GPU_group_ecd3f63ef35a3c653f2d69781ac9fe69, 0.0/2.0 CPU_group_ecd3f63ef35a3c653f2d69781ac9fe69, 0.0/2.0 CPU_group_0_ecd3f63ef35a3c653f2d69781ac9fe69)\n",
            "Current best trial: 228f8_00005 with auroc=0.502286970615387 and parameters={'learning_rate': 0.0001, 'ngf': 128, 'ndf': 64, 'batch_size': 64}\n",
            "Result logdir: /content/drive/MyDrive/Logs/Stage1_pbt_F\n",
            "Number of trials: 10/10 (5 PAUSED, 4 PENDING, 1 RUNNING)\n",
            "+------------------------------------------+----------+-----------------+-----------------+-------+-------+--------------+-----------+---------------+----------+---------------+----------------------+\n",
            "| Trial name                               | status   | loc             |   learning_rate |   ngf |   ndf |   batch_size |     lossG |         lossD |    auroc |   auroc_cross |   training_iteration |\n",
            "|------------------------------------------+----------+-----------------+-----------------+-------+-------+--------------+-----------+---------------+----------+---------------+----------------------|\n",
            "| train_Stage1_tune_checkpoint_228f8_00005 | RUNNING  | 172.28.0.2:2188 |          0.0001 |   128 |    64 |           64 |   0       | 100           | 0.502287 |      0.491941 |                    1 |\n",
            "| train_Stage1_tune_checkpoint_228f8_00000 | PAUSED   |                 |          0.0001 |   128 |    32 |           64 |   5.04182 |   0.0528528   | 0.461844 |      0.473679 |                    1 |\n",
            "| train_Stage1_tune_checkpoint_228f8_00001 | PAUSED   |                 |          0.0001 |   128 |   128 |           64 | 100       |  79.1561      | 0.309535 |      0.381914 |                    1 |\n",
            "| train_Stage1_tune_checkpoint_228f8_00002 | PAUSED   |                 |          0.0001 |    32 |    64 |           64 |   8.43246 |   0.000612127 | 0.474643 |      0.489745 |                    1 |\n",
            "| train_Stage1_tune_checkpoint_228f8_00003 | PAUSED   |                 |          0.0001 |    32 |    64 |           64 |   9.28841 |   0.000287676 | 0.50005  |      0.499648 |                    1 |\n",
            "| train_Stage1_tune_checkpoint_228f8_00004 | PAUSED   |                 |          0.0001 |    32 |    64 |           64 |   9.67261 |   0.000125631 | 0.4998   |      0.495837 |                    1 |\n",
            "| train_Stage1_tune_checkpoint_228f8_00006 | PENDING  |                 |          0.0001 |    32 |    64 |           64 |           |               |          |               |                      |\n",
            "| train_Stage1_tune_checkpoint_228f8_00007 | PENDING  |                 |          0.0001 |   128 |    64 |           64 |           |               |          |               |                      |\n",
            "| train_Stage1_tune_checkpoint_228f8_00008 | PENDING  |                 |          0.0001 |    32 |    64 |           64 |           |               |          |               |                      |\n",
            "| train_Stage1_tune_checkpoint_228f8_00009 | PENDING  |                 |          0.0001 |   128 |    32 |           64 |           |               |          |               |                      |\n",
            "+------------------------------------------+----------+-----------------+-----------------+-------+-------+--------------+-----------+---------------+----------+---------------+----------------------+\n",
            "\n",
            "\n",
            "Epoch 0: 100%|██████████| 1173/1173 [09:30<00:00,  2.06it/s, loss=50, v_num=.]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=2188)\u001b[0m 2021-07-11 06:28:40,749\tINFO trainable.py:76 -- Checkpoint size is 274610498 bytes\n",
            "\u001b[2m\u001b[36m(pid=2188)\u001b[0m 2021-07-11 06:28:41,805\tINFO trainable.py:76 -- Checkpoint size is 274610498 bytes\n",
            "\u001b[2m\u001b[36m(pid=2191)\u001b[0m GPU available: True, used: True\n",
            "\u001b[2m\u001b[36m(pid=2191)\u001b[0m TPU available: False, using: 0 TPU cores\n",
            "\u001b[2m\u001b[36m(pid=2191)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\u001b[2m\u001b[36m(pid=2191)\u001b[0m 2021-07-11 06:29:04.167224: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "\u001b[2m\u001b[36m(pid=2191)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=2191)\u001b[0m   | Name          | Type               | Params\n",
            "\u001b[2m\u001b[36m(pid=2191)\u001b[0m -----------------------------------------------------\n",
            "\u001b[2m\u001b[36m(pid=2191)\u001b[0m 0 | generator     | DCGANGenerator     | 1.1 M \n",
            "\u001b[2m\u001b[36m(pid=2191)\u001b[0m 1 | discriminator | DCGANDiscriminator | 2.8 M \n",
            "\u001b[2m\u001b[36m(pid=2191)\u001b[0m 2 | criterion     | BCELoss            | 0     \n",
            "\u001b[2m\u001b[36m(pid=2191)\u001b[0m 3 | modelF        | LensResnet         | 11.2 M\n",
            "\u001b[2m\u001b[36m(pid=2191)\u001b[0m 4 | modelJ        | LensResnet         | 11.2 M\n",
            "\u001b[2m\u001b[36m(pid=2191)\u001b[0m -----------------------------------------------------\n",
            "\u001b[2m\u001b[36m(pid=2191)\u001b[0m 26.2 M    Trainable params\n",
            "\u001b[2m\u001b[36m(pid=2191)\u001b[0m 0         Non-trainable params\n",
            "\u001b[2m\u001b[36m(pid=2191)\u001b[0m 26.2 M    Total params\n",
            "\u001b[2m\u001b[36m(pid=2191)\u001b[0m 104.826   Total estimated model params size (MB)\n",
            "\u001b[2m\u001b[36m(pid=2191)\u001b[0m /usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/data_loading.py:349: UserWarning: Your val_dataloader has `shuffle=True`, it is best practice to turn this off for val/test/predict dataloaders.\n",
            "\u001b[2m\u001b[36m(pid=2191)\u001b[0m   f'Your {mode}_dataloader has `shuffle=True`, it is best practice to turn'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=2191)\u001b[0m \rValidation sanity check: 0it [00:00, ?it/s]\rValidation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=2191)\u001b[0m /usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "\u001b[2m\u001b[36m(pid=2191)\u001b[0m   return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
            "\u001b[2m\u001b[36m(pid=2191)\u001b[0m /usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/deprecated_api.py:152: LightningDeprecationWarning: `Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\n",
            "\u001b[2m\u001b[36m(pid=2191)\u001b[0m   \"`Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=2191)\u001b[0m \r                                                              \r\n",
            "Epoch 0:   0%|          | 0/1173 [00:00<?, ?it/s] \n",
            "Epoch 0:   2%|▏         | 20/1173 [00:06<06:27,  2.98it/s, loss=2.46, v_num=.]\n",
            "Epoch 0:   3%|▎         | 40/1173 [00:12<06:03,  3.12it/s, loss=2.78, v_num=.]\n",
            "Epoch 0:   5%|▌         | 60/1173 [00:20<06:27,  2.87it/s, loss=3.12, v_num=.]\n",
            "Epoch 0:   7%|▋         | 80/1173 [00:31<07:06,  2.56it/s, loss=3.35, v_num=.]\n",
            "Epoch 0:   9%|▊         | 100/1173 [00:41<07:28,  2.39it/s, loss=3.58, v_num=.]\n",
            "Epoch 0:  10%|█         | 120/1173 [00:52<07:36,  2.31it/s, loss=3.78, v_num=.]\n",
            "Epoch 0:  12%|█▏        | 140/1173 [01:02<07:41,  2.24it/s, loss=3.76, v_num=.]\n",
            "Epoch 0:  14%|█▎        | 160/1173 [01:12<07:41,  2.20it/s, loss=3.84, v_num=.]\n",
            "Epoch 0:  15%|█▌        | 180/1173 [01:23<07:38,  2.17it/s, loss=3.81, v_num=.]\n",
            "Epoch 0:  17%|█▋        | 200/1173 [01:32<07:32,  2.15it/s, loss=3.87, v_num=.]\n",
            "Epoch 0:  19%|█▉        | 220/1173 [01:42<07:25,  2.14it/s, loss=3.91, v_num=.]\n",
            "Epoch 0:  20%|██        | 240/1173 [01:52<07:19,  2.12it/s, loss=3.96, v_num=.]\n",
            "Epoch 0:  22%|██▏       | 260/1173 [02:03<07:12,  2.11it/s, loss=3.98, v_num=.]\n",
            "Epoch 0:  24%|██▍       | 280/1173 [02:13<07:05,  2.10it/s, loss=4.06, v_num=.]\n",
            "Epoch 0:  26%|██▌       | 300/1173 [02:23<06:57,  2.09it/s, loss=4.1, v_num=.] \n",
            "Epoch 0:  27%|██▋       | 320/1173 [02:33<06:49,  2.08it/s, loss=4.09, v_num=.]\n",
            "Epoch 0:  29%|██▉       | 340/1173 [02:43<06:40,  2.08it/s, loss=4.14, v_num=.]\n",
            "Epoch 0:  31%|███       | 360/1173 [02:53<06:31,  2.08it/s, loss=4.13, v_num=.]\n",
            "Epoch 0:  32%|███▏      | 380/1173 [03:03<06:22,  2.07it/s, loss=4.12, v_num=.]\n",
            "Epoch 0:  34%|███▍      | 400/1173 [03:13<06:13,  2.07it/s, loss=4.1, v_num=.] \n",
            "Epoch 0:  36%|███▌      | 420/1173 [03:23<06:04,  2.07it/s, loss=4.13, v_num=.]\n",
            "Epoch 0:  38%|███▊      | 440/1173 [03:33<05:55,  2.06it/s, loss=4.17, v_num=.]\n",
            "Epoch 0:  39%|███▉      | 460/1173 [03:43<05:46,  2.06it/s, loss=4.19, v_num=.]\n",
            "Epoch 0:  41%|████      | 480/1173 [03:53<05:37,  2.05it/s, loss=4.18, v_num=.]\n",
            "Epoch 0:  43%|████▎     | 500/1173 [04:03<05:27,  2.05it/s, loss=4.14, v_num=.]\n",
            "Epoch 0:  44%|████▍     | 520/1173 [04:13<05:18,  2.05it/s, loss=4.2, v_num=.] \n",
            "Epoch 0:  46%|████▌     | 540/1173 [04:23<05:09,  2.05it/s, loss=4.2, v_num=.]\n",
            "Epoch 0:  48%|████▊     | 560/1173 [04:33<04:59,  2.05it/s, loss=4.23, v_num=.]\n",
            "Epoch 0:  49%|████▉     | 580/1173 [04:43<04:50,  2.04it/s, loss=4.24, v_num=.]\n",
            "Epoch 0:  51%|█████     | 600/1173 [04:53<04:40,  2.04it/s, loss=4.3, v_num=.] \n",
            "Epoch 0:  53%|█████▎    | 620/1173 [05:03<04:30,  2.04it/s, loss=4.33, v_num=.]\n",
            "Epoch 0:  55%|█████▍    | 640/1173 [05:13<04:21,  2.04it/s, loss=4.37, v_num=.]\n",
            "Epoch 0:  56%|█████▋    | 660/1173 [05:23<04:11,  2.04it/s, loss=4.33, v_num=.]\n",
            "Epoch 0:  58%|█████▊    | 680/1173 [05:34<04:02,  2.04it/s, loss=4.37, v_num=.]\n",
            "Epoch 0:  60%|█████▉    | 700/1173 [05:44<03:52,  2.03it/s, loss=4.36, v_num=.]\n",
            "Epoch 0:  61%|██████▏   | 720/1173 [05:54<03:42,  2.03it/s, loss=4.43, v_num=.]\n",
            "Epoch 0:  63%|██████▎   | 740/1173 [06:04<03:33,  2.03it/s, loss=4.41, v_num=.]\n",
            "Epoch 0:  65%|██████▍   | 760/1173 [06:14<03:23,  2.03it/s, loss=4.42, v_num=.]\n",
            "Epoch 0:  66%|██████▋   | 780/1173 [06:23<03:13,  2.03it/s, loss=4.48, v_num=.]\n",
            "Epoch 0:  68%|██████▊   | 800/1173 [06:33<03:03,  2.03it/s, loss=4.48, v_num=.]\n",
            "Epoch 0:  70%|██████▉   | 820/1173 [06:43<02:53,  2.03it/s, loss=4.47, v_num=.]\n",
            "Epoch 0:  72%|███████▏  | 840/1173 [06:53<02:43,  2.03it/s, loss=4.45, v_num=.]\n",
            "Epoch 0:  73%|███████▎  | 860/1173 [07:03<02:34,  2.03it/s, loss=4.48, v_num=.]\n",
            "Epoch 0:  75%|███████▌  | 880/1173 [07:13<02:24,  2.03it/s, loss=4.49, v_num=.]\n",
            "Epoch 0:  77%|███████▋  | 900/1173 [07:23<02:14,  2.03it/s, loss=4.47, v_num=.]\n",
            "Epoch 0:  78%|███████▊  | 920/1173 [07:33<02:04,  2.03it/s, loss=4.53, v_num=.]\n",
            "Epoch 0:  80%|████████  | 940/1173 [07:42<01:54,  2.03it/s, loss=4.53, v_num=.]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/235 [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[2m\u001b[36m(pid=2191)\u001b[0m \n",
            "Epoch 0:  82%|████████▏ | 960/1173 [07:52<01:44,  2.03it/s, loss=4.53, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=2191)\u001b[0m \n",
            "Epoch 0:  84%|████████▎ | 980/1173 [08:02<01:34,  2.03it/s, loss=4.53, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=2191)\u001b[0m \n",
            "Epoch 0:  85%|████████▌ | 1000/1173 [08:12<01:25,  2.03it/s, loss=4.53, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=2191)\u001b[0m \n",
            "Epoch 0:  87%|████████▋ | 1020/1173 [08:21<01:15,  2.03it/s, loss=4.53, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=2191)\u001b[0m \n",
            "Epoch 0:  89%|████████▊ | 1040/1173 [08:31<01:05,  2.03it/s, loss=4.53, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=2191)\u001b[0m \n",
            "Epoch 0:  90%|█████████ | 1060/1173 [08:41<00:55,  2.03it/s, loss=4.53, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=2191)\u001b[0m \n",
            "Epoch 0:  92%|█████████▏| 1080/1173 [08:51<00:45,  2.03it/s, loss=4.53, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=2191)\u001b[0m \n",
            "Epoch 0:  94%|█████████▍| 1100/1173 [09:01<00:35,  2.03it/s, loss=4.53, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=2191)\u001b[0m \n",
            "Epoch 0:  95%|█████████▌| 1120/1173 [09:11<00:26,  2.03it/s, loss=4.53, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=2191)\u001b[0m \n",
            "Epoch 0:  97%|█████████▋| 1140/1173 [09:21<00:16,  2.03it/s, loss=4.53, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=2191)\u001b[0m \n",
            "Epoch 0:  99%|█████████▉| 1160/1173 [09:31<00:06,  2.03it/s, loss=4.53, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=2191)\u001b[0m \n",
            "Validating: 100%|██████████| 235/235 [01:56<00:00,  2.04it/s]\u001b[A\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=2191)\u001b[0m /usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/deprecated_api.py:152: LightningDeprecationWarning: `Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\n",
            "\u001b[2m\u001b[36m(pid=2191)\u001b[0m   \"`Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for train_Stage1_tune_checkpoint_228f8_00006:\n",
            "  auroc: 0.5008350610733032\n",
            "  auroc_cross: 0.49981212615966797\n",
            "  date: 2021-07-11_06-38-49\n",
            "  done: false\n",
            "  experiment_id: 248d66ec7739450ba44e779c8ce972ca\n",
            "  hostname: bd0789e721af\n",
            "  iterations_since_restore: 1\n",
            "  lossD: 0.0002792597806546837\n",
            "  lossG: 8.948115348815918\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 2191\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 598.3700771331787\n",
            "  time_this_iter_s: 598.3700771331787\n",
            "  time_total_s: 598.3700771331787\n",
            "  timestamp: 1625985529\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 1\n",
            "  trial_id: 228f8_00006\n",
            "  \n",
            "== Status ==\n",
            "Memory usage on this node: 6.2/12.7 GiB\n",
            "PopulationBasedTraining: 5 checkpoints, 0 perturbs\n",
            "Resources requested: 2.0/2 CPUs, 1.0/1 GPUs, 0.0/7.51 GiB heap, 0.0/3.75 GiB objects (0.0/1.0 GPU_group_ecd3f63ef35a3c653f2d69781ac9fe69, 0.0/1.0 accelerator_type:T4, 0.0/1.0 GPU_group_0_ecd3f63ef35a3c653f2d69781ac9fe69, 0.0/2.0 CPU_group_ecd3f63ef35a3c653f2d69781ac9fe69, 0.0/2.0 CPU_group_0_ecd3f63ef35a3c653f2d69781ac9fe69)\n",
            "Current best trial: 228f8_00005 with auroc=0.502286970615387 and parameters={'learning_rate': 0.0001, 'ngf': 128, 'ndf': 64, 'batch_size': 64}\n",
            "Result logdir: /content/drive/MyDrive/Logs/Stage1_pbt_F\n",
            "Number of trials: 10/10 (6 PAUSED, 3 PENDING, 1 RUNNING)\n",
            "+------------------------------------------+----------+-----------------+-----------------+-------+-------+--------------+-----------+---------------+----------+---------------+----------------------+\n",
            "| Trial name                               | status   | loc             |   learning_rate |   ngf |   ndf |   batch_size |     lossG |         lossD |    auroc |   auroc_cross |   training_iteration |\n",
            "|------------------------------------------+----------+-----------------+-----------------+-------+-------+--------------+-----------+---------------+----------+---------------+----------------------|\n",
            "| train_Stage1_tune_checkpoint_228f8_00006 | RUNNING  | 172.28.0.2:2191 |          0.0001 |    32 |    64 |           64 |   8.94812 |   0.00027926  | 0.500835 |      0.499812 |                    1 |\n",
            "| train_Stage1_tune_checkpoint_228f8_00000 | PAUSED   |                 |          0.0001 |   128 |    32 |           64 |   5.04182 |   0.0528528   | 0.461844 |      0.473679 |                    1 |\n",
            "| train_Stage1_tune_checkpoint_228f8_00001 | PAUSED   |                 |          0.0001 |   128 |   128 |           64 | 100       |  79.1561      | 0.309535 |      0.381914 |                    1 |\n",
            "| train_Stage1_tune_checkpoint_228f8_00002 | PAUSED   |                 |          0.0001 |    32 |    64 |           64 |   8.43246 |   0.000612127 | 0.474643 |      0.489745 |                    1 |\n",
            "| train_Stage1_tune_checkpoint_228f8_00003 | PAUSED   |                 |          0.0001 |    32 |    64 |           64 |   9.28841 |   0.000287676 | 0.50005  |      0.499648 |                    1 |\n",
            "| train_Stage1_tune_checkpoint_228f8_00004 | PAUSED   |                 |          0.0001 |    32 |    64 |           64 |   9.67261 |   0.000125631 | 0.4998   |      0.495837 |                    1 |\n",
            "| train_Stage1_tune_checkpoint_228f8_00005 | PAUSED   |                 |          0.0001 |   128 |    64 |           64 |   0       | 100           | 0.502287 |      0.491941 |                    1 |\n",
            "| train_Stage1_tune_checkpoint_228f8_00007 | PENDING  |                 |          0.0001 |   128 |    64 |           64 |           |               |          |               |                      |\n",
            "| train_Stage1_tune_checkpoint_228f8_00008 | PENDING  |                 |          0.0001 |    32 |    64 |           64 |           |               |          |               |                      |\n",
            "| train_Stage1_tune_checkpoint_228f8_00009 | PENDING  |                 |          0.0001 |   128 |    32 |           64 |           |               |          |               |                      |\n",
            "+------------------------------------------+----------+-----------------+-----------------+-------+-------+--------------+-----------+---------------+----------+---------------+----------------------+\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=2191)\u001b[0m 2021-07-11 06:38:50,267\tINFO trainable.py:76 -- Checkpoint size is 135937282 bytes\n",
            "\u001b[2m\u001b[36m(pid=2191)\u001b[0m 2021-07-11 06:38:50,810\tINFO trainable.py:76 -- Checkpoint size is 135937282 bytes\n",
            "\u001b[2m\u001b[33m(raylet)\u001b[0m /usr/local/lib/python3.7/dist-packages/ray/autoscaler/_private/cli_logger.py:61: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
            "\u001b[2m\u001b[33m(raylet)\u001b[0m   \"update your install command.\", FutureWarning)\n",
            "\u001b[2m\u001b[36m(pid=2521)\u001b[0m GPU available: True, used: True\n",
            "\u001b[2m\u001b[36m(pid=2521)\u001b[0m TPU available: False, using: 0 TPU cores\n",
            "\u001b[2m\u001b[36m(pid=2521)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\u001b[2m\u001b[36m(pid=2521)\u001b[0m 2021-07-11 06:39:13.352508: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "\u001b[2m\u001b[36m(pid=2521)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=2521)\u001b[0m   | Name          | Type               | Params\n",
            "\u001b[2m\u001b[36m(pid=2521)\u001b[0m -----------------------------------------------------\n",
            "\u001b[2m\u001b[36m(pid=2521)\u001b[0m 0 | generator     | DCGANGenerator     | 12.7 M\n",
            "\u001b[2m\u001b[36m(pid=2521)\u001b[0m 1 | discriminator | DCGANDiscriminator | 2.8 M \n",
            "\u001b[2m\u001b[36m(pid=2521)\u001b[0m 2 | criterion     | BCELoss            | 0     \n",
            "\u001b[2m\u001b[36m(pid=2521)\u001b[0m 3 | modelF        | LensResnet         | 11.2 M\n",
            "\u001b[2m\u001b[36m(pid=2521)\u001b[0m 4 | modelJ        | LensResnet         | 11.2 M\n",
            "\u001b[2m\u001b[36m(pid=2521)\u001b[0m -----------------------------------------------------\n",
            "\u001b[2m\u001b[36m(pid=2521)\u001b[0m 37.8 M    Trainable params\n",
            "\u001b[2m\u001b[36m(pid=2521)\u001b[0m 0         Non-trainable params\n",
            "\u001b[2m\u001b[36m(pid=2521)\u001b[0m 37.8 M    Total params\n",
            "\u001b[2m\u001b[36m(pid=2521)\u001b[0m 151.047   Total estimated model params size (MB)\n",
            "\u001b[2m\u001b[36m(pid=2521)\u001b[0m /usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/data_loading.py:349: UserWarning: Your val_dataloader has `shuffle=True`, it is best practice to turn this off for val/test/predict dataloaders.\n",
            "\u001b[2m\u001b[36m(pid=2521)\u001b[0m   f'Your {mode}_dataloader has `shuffle=True`, it is best practice to turn'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=2521)\u001b[0m \rValidation sanity check: 0it [00:00, ?it/s]\rValidation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=2521)\u001b[0m /usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "\u001b[2m\u001b[36m(pid=2521)\u001b[0m   return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
            "\u001b[2m\u001b[36m(pid=2521)\u001b[0m /usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/deprecated_api.py:152: LightningDeprecationWarning: `Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\n",
            "\u001b[2m\u001b[36m(pid=2521)\u001b[0m   \"`Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0:   0%|          | 0/1173 [00:00<?, ?it/s] \n",
            "Epoch 0:   2%|▏         | 20/1173 [00:06<06:27,  2.98it/s, loss=2.91, v_num=.]\n",
            "Epoch 0:   3%|▎         | 40/1173 [00:15<07:22,  2.56it/s, loss=3.32, v_num=.]\n",
            "Epoch 0:   5%|▌         | 60/1173 [00:25<07:56,  2.33it/s, loss=3.54, v_num=.]\n",
            "Epoch 0:   7%|▋         | 80/1173 [00:36<08:12,  2.22it/s, loss=3.65, v_num=.]\n",
            "Epoch 0:   9%|▊         | 100/1173 [00:46<08:18,  2.15it/s, loss=3.69, v_num=.]\n",
            "Epoch 0:  10%|█         | 120/1173 [00:57<08:20,  2.10it/s, loss=3.61, v_num=.]\n",
            "Epoch 0:  12%|█▏        | 140/1173 [01:07<08:20,  2.06it/s, loss=3.59, v_num=.]\n",
            "Epoch 0:  14%|█▎        | 160/1173 [01:18<08:14,  2.05it/s, loss=3.62, v_num=.]\n",
            "Epoch 0:  15%|█▌        | 180/1173 [01:28<08:08,  2.03it/s, loss=3.57, v_num=.]\n",
            "Epoch 0:  17%|█▋        | 200/1173 [01:38<08:00,  2.03it/s, loss=3.58, v_num=.]\n",
            "Epoch 0:  19%|█▉        | 220/1173 [01:48<07:51,  2.02it/s, loss=3.58, v_num=.]\n",
            "Epoch 0:  20%|██        | 240/1173 [01:58<07:42,  2.02it/s, loss=3.54, v_num=.]\n",
            "Epoch 0:  22%|██▏       | 260/1173 [02:08<07:32,  2.02it/s, loss=3.64, v_num=.]\n",
            "Epoch 0:  24%|██▍       | 280/1173 [02:19<07:23,  2.01it/s, loss=3.64, v_num=.]\n",
            "Epoch 0:  26%|██▌       | 300/1173 [02:29<07:14,  2.01it/s, loss=3.67, v_num=.]\n",
            "Epoch 0:  27%|██▋       | 320/1173 [02:39<07:04,  2.01it/s, loss=3.7, v_num=.] \n",
            "Epoch 0:  29%|██▉       | 340/1173 [02:49<06:54,  2.01it/s, loss=3.6, v_num=.]\n",
            "Epoch 0:  31%|███       | 360/1173 [02:59<06:45,  2.00it/s, loss=3.62, v_num=.]\n",
            "Epoch 0:  32%|███▏      | 380/1173 [03:09<06:36,  2.00it/s, loss=3.66, v_num=.]\n",
            "Epoch 0:  34%|███▍      | 400/1173 [03:20<06:26,  2.00it/s, loss=50, v_num=.]  \n",
            "Epoch 0:  36%|███▌      | 420/1173 [03:30<06:17,  2.00it/s, loss=49.9, v_num=.]\n",
            "Epoch 0:  38%|███▊      | 440/1173 [03:40<06:07,  1.99it/s, loss=49.7, v_num=.]\n",
            "Epoch 0:  39%|███▉      | 460/1173 [03:50<05:57,  1.99it/s, loss=2.68, v_num=.]\n",
            "Epoch 0:  41%|████      | 480/1173 [04:01<05:48,  1.99it/s, loss=3.6, v_num=.] \n",
            "Epoch 0:  43%|████▎     | 500/1173 [04:11<05:38,  1.99it/s, loss=3.15, v_num=.]\n",
            "Epoch 0:  44%|████▍     | 520/1173 [04:21<05:28,  1.99it/s, loss=3.5, v_num=.] \n",
            "Epoch 0:  46%|████▌     | 540/1173 [04:31<05:18,  1.99it/s, loss=3.42, v_num=.]\n",
            "Epoch 0:  48%|████▊     | 560/1173 [04:41<05:08,  1.99it/s, loss=3.77, v_num=.]\n",
            "Epoch 0:  49%|████▉     | 580/1173 [04:51<04:58,  1.99it/s, loss=3.32, v_num=.]\n",
            "Epoch 0:  51%|█████     | 600/1173 [05:02<04:48,  1.99it/s, loss=3.42, v_num=.]\n",
            "Epoch 0:  53%|█████▎    | 620/1173 [05:12<04:38,  1.98it/s, loss=3.4, v_num=.] \n",
            "Epoch 0:  55%|█████▍    | 640/1173 [05:22<04:28,  1.98it/s, loss=3.4, v_num=.]\n",
            "Epoch 0:  56%|█████▋    | 660/1173 [05:32<04:18,  1.98it/s, loss=3.45, v_num=.]\n",
            "Epoch 0:  58%|█████▊    | 680/1173 [05:42<04:08,  1.98it/s, loss=3.43, v_num=.]\n",
            "Epoch 0:  60%|█████▉    | 700/1173 [05:53<03:58,  1.98it/s, loss=3.33, v_num=.]\n",
            "Epoch 0:  61%|██████▏   | 720/1173 [06:03<03:48,  1.98it/s, loss=3.34, v_num=.]\n",
            "Epoch 0:  63%|██████▎   | 740/1173 [06:13<03:38,  1.98it/s, loss=3.36, v_num=.]\n",
            "Epoch 0:  65%|██████▍   | 760/1173 [06:24<03:28,  1.98it/s, loss=3.24, v_num=.]\n",
            "Epoch 0:  66%|██████▋   | 780/1173 [06:34<03:18,  1.98it/s, loss=3.14, v_num=.]\n",
            "Epoch 0:  68%|██████▊   | 800/1173 [06:45<03:08,  1.98it/s, loss=2.09, v_num=.]\n",
            "Epoch 0:  70%|██████▉   | 820/1173 [06:55<02:58,  1.97it/s, loss=2.33, v_num=.]\n",
            "Epoch 0:  72%|███████▏  | 840/1173 [07:05<02:48,  1.97it/s, loss=2.46, v_num=.]\n",
            "Epoch 0:  73%|███████▎  | 860/1173 [07:15<02:38,  1.97it/s, loss=2.58, v_num=.]\n",
            "Epoch 0:  75%|███████▌  | 880/1173 [07:25<02:28,  1.97it/s, loss=2.79, v_num=.]\n",
            "Epoch 0:  77%|███████▋  | 900/1173 [07:35<02:18,  1.97it/s, loss=2.73, v_num=.]\n",
            "Epoch 0:  78%|███████▊  | 920/1173 [07:45<02:08,  1.98it/s, loss=2.82, v_num=.]\n",
            "Epoch 0:  80%|████████  | 940/1173 [07:54<01:57,  1.98it/s, loss=2.82, v_num=.]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/235 [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[2m\u001b[36m(pid=2521)\u001b[0m \n",
            "Epoch 0:  82%|████████▏ | 960/1173 [08:03<01:47,  1.98it/s, loss=2.82, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=2521)\u001b[0m \n",
            "Epoch 0:  84%|████████▎ | 980/1173 [08:14<01:37,  1.98it/s, loss=2.82, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=2521)\u001b[0m \n",
            "Epoch 0:  85%|████████▌ | 1000/1173 [08:24<01:27,  1.98it/s, loss=2.82, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=2521)\u001b[0m \n",
            "Epoch 0:  87%|████████▋ | 1020/1173 [08:34<01:17,  1.98it/s, loss=2.82, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=2521)\u001b[0m \n",
            "Epoch 0:  89%|████████▊ | 1040/1173 [08:44<01:07,  1.98it/s, loss=2.82, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=2521)\u001b[0m \n",
            "Epoch 0:  90%|█████████ | 1060/1173 [08:54<00:56,  1.98it/s, loss=2.82, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=2521)\u001b[0m \n",
            "Epoch 0:  92%|█████████▏| 1080/1173 [09:04<00:46,  1.98it/s, loss=2.82, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=2521)\u001b[0m \n",
            "Epoch 0:  94%|█████████▍| 1100/1173 [09:15<00:36,  1.98it/s, loss=2.82, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=2521)\u001b[0m \n",
            "Epoch 0:  95%|█████████▌| 1120/1173 [09:25<00:26,  1.98it/s, loss=2.82, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=2521)\u001b[0m \n",
            "Epoch 0:  97%|█████████▋| 1140/1173 [09:35<00:16,  1.98it/s, loss=2.82, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=2521)\u001b[0m \n",
            "Epoch 0:  99%|█████████▉| 1160/1173 [09:45<00:06,  1.98it/s, loss=2.82, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=2521)\u001b[0m \n",
            "Epoch 0: 100%|██████████| 1173/1173 [09:52<00:00,  1.98it/s, loss=2.82, v_num=.]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=2521)\u001b[0m /usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/deprecated_api.py:152: LightningDeprecationWarning: `Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\n",
            "\u001b[2m\u001b[36m(pid=2521)\u001b[0m   \"`Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for train_Stage1_tune_checkpoint_228f8_00007:\n",
            "  auroc: 0.5541192889213562\n",
            "  auroc_cross: 0.5520529747009277\n",
            "  date: 2021-07-11_06-49-14\n",
            "  done: false\n",
            "  experiment_id: 20de904516d8412688144064562e4339\n",
            "  hostname: bd0789e721af\n",
            "  iterations_since_restore: 1\n",
            "  lossD: 0.012883458286523819\n",
            "  lossG: 5.992850303649902\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 2521\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 615.6806035041809\n",
            "  time_this_iter_s: 615.6806035041809\n",
            "  time_total_s: 615.6806035041809\n",
            "  timestamp: 1625986154\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 1\n",
            "  trial_id: 228f8_00007\n",
            "  \n",
            "== Status ==\n",
            "Memory usage on this node: 6.4/12.7 GiB\n",
            "PopulationBasedTraining: 6 checkpoints, 0 perturbs\n",
            "Resources requested: 2.0/2 CPUs, 1.0/1 GPUs, 0.0/7.51 GiB heap, 0.0/3.75 GiB objects (0.0/1.0 GPU_group_ecd3f63ef35a3c653f2d69781ac9fe69, 0.0/2.0 CPU_group_0_ecd3f63ef35a3c653f2d69781ac9fe69, 0.0/1.0 GPU_group_0_ecd3f63ef35a3c653f2d69781ac9fe69, 0.0/1.0 accelerator_type:T4, 0.0/2.0 CPU_group_ecd3f63ef35a3c653f2d69781ac9fe69)\n",
            "Current best trial: 228f8_00007 with auroc=0.5541192889213562 and parameters={'learning_rate': 0.0001, 'ngf': 128, 'ndf': 64, 'batch_size': 64}\n",
            "Result logdir: /content/drive/MyDrive/Logs/Stage1_pbt_F\n",
            "Number of trials: 10/10 (7 PAUSED, 2 PENDING, 1 RUNNING)\n",
            "+------------------------------------------+----------+-----------------+-----------------+-------+-------+--------------+-----------+---------------+----------+---------------+----------------------+\n",
            "| Trial name                               | status   | loc             |   learning_rate |   ngf |   ndf |   batch_size |     lossG |         lossD |    auroc |   auroc_cross |   training_iteration |\n",
            "|------------------------------------------+----------+-----------------+-----------------+-------+-------+--------------+-----------+---------------+----------+---------------+----------------------|\n",
            "| train_Stage1_tune_checkpoint_228f8_00007 | RUNNING  | 172.28.0.2:2521 |          0.0001 |   128 |    64 |           64 |   5.99285 |   0.0128835   | 0.554119 |      0.552053 |                    1 |\n",
            "| train_Stage1_tune_checkpoint_228f8_00000 | PAUSED   |                 |          0.0001 |   128 |    32 |           64 |   5.04182 |   0.0528528   | 0.461844 |      0.473679 |                    1 |\n",
            "| train_Stage1_tune_checkpoint_228f8_00001 | PAUSED   |                 |          0.0001 |   128 |   128 |           64 | 100       |  79.1561      | 0.309535 |      0.381914 |                    1 |\n",
            "| train_Stage1_tune_checkpoint_228f8_00002 | PAUSED   |                 |          0.0001 |    32 |    64 |           64 |   8.43246 |   0.000612127 | 0.474643 |      0.489745 |                    1 |\n",
            "| train_Stage1_tune_checkpoint_228f8_00003 | PAUSED   |                 |          0.0001 |    32 |    64 |           64 |   9.28841 |   0.000287676 | 0.50005  |      0.499648 |                    1 |\n",
            "| train_Stage1_tune_checkpoint_228f8_00004 | PAUSED   |                 |          0.0001 |    32 |    64 |           64 |   9.67261 |   0.000125631 | 0.4998   |      0.495837 |                    1 |\n",
            "| train_Stage1_tune_checkpoint_228f8_00005 | PAUSED   |                 |          0.0001 |   128 |    64 |           64 |   0       | 100           | 0.502287 |      0.491941 |                    1 |\n",
            "| train_Stage1_tune_checkpoint_228f8_00006 | PAUSED   |                 |          0.0001 |    32 |    64 |           64 |   8.94812 |   0.00027926  | 0.500835 |      0.499812 |                    1 |\n",
            "| train_Stage1_tune_checkpoint_228f8_00008 | PENDING  |                 |          0.0001 |    32 |    64 |           64 |           |               |          |               |                      |\n",
            "| train_Stage1_tune_checkpoint_228f8_00009 | PENDING  |                 |          0.0001 |   128 |    32 |           64 |           |               |          |               |                      |\n",
            "+------------------------------------------+----------+-----------------+-----------------+-------+-------+--------------+-----------+---------------+----------+---------------+----------------------+\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=2521)\u001b[0m 2021-07-11 06:49:15,322\tINFO trainable.py:76 -- Checkpoint size is 274610498 bytes\n",
            "\u001b[2m\u001b[36m(pid=2521)\u001b[0m 2021-07-11 06:49:16,488\tINFO trainable.py:76 -- Checkpoint size is 274610498 bytes\n",
            "\u001b[2m\u001b[33m(raylet)\u001b[0m /usr/local/lib/python3.7/dist-packages/ray/autoscaler/_private/cli_logger.py:61: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
            "\u001b[2m\u001b[33m(raylet)\u001b[0m   \"update your install command.\", FutureWarning)\n",
            "\u001b[2m\u001b[36m(pid=2650)\u001b[0m GPU available: True, used: True\n",
            "\u001b[2m\u001b[36m(pid=2650)\u001b[0m TPU available: False, using: 0 TPU cores\n",
            "\u001b[2m\u001b[36m(pid=2650)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\u001b[2m\u001b[36m(pid=2650)\u001b[0m 2021-07-11 06:49:36.122229: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "\u001b[2m\u001b[36m(pid=2650)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=2650)\u001b[0m   | Name          | Type               | Params\n",
            "\u001b[2m\u001b[36m(pid=2650)\u001b[0m -----------------------------------------------------\n",
            "\u001b[2m\u001b[36m(pid=2650)\u001b[0m 0 | generator     | DCGANGenerator     | 1.1 M \n",
            "\u001b[2m\u001b[36m(pid=2650)\u001b[0m 1 | discriminator | DCGANDiscriminator | 2.8 M \n",
            "\u001b[2m\u001b[36m(pid=2650)\u001b[0m 2 | criterion     | BCELoss            | 0     \n",
            "\u001b[2m\u001b[36m(pid=2650)\u001b[0m 3 | modelF        | LensResnet         | 11.2 M\n",
            "\u001b[2m\u001b[36m(pid=2650)\u001b[0m 4 | modelJ        | LensResnet         | 11.2 M\n",
            "\u001b[2m\u001b[36m(pid=2650)\u001b[0m -----------------------------------------------------\n",
            "\u001b[2m\u001b[36m(pid=2650)\u001b[0m 26.2 M    Trainable params\n",
            "\u001b[2m\u001b[36m(pid=2650)\u001b[0m 0         Non-trainable params\n",
            "\u001b[2m\u001b[36m(pid=2650)\u001b[0m 26.2 M    Total params\n",
            "\u001b[2m\u001b[36m(pid=2650)\u001b[0m 104.826   Total estimated model params size (MB)\n",
            "\u001b[2m\u001b[36m(pid=2650)\u001b[0m /usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/data_loading.py:349: UserWarning: Your val_dataloader has `shuffle=True`, it is best practice to turn this off for val/test/predict dataloaders.\n",
            "\u001b[2m\u001b[36m(pid=2650)\u001b[0m   f'Your {mode}_dataloader has `shuffle=True`, it is best practice to turn'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=2650)\u001b[0m \rValidation sanity check: 0it [00:00, ?it/s]\rValidation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=2650)\u001b[0m /usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "\u001b[2m\u001b[36m(pid=2650)\u001b[0m   return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
            "\u001b[2m\u001b[36m(pid=2650)\u001b[0m /usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/deprecated_api.py:152: LightningDeprecationWarning: `Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\n",
            "\u001b[2m\u001b[36m(pid=2650)\u001b[0m   \"`Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0:   0%|          | 0/1173 [00:00<?, ?it/s] \n",
            "Epoch 0:   2%|▏         | 20/1173 [00:06<06:34,  2.92it/s, loss=2.62, v_num=.]\n",
            "Epoch 0:   3%|▎         | 40/1173 [00:13<06:08,  3.07it/s, loss=3.13, v_num=.]\n",
            "Epoch 0:   5%|▌         | 60/1173 [00:19<05:58,  3.10it/s, loss=3.53, v_num=.]\n",
            "Epoch 0:   7%|▋         | 80/1173 [00:29<06:47,  2.68it/s, loss=3.79, v_num=.]\n",
            "Epoch 0:   9%|▊         | 100/1173 [00:40<07:16,  2.46it/s, loss=3.91, v_num=.]\n",
            "Epoch 0:  10%|█         | 120/1173 [00:51<07:30,  2.34it/s, loss=4.11, v_num=.]\n",
            "Epoch 0:  12%|█▏        | 140/1173 [01:02<07:38,  2.25it/s, loss=4.08, v_num=.]\n",
            "Epoch 0:  14%|█▎        | 160/1173 [01:12<07:41,  2.19it/s, loss=4.06, v_num=.]\n",
            "Epoch 0:  15%|█▌        | 180/1173 [01:23<07:39,  2.16it/s, loss=4.13, v_num=.]\n",
            "Epoch 0:  17%|█▋        | 200/1173 [01:33<07:35,  2.13it/s, loss=4.16, v_num=.]\n",
            "Epoch 0:  19%|█▉        | 220/1173 [01:44<07:31,  2.11it/s, loss=11.7, v_num=.]\n",
            "Epoch 0:  20%|██        | 240/1173 [01:54<07:25,  2.10it/s, loss=4.48, v_num=.]\n",
            "Epoch 0:  22%|██▏       | 260/1173 [02:04<07:18,  2.08it/s, loss=3.29, v_num=.]\n",
            "Epoch 0:  24%|██▍       | 280/1173 [02:15<07:11,  2.07it/s, loss=3.5, v_num=.] \n",
            "Epoch 0:  26%|██▌       | 300/1173 [02:25<07:03,  2.06it/s, loss=3.5, v_num=.]\n",
            "Epoch 0:  27%|██▋       | 320/1173 [02:35<06:54,  2.06it/s, loss=3.51, v_num=.]\n",
            "Epoch 0:  29%|██▉       | 340/1173 [02:45<06:45,  2.05it/s, loss=3.43, v_num=.]\n",
            "Epoch 0:  31%|███       | 360/1173 [02:55<06:37,  2.05it/s, loss=3.45, v_num=.]\n",
            "Epoch 0:  32%|███▏      | 380/1173 [03:06<06:28,  2.04it/s, loss=3.39, v_num=.]\n",
            "Epoch 0:  34%|███▍      | 400/1173 [03:16<06:19,  2.04it/s, loss=3.42, v_num=.]\n",
            "Epoch 0:  36%|███▌      | 420/1173 [03:26<06:10,  2.03it/s, loss=3.42, v_num=.]\n",
            "Epoch 0:  38%|███▊      | 440/1173 [03:37<06:01,  2.02it/s, loss=3.49, v_num=.]\n",
            "Epoch 0:  39%|███▉      | 460/1173 [03:47<05:52,  2.02it/s, loss=3.57, v_num=.]\n",
            "Epoch 0:  41%|████      | 480/1173 [03:57<05:43,  2.02it/s, loss=3.55, v_num=.]\n",
            "Epoch 0:  43%|████▎     | 500/1173 [04:08<05:34,  2.01it/s, loss=3.55, v_num=.]\n",
            "Epoch 0:  43%|████▎     | 500/1173 [04:08<05:34,  2.01it/s, loss=3.54, v_num=.]\n",
            "Epoch 0:  44%|████▍     | 520/1173 [04:18<05:24,  2.01it/s, loss=3.53, v_num=.]\n",
            "Epoch 0:  46%|████▌     | 540/1173 [04:28<05:14,  2.01it/s, loss=3.55, v_num=.]\n",
            "Epoch 0:  48%|████▊     | 560/1173 [04:38<05:05,  2.01it/s, loss=3.56, v_num=.]\n",
            "Epoch 0:  49%|████▉     | 580/1173 [04:49<04:55,  2.01it/s, loss=3.55, v_num=.]\n",
            "Epoch 0:  51%|█████     | 600/1173 [04:59<04:46,  2.00it/s, loss=3.62, v_num=.]\n",
            "Epoch 0:  53%|█████▎    | 620/1173 [05:09<04:36,  2.00it/s, loss=3.67, v_num=.]\n",
            "Epoch 0:  55%|█████▍    | 640/1173 [05:20<04:26,  2.00it/s, loss=3.67, v_num=.]\n",
            "Epoch 0:  56%|█████▋    | 660/1173 [05:30<04:16,  2.00it/s, loss=3.76, v_num=.]\n",
            "Epoch 0:  58%|█████▊    | 680/1173 [05:40<04:07,  1.99it/s, loss=3.78, v_num=.]\n",
            "Epoch 0:  60%|█████▉    | 700/1173 [05:51<03:57,  1.99it/s, loss=3.73, v_num=.]\n",
            "Epoch 0:  61%|██████▏   | 720/1173 [06:01<03:47,  1.99it/s, loss=3.75, v_num=.]\n",
            "Epoch 0:  63%|██████▎   | 740/1173 [06:11<03:37,  1.99it/s, loss=3.8, v_num=.] \n",
            "Epoch 0:  65%|██████▍   | 760/1173 [06:22<03:27,  1.99it/s, loss=3.82, v_num=.]\n",
            "Epoch 0:  66%|██████▋   | 780/1173 [06:32<03:17,  1.99it/s, loss=3.86, v_num=.]\n",
            "Epoch 0:  68%|██████▊   | 800/1173 [06:43<03:08,  1.98it/s, loss=3.84, v_num=.]\n",
            "Epoch 0:  70%|██████▉   | 820/1173 [06:53<02:58,  1.98it/s, loss=3.8, v_num=.] \n",
            "Epoch 0:  72%|███████▏  | 840/1173 [07:04<02:48,  1.98it/s, loss=3.88, v_num=.]\n",
            "Epoch 0:  73%|███████▎  | 860/1173 [07:14<02:38,  1.98it/s, loss=3.93, v_num=.]\n",
            "Epoch 0:  75%|███████▌  | 880/1173 [07:24<02:28,  1.98it/s, loss=3.94, v_num=.]\n",
            "Epoch 0:  77%|███████▋  | 900/1173 [07:35<02:18,  1.98it/s, loss=3.94, v_num=.]\n",
            "Epoch 0:  78%|███████▊  | 920/1173 [07:45<02:08,  1.97it/s, loss=3.93, v_num=.]\n",
            "Epoch 0:  80%|████████  | 940/1173 [07:54<01:57,  1.98it/s, loss=3.93, v_num=.]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/235 [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[2m\u001b[36m(pid=2650)\u001b[0m \n",
            "Epoch 0:  82%|████████▏ | 960/1173 [08:04<01:47,  1.98it/s, loss=3.93, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=2650)\u001b[0m \n",
            "Epoch 0:  84%|████████▎ | 980/1173 [08:14<01:37,  1.98it/s, loss=3.93, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=2650)\u001b[0m \n",
            "Epoch 0:  85%|████████▌ | 1000/1173 [08:25<01:27,  1.98it/s, loss=3.93, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=2650)\u001b[0m \n",
            "Epoch 0:  87%|████████▋ | 1020/1173 [08:35<01:17,  1.98it/s, loss=3.93, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=2650)\u001b[0m \n",
            "Epoch 0:  89%|████████▊ | 1040/1173 [08:45<01:07,  1.98it/s, loss=3.93, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=2650)\u001b[0m \n",
            "Epoch 0:  90%|█████████ | 1060/1173 [08:56<00:57,  1.98it/s, loss=3.93, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=2650)\u001b[0m \n",
            "Epoch 0:  92%|█████████▏| 1080/1173 [09:06<00:47,  1.98it/s, loss=3.93, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=2650)\u001b[0m \n",
            "Epoch 0:  94%|█████████▍| 1100/1173 [09:16<00:36,  1.98it/s, loss=3.93, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=2650)\u001b[0m \n",
            "Epoch 0:  95%|█████████▌| 1120/1173 [09:27<00:26,  1.97it/s, loss=3.93, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=2650)\u001b[0m \n",
            "Epoch 0:  97%|█████████▋| 1140/1173 [09:37<00:16,  1.97it/s, loss=3.93, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=2650)\u001b[0m \n",
            "Epoch 0:  99%|█████████▉| 1160/1173 [09:47<00:06,  1.97it/s, loss=3.93, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=2650)\u001b[0m \n",
            "Epoch 0: 100%|██████████| 1173/1173 [09:55<00:00,  1.97it/s, loss=3.93, v_num=.]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=2650)\u001b[0m /usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/deprecated_api.py:152: LightningDeprecationWarning: `Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\n",
            "\u001b[2m\u001b[36m(pid=2650)\u001b[0m   \"`Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\"\n",
            "2021-07-11 06:59:37,971\tINFO pbt.py:543 -- [exploit] transferring weights from trial train_Stage1_tune_checkpoint_228f8_00006 (score 0.5008350610733032) -> train_Stage1_tune_checkpoint_228f8_00008 (score 0.4650954008102417)\n",
            "2021-07-11 06:59:37,978\tINFO pbt.py:558 -- [explore] perturbed config from {'learning_rate': 0.0001, 'batch_size': 64} -> {'learning_rate': 0.00041336110351827344, 'batch_size': 128}\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for train_Stage1_tune_checkpoint_228f8_00008:\n",
            "  auroc: 0.4650954008102417\n",
            "  auroc_cross: 0.5\n",
            "  date: 2021-07-11_06-59-37\n",
            "  done: false\n",
            "  experiment_id: afc6a7241f6144408dc823a1e931ac1c\n",
            "  hostname: bd0789e721af\n",
            "  iterations_since_restore: 1\n",
            "  lossD: 0.0007319121505133808\n",
            "  lossG: 7.817005634307861\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 2650\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 614.1262702941895\n",
            "  time_this_iter_s: 614.1262702941895\n",
            "  time_total_s: 614.1262702941895\n",
            "  timestamp: 1625986777\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 1\n",
            "  trial_id: 228f8_00008\n",
            "  \n",
            "== Status ==\n",
            "Memory usage on this node: 7.0/12.7 GiB\n",
            "PopulationBasedTraining: 6 checkpoints, 1 perturbs\n",
            "Resources requested: 2.0/2 CPUs, 1.0/1 GPUs, 0.0/7.51 GiB heap, 0.0/3.75 GiB objects (0.0/2.0 CPU_group_0_ecd3f63ef35a3c653f2d69781ac9fe69, 0.0/1.0 accelerator_type:T4, 0.0/1.0 GPU_group_ecd3f63ef35a3c653f2d69781ac9fe69, 0.0/2.0 CPU_group_ecd3f63ef35a3c653f2d69781ac9fe69, 0.0/1.0 GPU_group_0_ecd3f63ef35a3c653f2d69781ac9fe69)\n",
            "Current best trial: 228f8_00007 with auroc=0.5541192889213562 and parameters={'learning_rate': 0.0001, 'ngf': 128, 'ndf': 64, 'batch_size': 64}\n",
            "Result logdir: /content/drive/MyDrive/Logs/Stage1_pbt_F\n",
            "Number of trials: 10/10 (8 PAUSED, 1 PENDING, 1 RUNNING)\n",
            "+------------------------------------------+----------+-----------------+-----------------+-------+-------+--------------+-----------+---------------+----------+---------------+----------------------+\n",
            "| Trial name                               | status   | loc             |   learning_rate |   ngf |   ndf |   batch_size |     lossG |         lossD |    auroc |   auroc_cross |   training_iteration |\n",
            "|------------------------------------------+----------+-----------------+-----------------+-------+-------+--------------+-----------+---------------+----------+---------------+----------------------|\n",
            "| train_Stage1_tune_checkpoint_228f8_00008 | RUNNING  | 172.28.0.2:2650 |     0.000413361 |    32 |    64 |          128 |   7.81701 |   0.000731912 | 0.465095 |      0.5      |                    1 |\n",
            "| train_Stage1_tune_checkpoint_228f8_00000 | PAUSED   |                 |     0.0001      |   128 |    32 |           64 |   5.04182 |   0.0528528   | 0.461844 |      0.473679 |                    1 |\n",
            "| train_Stage1_tune_checkpoint_228f8_00001 | PAUSED   |                 |     0.0001      |   128 |   128 |           64 | 100       |  79.1561      | 0.309535 |      0.381914 |                    1 |\n",
            "| train_Stage1_tune_checkpoint_228f8_00002 | PAUSED   |                 |     0.0001      |    32 |    64 |           64 |   8.43246 |   0.000612127 | 0.474643 |      0.489745 |                    1 |\n",
            "| train_Stage1_tune_checkpoint_228f8_00003 | PAUSED   |                 |     0.0001      |    32 |    64 |           64 |   9.28841 |   0.000287676 | 0.50005  |      0.499648 |                    1 |\n",
            "| train_Stage1_tune_checkpoint_228f8_00004 | PAUSED   |                 |     0.0001      |    32 |    64 |           64 |   9.67261 |   0.000125631 | 0.4998   |      0.495837 |                    1 |\n",
            "| train_Stage1_tune_checkpoint_228f8_00005 | PAUSED   |                 |     0.0001      |   128 |    64 |           64 |   0       | 100           | 0.502287 |      0.491941 |                    1 |\n",
            "| train_Stage1_tune_checkpoint_228f8_00006 | PAUSED   |                 |     0.0001      |    32 |    64 |           64 |   8.94812 |   0.00027926  | 0.500835 |      0.499812 |                    1 |\n",
            "| train_Stage1_tune_checkpoint_228f8_00007 | PAUSED   |                 |     0.0001      |   128 |    64 |           64 |   5.99285 |   0.0128835   | 0.554119 |      0.552053 |                    1 |\n",
            "| train_Stage1_tune_checkpoint_228f8_00009 | PENDING  |                 |     0.0001      |   128 |    32 |           64 |           |               |          |               |                      |\n",
            "+------------------------------------------+----------+-----------------+-----------------+-------+-------+--------------+-----------+---------------+----------+---------------+----------------------+\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=2650)\u001b[0m 2021-07-11 06:59:38,850\tINFO trainable.py:378 -- Restored on 172.28.0.2 from checkpoint: /content/drive/MyDrive/Logs/Stage1_pbt_F/train_Stage1_tune_checkpoint_228f8_00008_8_ndf=64,ngf=32_2021-07-11_06-38-50/checkpoint_tmp83b1bf/./\n",
            "\u001b[2m\u001b[36m(pid=2650)\u001b[0m 2021-07-11 06:59:38,850\tINFO trainable.py:385 -- Current state after restoring: {'_iteration': 1, '_timesteps_total': None, '_time_total': 598.3700771331787, '_episodes_total': None}\n",
            "\u001b[2m\u001b[36m(pid=2650)\u001b[0m 2021-07-11 06:59:40,204\tINFO trainable.py:76 -- Checkpoint size is 135937282 bytes\n",
            "\u001b[2m\u001b[33m(raylet)\u001b[0m /usr/local/lib/python3.7/dist-packages/ray/autoscaler/_private/cli_logger.py:61: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
            "\u001b[2m\u001b[33m(raylet)\u001b[0m   \"update your install command.\", FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "== Status ==\n",
            "Memory usage on this node: 4.4/12.7 GiB\n",
            "PopulationBasedTraining: 6 checkpoints, 1 perturbs\n",
            "Resources requested: 2.0/2 CPUs, 1.0/1 GPUs, 0.0/7.51 GiB heap, 0.0/3.75 GiB objects (0.0/1.0 accelerator_type:T4, 0.0/1.0 GPU_group_ecd3f63ef35a3c653f2d69781ac9fe69, 0.0/2.0 CPU_group_0_ecd3f63ef35a3c653f2d69781ac9fe69, 0.0/1.0 GPU_group_0_ecd3f63ef35a3c653f2d69781ac9fe69, 0.0/2.0 CPU_group_ecd3f63ef35a3c653f2d69781ac9fe69)\n",
            "Current best trial: 228f8_00007 with auroc=0.5541192889213562 and parameters={'learning_rate': 0.0001, 'ngf': 128, 'ndf': 64, 'batch_size': 64}\n",
            "Result logdir: /content/drive/MyDrive/Logs/Stage1_pbt_F\n",
            "Number of trials: 10/10 (8 PAUSED, 1 PENDING, 1 RUNNING)\n",
            "+------------------------------------------+----------+-------+-----------------+-------+-------+--------------+-----------+---------------+----------+---------------+----------------------+\n",
            "| Trial name                               | status   | loc   |   learning_rate |   ngf |   ndf |   batch_size |     lossG |         lossD |    auroc |   auroc_cross |   training_iteration |\n",
            "|------------------------------------------+----------+-------+-----------------+-------+-------+--------------+-----------+---------------+----------+---------------+----------------------|\n",
            "| train_Stage1_tune_checkpoint_228f8_00009 | RUNNING  |       |     0.0001      |   128 |    32 |           64 |           |               |          |               |                      |\n",
            "| train_Stage1_tune_checkpoint_228f8_00000 | PAUSED   |       |     0.0001      |   128 |    32 |           64 |   5.04182 |   0.0528528   | 0.461844 |      0.473679 |                    1 |\n",
            "| train_Stage1_tune_checkpoint_228f8_00002 | PAUSED   |       |     0.0001      |    32 |    64 |           64 |   8.43246 |   0.000612127 | 0.474643 |      0.489745 |                    1 |\n",
            "| train_Stage1_tune_checkpoint_228f8_00003 | PAUSED   |       |     0.0001      |    32 |    64 |           64 |   9.28841 |   0.000287676 | 0.50005  |      0.499648 |                    1 |\n",
            "| train_Stage1_tune_checkpoint_228f8_00004 | PAUSED   |       |     0.0001      |    32 |    64 |           64 |   9.67261 |   0.000125631 | 0.4998   |      0.495837 |                    1 |\n",
            "| train_Stage1_tune_checkpoint_228f8_00005 | PAUSED   |       |     0.0001      |   128 |    64 |           64 |   0       | 100           | 0.502287 |      0.491941 |                    1 |\n",
            "| train_Stage1_tune_checkpoint_228f8_00006 | PAUSED   |       |     0.0001      |    32 |    64 |           64 |   8.94812 |   0.00027926  | 0.500835 |      0.499812 |                    1 |\n",
            "| train_Stage1_tune_checkpoint_228f8_00007 | PAUSED   |       |     0.0001      |   128 |    64 |           64 |   5.99285 |   0.0128835   | 0.554119 |      0.552053 |                    1 |\n",
            "| train_Stage1_tune_checkpoint_228f8_00008 | PAUSED   |       |     0.000413361 |    32 |    64 |          128 |   7.81701 |   0.000731912 | 0.465095 |      0.5      |                    1 |\n",
            "| train_Stage1_tune_checkpoint_228f8_00001 | PENDING  |       |     0.0001      |   128 |   128 |           64 | 100       |  79.1561      | 0.309535 |      0.381914 |                    1 |\n",
            "+------------------------------------------+----------+-------+-----------------+-------+-------+--------------+-----------+---------------+----------+---------------+----------------------+\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=2845)\u001b[0m GPU available: True, used: True\n",
            "\u001b[2m\u001b[36m(pid=2845)\u001b[0m TPU available: False, using: 0 TPU cores\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "== Status ==\n",
            "Memory usage on this node: 4.6/12.7 GiB\n",
            "PopulationBasedTraining: 6 checkpoints, 1 perturbs\n",
            "Resources requested: 2.0/2 CPUs, 1.0/1 GPUs, 0.0/7.51 GiB heap, 0.0/3.75 GiB objects (0.0/1.0 accelerator_type:T4, 0.0/1.0 GPU_group_ecd3f63ef35a3c653f2d69781ac9fe69, 0.0/2.0 CPU_group_0_ecd3f63ef35a3c653f2d69781ac9fe69, 0.0/1.0 GPU_group_0_ecd3f63ef35a3c653f2d69781ac9fe69, 0.0/2.0 CPU_group_ecd3f63ef35a3c653f2d69781ac9fe69)\n",
            "Current best trial: 228f8_00007 with auroc=0.5541192889213562 and parameters={'learning_rate': 0.0001, 'ngf': 128, 'ndf': 64, 'batch_size': 64}\n",
            "Result logdir: /content/drive/MyDrive/Logs/Stage1_pbt_F\n",
            "Number of trials: 10/10 (8 PAUSED, 1 PENDING, 1 RUNNING)\n",
            "+------------------------------------------+----------+-------+-----------------+-------+-------+--------------+-----------+---------------+----------+---------------+----------------------+\n",
            "| Trial name                               | status   | loc   |   learning_rate |   ngf |   ndf |   batch_size |     lossG |         lossD |    auroc |   auroc_cross |   training_iteration |\n",
            "|------------------------------------------+----------+-------+-----------------+-------+-------+--------------+-----------+---------------+----------+---------------+----------------------|\n",
            "| train_Stage1_tune_checkpoint_228f8_00009 | RUNNING  |       |     0.0001      |   128 |    32 |           64 |           |               |          |               |                      |\n",
            "| train_Stage1_tune_checkpoint_228f8_00000 | PAUSED   |       |     0.0001      |   128 |    32 |           64 |   5.04182 |   0.0528528   | 0.461844 |      0.473679 |                    1 |\n",
            "| train_Stage1_tune_checkpoint_228f8_00002 | PAUSED   |       |     0.0001      |    32 |    64 |           64 |   8.43246 |   0.000612127 | 0.474643 |      0.489745 |                    1 |\n",
            "| train_Stage1_tune_checkpoint_228f8_00003 | PAUSED   |       |     0.0001      |    32 |    64 |           64 |   9.28841 |   0.000287676 | 0.50005  |      0.499648 |                    1 |\n",
            "| train_Stage1_tune_checkpoint_228f8_00004 | PAUSED   |       |     0.0001      |    32 |    64 |           64 |   9.67261 |   0.000125631 | 0.4998   |      0.495837 |                    1 |\n",
            "| train_Stage1_tune_checkpoint_228f8_00005 | PAUSED   |       |     0.0001      |   128 |    64 |           64 |   0       | 100           | 0.502287 |      0.491941 |                    1 |\n",
            "| train_Stage1_tune_checkpoint_228f8_00006 | PAUSED   |       |     0.0001      |    32 |    64 |           64 |   8.94812 |   0.00027926  | 0.500835 |      0.499812 |                    1 |\n",
            "| train_Stage1_tune_checkpoint_228f8_00007 | PAUSED   |       |     0.0001      |   128 |    64 |           64 |   5.99285 |   0.0128835   | 0.554119 |      0.552053 |                    1 |\n",
            "| train_Stage1_tune_checkpoint_228f8_00008 | PAUSED   |       |     0.000413361 |    32 |    64 |          128 |   7.81701 |   0.000731912 | 0.465095 |      0.5      |                    1 |\n",
            "| train_Stage1_tune_checkpoint_228f8_00001 | PENDING  |       |     0.0001      |   128 |   128 |           64 | 100       |  79.1561      | 0.309535 |      0.381914 |                    1 |\n",
            "+------------------------------------------+----------+-------+-----------------+-------+-------+--------------+-----------+---------------+----------+---------------+----------------------+\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=2845)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\u001b[2m\u001b[36m(pid=2845)\u001b[0m 2021-07-11 07:00:01.571625: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "\u001b[2m\u001b[36m(pid=2845)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=2845)\u001b[0m   | Name          | Type               | Params\n",
            "\u001b[2m\u001b[36m(pid=2845)\u001b[0m -----------------------------------------------------\n",
            "\u001b[2m\u001b[36m(pid=2845)\u001b[0m 0 | generator     | DCGANGenerator     | 12.7 M\n",
            "\u001b[2m\u001b[36m(pid=2845)\u001b[0m 1 | discriminator | DCGANDiscriminator | 693 K \n",
            "\u001b[2m\u001b[36m(pid=2845)\u001b[0m 2 | criterion     | BCELoss            | 0     \n",
            "\u001b[2m\u001b[36m(pid=2845)\u001b[0m 3 | modelF        | LensResnet         | 11.2 M\n",
            "\u001b[2m\u001b[36m(pid=2845)\u001b[0m 4 | modelJ        | LensResnet         | 11.2 M\n",
            "\u001b[2m\u001b[36m(pid=2845)\u001b[0m -----------------------------------------------------\n",
            "\u001b[2m\u001b[36m(pid=2845)\u001b[0m 35.7 M    Trainable params\n",
            "\u001b[2m\u001b[36m(pid=2845)\u001b[0m 0         Non-trainable params\n",
            "\u001b[2m\u001b[36m(pid=2845)\u001b[0m 35.7 M    Total params\n",
            "\u001b[2m\u001b[36m(pid=2845)\u001b[0m 142.767   Total estimated model params size (MB)\n",
            "\u001b[2m\u001b[36m(pid=2845)\u001b[0m /usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/data_loading.py:349: UserWarning: Your val_dataloader has `shuffle=True`, it is best practice to turn this off for val/test/predict dataloaders.\n",
            "\u001b[2m\u001b[36m(pid=2845)\u001b[0m   f'Your {mode}_dataloader has `shuffle=True`, it is best practice to turn'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=2845)\u001b[0m \rValidation sanity check: 0it [00:00, ?it/s]\rValidation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=2845)\u001b[0m /usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "\u001b[2m\u001b[36m(pid=2845)\u001b[0m   return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
            "\u001b[2m\u001b[36m(pid=2845)\u001b[0m /usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/deprecated_api.py:152: LightningDeprecationWarning: `Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\n",
            "\u001b[2m\u001b[36m(pid=2845)\u001b[0m   \"`Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=2845)\u001b[0m \r                                                              \r\rTraining: 0it [00:00, ?it/s]\n",
            "Epoch 0:   0%|          | 0/1173 [00:00<?, ?it/s] \n",
            "Epoch 0:   2%|▏         | 20/1173 [00:06<06:41,  2.87it/s, loss=1.69, v_num=.]\n",
            "Epoch 0:   3%|▎         | 40/1173 [00:13<06:31,  2.90it/s, loss=1.98, v_num=.]\n",
            "Epoch 0:   5%|▌         | 60/1173 [00:24<07:37,  2.43it/s, loss=2.01, v_num=.]\n",
            "Epoch 0:   7%|▋         | 80/1173 [00:35<08:04,  2.26it/s, loss=2.03, v_num=.]\n",
            "Epoch 0:   9%|▊         | 100/1173 [00:46<08:18,  2.15it/s, loss=2.08, v_num=.]\n",
            "Epoch 0:  10%|█         | 120/1173 [00:57<08:21,  2.10it/s, loss=2.18, v_num=.]\n",
            "Epoch 0:  12%|█▏        | 140/1173 [01:07<08:19,  2.07it/s, loss=2.17, v_num=.]\n",
            "Epoch 0:  14%|█▎        | 160/1173 [01:18<08:15,  2.04it/s, loss=2.19, v_num=.]\n",
            "Epoch 0:  15%|█▌        | 180/1173 [01:28<08:10,  2.03it/s, loss=2.24, v_num=.]\n",
            "Epoch 0:  17%|█▋        | 200/1173 [01:39<08:02,  2.02it/s, loss=2.26, v_num=.]\n",
            "Epoch 0:  19%|█▉        | 220/1173 [01:49<07:54,  2.01it/s, loss=2.32, v_num=.]\n",
            "Epoch 0:  20%|██        | 240/1173 [01:59<07:45,  2.01it/s, loss=2.34, v_num=.]\n",
            "Epoch 0:  22%|██▏       | 260/1173 [02:09<07:36,  2.00it/s, loss=2.34, v_num=.]\n",
            "Epoch 0:  24%|██▍       | 280/1173 [02:20<07:27,  2.00it/s, loss=2.37, v_num=.]\n",
            "Epoch 0:  26%|██▌       | 300/1173 [02:30<07:18,  1.99it/s, loss=2.43, v_num=.]\n",
            "Epoch 0:  27%|██▋       | 320/1173 [02:41<07:10,  1.98it/s, loss=2.46, v_num=.]\n",
            "Epoch 0:  29%|██▉       | 340/1173 [02:52<07:01,  1.98it/s, loss=2.5, v_num=.] \n",
            "Epoch 0:  31%|███       | 360/1173 [03:02<06:52,  1.97it/s, loss=2.49, v_num=.]\n",
            "Epoch 0:  32%|███▏      | 380/1173 [03:13<06:42,  1.97it/s, loss=2.46, v_num=.]\n",
            "Epoch 0:  34%|███▍      | 400/1173 [03:23<06:33,  1.96it/s, loss=2.46, v_num=.]\n",
            "Epoch 0:  36%|███▌      | 420/1173 [03:34<06:23,  1.96it/s, loss=2.6, v_num=.] \n",
            "Epoch 0:  38%|███▊      | 440/1173 [03:44<06:14,  1.96it/s, loss=2.91, v_num=.]\n",
            "Epoch 0:  39%|███▉      | 460/1173 [03:55<06:04,  1.96it/s, loss=2.26, v_num=.]\n",
            "Epoch 0:  41%|████      | 480/1173 [04:05<05:54,  1.96it/s, loss=2.39, v_num=.]\n",
            "Epoch 0:  43%|████▎     | 500/1173 [04:16<05:44,  1.95it/s, loss=2.33, v_num=.]\n",
            "Epoch 0:  44%|████▍     | 520/1173 [04:26<05:34,  1.95it/s, loss=2.44, v_num=.]\n",
            "Epoch 0:  46%|████▌     | 540/1173 [04:36<05:24,  1.95it/s, loss=2.5, v_num=.] \n",
            "Epoch 0:  48%|████▊     | 560/1173 [04:47<05:14,  1.95it/s, loss=2.55, v_num=.]\n",
            "Epoch 0:  49%|████▉     | 580/1173 [04:57<05:04,  1.95it/s, loss=2.62, v_num=.]\n",
            "Epoch 0:  51%|█████     | 600/1173 [05:08<04:54,  1.94it/s, loss=2.68, v_num=.]\n",
            "Epoch 0:  53%|█████▎    | 620/1173 [05:19<04:44,  1.94it/s, loss=2.72, v_num=.]\n",
            "Epoch 0:  55%|█████▍    | 640/1173 [05:29<04:34,  1.94it/s, loss=2.68, v_num=.]\n",
            "Epoch 0:  56%|█████▋    | 660/1173 [05:40<04:24,  1.94it/s, loss=2.69, v_num=.]\n",
            "Epoch 0:  58%|█████▊    | 680/1173 [05:50<04:14,  1.94it/s, loss=2.58, v_num=.]\n",
            "Epoch 0:  60%|█████▉    | 700/1173 [06:01<04:04,  1.94it/s, loss=2.59, v_num=.]\n",
            "Epoch 0:  61%|██████▏   | 720/1173 [06:11<03:53,  1.94it/s, loss=2.7, v_num=.] \n",
            "Epoch 0:  63%|██████▎   | 740/1173 [06:22<03:43,  1.94it/s, loss=2.8, v_num=.]\n",
            "Epoch 0:  65%|██████▍   | 760/1173 [06:32<03:33,  1.94it/s, loss=2.81, v_num=.]\n",
            "Epoch 0:  66%|██████▋   | 780/1173 [06:42<03:23,  1.94it/s, loss=2.82, v_num=.]\n",
            "Epoch 0:  68%|██████▊   | 800/1173 [06:53<03:12,  1.94it/s, loss=2.77, v_num=.]\n",
            "Epoch 0:  70%|██████▉   | 820/1173 [07:03<03:02,  1.94it/s, loss=2.81, v_num=.]\n",
            "Epoch 0:  72%|███████▏  | 840/1173 [07:14<02:52,  1.93it/s, loss=2.81, v_num=.]\n",
            "Epoch 0:  73%|███████▎  | 860/1173 [07:24<02:41,  1.93it/s, loss=2.82, v_num=.]\n",
            "Epoch 0:  75%|███████▌  | 880/1173 [07:35<02:31,  1.93it/s, loss=2.84, v_num=.]\n",
            "Epoch 0:  77%|███████▋  | 900/1173 [07:45<02:21,  1.93it/s, loss=2.91, v_num=.]\n",
            "Epoch 0:  78%|███████▊  | 920/1173 [07:56<02:10,  1.93it/s, loss=2.96, v_num=.]\n",
            "Epoch 0:  80%|████████  | 940/1173 [08:05<02:00,  1.94it/s, loss=2.96, v_num=.]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/235 [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[2m\u001b[36m(pid=2845)\u001b[0m \n",
            "Epoch 0:  82%|████████▏ | 960/1173 [08:14<01:49,  1.94it/s, loss=2.96, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=2845)\u001b[0m \n",
            "Epoch 0:  84%|████████▎ | 980/1173 [08:25<01:39,  1.94it/s, loss=2.96, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=2845)\u001b[0m \n",
            "Epoch 0:  85%|████████▌ | 1000/1173 [08:35<01:29,  1.94it/s, loss=2.96, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=2845)\u001b[0m \n",
            "Epoch 0:  87%|████████▋ | 1020/1173 [08:46<01:18,  1.94it/s, loss=2.96, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=2845)\u001b[0m \n",
            "Epoch 0:  89%|████████▊ | 1040/1173 [08:56<01:08,  1.94it/s, loss=2.96, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=2845)\u001b[0m \n",
            "Epoch 0:  90%|█████████ | 1060/1173 [09:07<00:58,  1.94it/s, loss=2.96, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=2845)\u001b[0m \n",
            "Epoch 0:  92%|█████████▏| 1080/1173 [09:17<00:48,  1.94it/s, loss=2.96, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=2845)\u001b[0m \n",
            "Epoch 0:  94%|█████████▍| 1100/1173 [09:27<00:37,  1.94it/s, loss=2.96, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=2845)\u001b[0m \n",
            "Epoch 0:  95%|█████████▌| 1120/1173 [09:38<00:27,  1.94it/s, loss=2.96, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=2845)\u001b[0m \n",
            "Epoch 0:  97%|█████████▋| 1140/1173 [09:48<00:17,  1.94it/s, loss=2.96, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=2845)\u001b[0m \n",
            "Epoch 0:  99%|█████████▉| 1160/1173 [09:59<00:06,  1.93it/s, loss=2.96, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=2845)\u001b[0m \n",
            "Epoch 0: 100%|██████████| 1173/1173 [10:07<00:00,  1.93it/s, loss=2.96, v_num=.]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=2845)\u001b[0m /usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/deprecated_api.py:152: LightningDeprecationWarning: `Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\n",
            "\u001b[2m\u001b[36m(pid=2845)\u001b[0m   \"`Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\"\n",
            "2021-07-11 07:10:16,745\tINFO pbt.py:543 -- [exploit] transferring weights from trial train_Stage1_tune_checkpoint_228f8_00005 (score 0.502286970615387) -> train_Stage1_tune_checkpoint_228f8_00009 (score 0.43624940514564514)\n",
            "2021-07-11 07:10:16,747\tINFO pbt.py:558 -- [explore] perturbed config from {'learning_rate': 0.0001, 'batch_size': 64} -> {'learning_rate': 0.00012, 'batch_size': 32}\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for train_Stage1_tune_checkpoint_228f8_00009:\n",
            "  auroc: 0.43624940514564514\n",
            "  auroc_cross: 0.4027217924594879\n",
            "  date: 2021-07-11_07-10-16\n",
            "  done: false\n",
            "  experiment_id: 8df578031a8d4345b8903f16ea692868\n",
            "  hostname: bd0789e721af\n",
            "  iterations_since_restore: 1\n",
            "  lossD: 0.004046842455863953\n",
            "  lossG: 6.178808212280273\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 2845\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 628.9234342575073\n",
            "  time_this_iter_s: 628.9234342575073\n",
            "  time_total_s: 628.9234342575073\n",
            "  timestamp: 1625987416\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 1\n",
            "  trial_id: 228f8_00009\n",
            "  \n",
            "== Status ==\n",
            "Memory usage on this node: 7.1/12.7 GiB\n",
            "PopulationBasedTraining: 6 checkpoints, 2 perturbs\n",
            "Resources requested: 2.0/2 CPUs, 1.0/1 GPUs, 0.0/7.51 GiB heap, 0.0/3.75 GiB objects (0.0/2.0 CPU_group_ecd3f63ef35a3c653f2d69781ac9fe69, 0.0/1.0 GPU_group_0_ecd3f63ef35a3c653f2d69781ac9fe69, 0.0/2.0 CPU_group_0_ecd3f63ef35a3c653f2d69781ac9fe69, 0.0/1.0 accelerator_type:T4, 0.0/1.0 GPU_group_ecd3f63ef35a3c653f2d69781ac9fe69)\n",
            "Current best trial: 228f8_00007 with auroc=0.5541192889213562 and parameters={'learning_rate': 0.0001, 'ngf': 128, 'ndf': 64, 'batch_size': 64}\n",
            "Result logdir: /content/drive/MyDrive/Logs/Stage1_pbt_F\n",
            "Number of trials: 10/10 (8 PAUSED, 1 PENDING, 1 RUNNING)\n",
            "+------------------------------------------+----------+-----------------+-----------------+-------+-------+--------------+-----------+---------------+----------+---------------+----------------------+\n",
            "| Trial name                               | status   | loc             |   learning_rate |   ngf |   ndf |   batch_size |     lossG |         lossD |    auroc |   auroc_cross |   training_iteration |\n",
            "|------------------------------------------+----------+-----------------+-----------------+-------+-------+--------------+-----------+---------------+----------+---------------+----------------------|\n",
            "| train_Stage1_tune_checkpoint_228f8_00009 | RUNNING  | 172.28.0.2:2845 |     0.00012     |   128 |    64 |           32 |   6.17881 |   0.00404684  | 0.436249 |      0.402722 |                    1 |\n",
            "| train_Stage1_tune_checkpoint_228f8_00000 | PAUSED   |                 |     0.0001      |   128 |    32 |           64 |   5.04182 |   0.0528528   | 0.461844 |      0.473679 |                    1 |\n",
            "| train_Stage1_tune_checkpoint_228f8_00002 | PAUSED   |                 |     0.0001      |    32 |    64 |           64 |   8.43246 |   0.000612127 | 0.474643 |      0.489745 |                    1 |\n",
            "| train_Stage1_tune_checkpoint_228f8_00003 | PAUSED   |                 |     0.0001      |    32 |    64 |           64 |   9.28841 |   0.000287676 | 0.50005  |      0.499648 |                    1 |\n",
            "| train_Stage1_tune_checkpoint_228f8_00004 | PAUSED   |                 |     0.0001      |    32 |    64 |           64 |   9.67261 |   0.000125631 | 0.4998   |      0.495837 |                    1 |\n",
            "| train_Stage1_tune_checkpoint_228f8_00005 | PAUSED   |                 |     0.0001      |   128 |    64 |           64 |   0       | 100           | 0.502287 |      0.491941 |                    1 |\n",
            "| train_Stage1_tune_checkpoint_228f8_00006 | PAUSED   |                 |     0.0001      |    32 |    64 |           64 |   8.94812 |   0.00027926  | 0.500835 |      0.499812 |                    1 |\n",
            "| train_Stage1_tune_checkpoint_228f8_00007 | PAUSED   |                 |     0.0001      |   128 |    64 |           64 |   5.99285 |   0.0128835   | 0.554119 |      0.552053 |                    1 |\n",
            "| train_Stage1_tune_checkpoint_228f8_00008 | PAUSED   |                 |     0.000413361 |    32 |    64 |          128 |   7.81701 |   0.000731912 | 0.465095 |      0.5      |                    1 |\n",
            "| train_Stage1_tune_checkpoint_228f8_00001 | PENDING  |                 |     0.0001      |   128 |   128 |           64 | 100       |  79.1561      | 0.309535 |      0.381914 |                    1 |\n",
            "+------------------------------------------+----------+-----------------+-----------------+-------+-------+--------------+-----------+---------------+----------+---------------+----------------------+\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=2845)\u001b[0m 2021-07-11 07:10:18,185\tINFO trainable.py:378 -- Restored on 172.28.0.2 from checkpoint: /content/drive/MyDrive/Logs/Stage1_pbt_F/train_Stage1_tune_checkpoint_228f8_00009_9_ndf=32,ngf=128_2021-07-11_06-49-16/checkpoint_tmp441b2a/./\n",
            "\u001b[2m\u001b[36m(pid=2845)\u001b[0m 2021-07-11 07:10:18,186\tINFO trainable.py:385 -- Current state after restoring: {'_iteration': 1, '_timesteps_total': None, '_time_total': 589.414802312851, '_episodes_total': None}\n",
            "\u001b[2m\u001b[36m(pid=2845)\u001b[0m 2021-07-11 07:10:23,553\tINFO trainable.py:76 -- Checkpoint size is 274610498 bytes\n",
            "\u001b[2m\u001b[33m(raylet)\u001b[0m /usr/local/lib/python3.7/dist-packages/ray/autoscaler/_private/cli_logger.py:61: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
            "\u001b[2m\u001b[33m(raylet)\u001b[0m   \"update your install command.\", FutureWarning)\n",
            "\u001b[2m\u001b[33m(raylet)\u001b[0m /usr/local/lib/python3.7/dist-packages/ray/autoscaler/_private/cli_logger.py:61: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
            "\u001b[2m\u001b[33m(raylet)\u001b[0m   \"update your install command.\", FutureWarning)\n",
            "2021-07-11 07:10:31,808\tINFO trainable.py:76 -- Checkpoint size is 373840194 bytes\n",
            "2021-07-11 07:10:32,524\tWARNING util.py:162 -- The `start_trial` operation took 12.420 s, which may be a performance bottleneck.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "== Status ==\n",
            "Memory usage on this node: 5.1/12.7 GiB\n",
            "PopulationBasedTraining: 6 checkpoints, 2 perturbs\n",
            "Resources requested: 2.0/2 CPUs, 1.0/1 GPUs, 0.0/7.51 GiB heap, 0.0/3.75 GiB objects (0.0/2.0 CPU_group_ecd3f63ef35a3c653f2d69781ac9fe69, 0.0/1.0 GPU_group_0_ecd3f63ef35a3c653f2d69781ac9fe69, 0.0/1.0 GPU_group_ecd3f63ef35a3c653f2d69781ac9fe69, 0.0/2.0 CPU_group_0_ecd3f63ef35a3c653f2d69781ac9fe69, 0.0/1.0 accelerator_type:T4)\n",
            "Current best trial: 228f8_00007 with auroc=0.5541192889213562 and parameters={'learning_rate': 0.0001, 'ngf': 128, 'ndf': 64, 'batch_size': 64}\n",
            "Result logdir: /content/drive/MyDrive/Logs/Stage1_pbt_F\n",
            "Number of trials: 10/10 (9 PAUSED, 1 RUNNING)\n",
            "+------------------------------------------+----------+-------+-----------------+-------+-------+--------------+-----------+---------------+----------+---------------+----------------------+\n",
            "| Trial name                               | status   | loc   |   learning_rate |   ngf |   ndf |   batch_size |     lossG |         lossD |    auroc |   auroc_cross |   training_iteration |\n",
            "|------------------------------------------+----------+-------+-----------------+-------+-------+--------------+-----------+---------------+----------+---------------+----------------------|\n",
            "| train_Stage1_tune_checkpoint_228f8_00001 | RUNNING  |       |     0.0001      |   128 |   128 |           64 | 100       |  79.1561      | 0.309535 |      0.381914 |                    1 |\n",
            "| train_Stage1_tune_checkpoint_228f8_00000 | PAUSED   |       |     0.0001      |   128 |    32 |           64 |   5.04182 |   0.0528528   | 0.461844 |      0.473679 |                    1 |\n",
            "| train_Stage1_tune_checkpoint_228f8_00002 | PAUSED   |       |     0.0001      |    32 |    64 |           64 |   8.43246 |   0.000612127 | 0.474643 |      0.489745 |                    1 |\n",
            "| train_Stage1_tune_checkpoint_228f8_00003 | PAUSED   |       |     0.0001      |    32 |    64 |           64 |   9.28841 |   0.000287676 | 0.50005  |      0.499648 |                    1 |\n",
            "| train_Stage1_tune_checkpoint_228f8_00004 | PAUSED   |       |     0.0001      |    32 |    64 |           64 |   9.67261 |   0.000125631 | 0.4998   |      0.495837 |                    1 |\n",
            "| train_Stage1_tune_checkpoint_228f8_00005 | PAUSED   |       |     0.0001      |   128 |    64 |           64 |   0       | 100           | 0.502287 |      0.491941 |                    1 |\n",
            "| train_Stage1_tune_checkpoint_228f8_00006 | PAUSED   |       |     0.0001      |    32 |    64 |           64 |   8.94812 |   0.00027926  | 0.500835 |      0.499812 |                    1 |\n",
            "| train_Stage1_tune_checkpoint_228f8_00007 | PAUSED   |       |     0.0001      |   128 |    64 |           64 |   5.99285 |   0.0128835   | 0.554119 |      0.552053 |                    1 |\n",
            "| train_Stage1_tune_checkpoint_228f8_00008 | PAUSED   |       |     0.000413361 |    32 |    64 |          128 |   7.81701 |   0.000731912 | 0.465095 |      0.5      |                    1 |\n",
            "| train_Stage1_tune_checkpoint_228f8_00009 | PAUSED   |       |     0.00012     |   128 |    64 |           32 |   6.17881 |   0.00404684  | 0.436249 |      0.402722 |                    1 |\n",
            "+------------------------------------------+----------+-------+-----------------+-------+-------+--------------+-----------+---------------+----------+---------------+----------------------+\n",
            "\n",
            "\n",
            "== Status ==\n",
            "Memory usage on this node: 5.9/12.7 GiB\n",
            "PopulationBasedTraining: 6 checkpoints, 2 perturbs\n",
            "Resources requested: 2.0/2 CPUs, 1.0/1 GPUs, 0.0/7.51 GiB heap, 0.0/3.75 GiB objects (0.0/1.0 GPU_group_0_ecd3f63ef35a3c653f2d69781ac9fe69, 0.0/2.0 CPU_group_0_ecd3f63ef35a3c653f2d69781ac9fe69, 0.0/1.0 GPU_group_ecd3f63ef35a3c653f2d69781ac9fe69, 0.0/2.0 CPU_group_ecd3f63ef35a3c653f2d69781ac9fe69, 0.0/1.0 accelerator_type:T4)\n",
            "Current best trial: 228f8_00007 with auroc=0.5541192889213562 and parameters={'learning_rate': 0.0001, 'ngf': 128, 'ndf': 64, 'batch_size': 64}\n",
            "Result logdir: /content/drive/MyDrive/Logs/Stage1_pbt_F\n",
            "Number of trials: 10/10 (8 PAUSED, 1 PENDING, 1 RUNNING)\n",
            "+------------------------------------------+----------+-------+-----------------+-------+-------+--------------+-----------+---------------+----------+---------------+----------------------+\n",
            "| Trial name                               | status   | loc   |   learning_rate |   ngf |   ndf |   batch_size |     lossG |         lossD |    auroc |   auroc_cross |   training_iteration |\n",
            "|------------------------------------------+----------+-------+-----------------+-------+-------+--------------+-----------+---------------+----------+---------------+----------------------|\n",
            "| train_Stage1_tune_checkpoint_228f8_00001 | RUNNING  |       |     0.0001      |   128 |   128 |           64 | 100       |  79.1561      | 0.309535 |      0.381914 |                    1 |\n",
            "| train_Stage1_tune_checkpoint_228f8_00002 | PAUSED   |       |     0.0001      |    32 |    64 |           64 |   8.43246 |   0.000612127 | 0.474643 |      0.489745 |                    1 |\n",
            "| train_Stage1_tune_checkpoint_228f8_00003 | PAUSED   |       |     0.0001      |    32 |    64 |           64 |   9.28841 |   0.000287676 | 0.50005  |      0.499648 |                    1 |\n",
            "| train_Stage1_tune_checkpoint_228f8_00004 | PAUSED   |       |     0.0001      |    32 |    64 |           64 |   9.67261 |   0.000125631 | 0.4998   |      0.495837 |                    1 |\n",
            "| train_Stage1_tune_checkpoint_228f8_00005 | PAUSED   |       |     0.0001      |   128 |    64 |           64 |   0       | 100           | 0.502287 |      0.491941 |                    1 |\n",
            "| train_Stage1_tune_checkpoint_228f8_00006 | PAUSED   |       |     0.0001      |    32 |    64 |           64 |   8.94812 |   0.00027926  | 0.500835 |      0.499812 |                    1 |\n",
            "| train_Stage1_tune_checkpoint_228f8_00007 | PAUSED   |       |     0.0001      |   128 |    64 |           64 |   5.99285 |   0.0128835   | 0.554119 |      0.552053 |                    1 |\n",
            "| train_Stage1_tune_checkpoint_228f8_00008 | PAUSED   |       |     0.000413361 |    32 |    64 |          128 |   7.81701 |   0.000731912 | 0.465095 |      0.5      |                    1 |\n",
            "| train_Stage1_tune_checkpoint_228f8_00009 | PAUSED   |       |     0.00012     |   128 |    64 |           32 |   6.17881 |   0.00404684  | 0.436249 |      0.402722 |                    1 |\n",
            "| train_Stage1_tune_checkpoint_228f8_00000 | PENDING  |       |     0.0001      |   128 |    32 |           64 |   5.04182 |   0.0528528   | 0.461844 |      0.473679 |                    1 |\n",
            "+------------------------------------------+----------+-------+-----------------+-------+-------+--------------+-----------+---------------+----------+---------------+----------------------+\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=2978)\u001b[0m 2021-07-11 07:10:38,658\tINFO trainable.py:378 -- Restored on 172.28.0.2 from checkpoint: /content/drive/MyDrive/Logs/Stage1_pbt_F/train_Stage1_tune_checkpoint_228f8_00001_1_ndf=128,ngf=128_2021-07-11_05-30-43/checkpoint_tmpc15e82/./\n",
            "\u001b[2m\u001b[36m(pid=2978)\u001b[0m 2021-07-11 07:10:38,658\tINFO trainable.py:385 -- Current state after restoring: {'_iteration': 1, '_timesteps_total': None, '_time_total': 558.8177676200867, '_episodes_total': None}\n",
            "\u001b[2m\u001b[36m(pid=2978)\u001b[0m GPU available: True, used: True\n",
            "\u001b[2m\u001b[36m(pid=2978)\u001b[0m TPU available: False, using: 0 TPU cores\n",
            "\u001b[2m\u001b[36m(pid=2978)\u001b[0m 2021-07-11 07:10:39,302\tERROR function_runner.py:254 -- Runner Thread raised error.\n",
            "\u001b[2m\u001b[36m(pid=2978)\u001b[0m Traceback (most recent call last):\n",
            "\u001b[2m\u001b[36m(pid=2978)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/ray/tune/function_runner.py\", line 248, in run\n",
            "\u001b[2m\u001b[36m(pid=2978)\u001b[0m     self._entrypoint()\n",
            "\u001b[2m\u001b[36m(pid=2978)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/ray/tune/function_runner.py\", line 316, in entrypoint\n",
            "\u001b[2m\u001b[36m(pid=2978)\u001b[0m     self._status_reporter.get_checkpoint())\n",
            "\u001b[2m\u001b[36m(pid=2978)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/ray/tune/function_runner.py\", line 580, in _trainable_func\n",
            "\u001b[2m\u001b[36m(pid=2978)\u001b[0m     output = fn()\n",
            "\u001b[2m\u001b[36m(pid=2978)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/ray/tune/utils/trainable.py\", line 331, in inner\n",
            "\u001b[2m\u001b[36m(pid=2978)\u001b[0m     trainable(config, **fn_kwargs)\n",
            "\u001b[2m\u001b[36m(pid=2978)\u001b[0m   File \"<ipython-input-17-509686d32ba6>\", line 37, in train_Stage1_tune_checkpoint\n",
            "\u001b[2m\u001b[36m(pid=2978)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/core/saving.py\", line 199, in _load_model_state\n",
            "\u001b[2m\u001b[36m(pid=2978)\u001b[0m     model = cls(**_cls_kwargs)\n",
            "\u001b[2m\u001b[36m(pid=2978)\u001b[0m   File \"<ipython-input-14-52f37f71553b>\", line 3, in __init__\n",
            "\u001b[2m\u001b[36m(pid=2978)\u001b[0m TypeError: __init__() got multiple values for keyword argument 'feature_maps_gen'\n",
            "\u001b[2m\u001b[36m(pid=2978)\u001b[0m Exception in thread Thread-2:\n",
            "\u001b[2m\u001b[36m(pid=2978)\u001b[0m Traceback (most recent call last):\n",
            "\u001b[2m\u001b[36m(pid=2978)\u001b[0m   File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
            "\u001b[2m\u001b[36m(pid=2978)\u001b[0m     self.run()\n",
            "\u001b[2m\u001b[36m(pid=2978)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/ray/tune/function_runner.py\", line 267, in run\n",
            "\u001b[2m\u001b[36m(pid=2978)\u001b[0m     raise e\n",
            "\u001b[2m\u001b[36m(pid=2978)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/ray/tune/function_runner.py\", line 248, in run\n",
            "\u001b[2m\u001b[36m(pid=2978)\u001b[0m     self._entrypoint()\n",
            "\u001b[2m\u001b[36m(pid=2978)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/ray/tune/function_runner.py\", line 316, in entrypoint\n",
            "\u001b[2m\u001b[36m(pid=2978)\u001b[0m     self._status_reporter.get_checkpoint())\n",
            "\u001b[2m\u001b[36m(pid=2978)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/ray/tune/function_runner.py\", line 580, in _trainable_func\n",
            "\u001b[2m\u001b[36m(pid=2978)\u001b[0m     output = fn()\n",
            "\u001b[2m\u001b[36m(pid=2978)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/ray/tune/utils/trainable.py\", line 331, in inner\n",
            "\u001b[2m\u001b[36m(pid=2978)\u001b[0m     trainable(config, **fn_kwargs)\n",
            "\u001b[2m\u001b[36m(pid=2978)\u001b[0m   File \"<ipython-input-17-509686d32ba6>\", line 37, in train_Stage1_tune_checkpoint\n",
            "\u001b[2m\u001b[36m(pid=2978)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/core/saving.py\", line 199, in _load_model_state\n",
            "\u001b[2m\u001b[36m(pid=2978)\u001b[0m     model = cls(**_cls_kwargs)\n",
            "\u001b[2m\u001b[36m(pid=2978)\u001b[0m   File \"<ipython-input-14-52f37f71553b>\", line 3, in __init__\n",
            "\u001b[2m\u001b[36m(pid=2978)\u001b[0m TypeError: __init__() got multiple values for keyword argument 'feature_maps_gen'\n",
            "\u001b[2m\u001b[36m(pid=2978)\u001b[0m \n",
            "2021-07-11 07:10:39,540\tERROR trial_runner.py:748 -- Trial train_Stage1_tune_checkpoint_228f8_00001: Error processing event.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ray/tune/trial_runner.py\", line 718, in _process_trial\n",
            "    results = self.trial_executor.fetch_result(trial)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ray/tune/ray_trial_executor.py\", line 688, in fetch_result\n",
            "    result = ray.get(trial_future[0], timeout=DEFAULT_GET_TIMEOUT)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ray/_private/client_mode_hook.py\", line 62, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ray/worker.py\", line 1495, in get\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(TuneError): \u001b[36mray::ImplicitFunc.train_buffered()\u001b[39m (pid=2978, ip=172.28.0.2)\n",
            "  File \"python/ray/_raylet.pyx\", line 501, in ray._raylet.execute_task\n",
            "  File \"python/ray/_raylet.pyx\", line 451, in ray._raylet.execute_task.function_executor\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ray/_private/function_manager.py\", line 563, in actor_method_executor\n",
            "    return method(__ray_actor, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ray/tune/trainable.py\", line 173, in train_buffered\n",
            "    result = self.train()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ray/tune/trainable.py\", line 232, in train\n",
            "    result = self.step()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ray/tune/function_runner.py\", line 366, in step\n",
            "    self._report_thread_runner_error(block=True)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ray/tune/function_runner.py\", line 513, in _report_thread_runner_error\n",
            "    (\"Trial raised an exception. Traceback:\\n{}\".format(err_tb_str)\n",
            "ray.tune.error.TuneError: Trial raised an exception. Traceback:\n",
            "\u001b[36mray::ImplicitFunc.train_buffered()\u001b[39m (pid=2978, ip=172.28.0.2)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ray/tune/function_runner.py\", line 248, in run\n",
            "    self._entrypoint()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ray/tune/function_runner.py\", line 316, in entrypoint\n",
            "    self._status_reporter.get_checkpoint())\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ray/tune/function_runner.py\", line 580, in _trainable_func\n",
            "    output = fn()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ray/tune/utils/trainable.py\", line 331, in inner\n",
            "    trainable(config, **fn_kwargs)\n",
            "  File \"<ipython-input-17-509686d32ba6>\", line 37, in train_Stage1_tune_checkpoint\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/core/saving.py\", line 199, in _load_model_state\n",
            "    model = cls(**_cls_kwargs)\n",
            "  File \"<ipython-input-14-52f37f71553b>\", line 3, in __init__\n",
            "TypeError: __init__() got multiple values for keyword argument 'feature_maps_gen'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for train_Stage1_tune_checkpoint_228f8_00001:\n",
            "  auroc: 0.3095346987247467\n",
            "  auroc_cross: 0.381914347410202\n",
            "  date: 2021-07-11_05-49-29\n",
            "  done: false\n",
            "  experiment_id: f111bebca8014281875ff9bfc982c872\n",
            "  experiment_tag: 1_ndf=128,ngf=128\n",
            "  hostname: bd0789e721af\n",
            "  iterations_since_restore: 1\n",
            "  lossD: 79.15614318847656\n",
            "  lossG: 100.0\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 1265\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 558.8177676200867\n",
            "  time_this_iter_s: 558.8177676200867\n",
            "  time_total_s: 558.8177676200867\n",
            "  timestamp: 1625982569\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 1\n",
            "  trial_id: 228f8_00001\n",
            "  \n",
            "== Status ==\n",
            "Memory usage on this node: 5.1/12.7 GiB\n",
            "PopulationBasedTraining: 6 checkpoints, 2 perturbs\n",
            "Resources requested: 0/2 CPUs, 0/1 GPUs, 0.0/7.51 GiB heap, 0.0/3.75 GiB objects (0.0/1.0 GPU_group_0_ecd3f63ef35a3c653f2d69781ac9fe69, 0.0/2.0 CPU_group_0_ecd3f63ef35a3c653f2d69781ac9fe69, 0.0/1.0 GPU_group_ecd3f63ef35a3c653f2d69781ac9fe69, 0.0/2.0 CPU_group_ecd3f63ef35a3c653f2d69781ac9fe69, 0.0/1.0 accelerator_type:T4)\n",
            "Current best trial: 228f8_00007 with auroc=0.5541192889213562 and parameters={'learning_rate': 0.0001, 'ngf': 128, 'ndf': 64, 'batch_size': 64}\n",
            "Result logdir: /content/drive/MyDrive/Logs/Stage1_pbt_F\n",
            "Number of trials: 10/10 (1 ERROR, 9 TERMINATED)\n",
            "+------------------------------------------+------------+-------+-----------------+-------+-------+--------------+-----------+---------------+----------+---------------+----------------------+\n",
            "| Trial name                               | status     | loc   |   learning_rate |   ngf |   ndf |   batch_size |     lossG |         lossD |    auroc |   auroc_cross |   training_iteration |\n",
            "|------------------------------------------+------------+-------+-----------------+-------+-------+--------------+-----------+---------------+----------+---------------+----------------------|\n",
            "| train_Stage1_tune_checkpoint_228f8_00000 | TERMINATED |       |     0.0001      |   128 |    32 |           64 |   5.04182 |   0.0528528   | 0.461844 |      0.473679 |                    1 |\n",
            "| train_Stage1_tune_checkpoint_228f8_00002 | TERMINATED |       |     0.0001      |    32 |    64 |           64 |   8.43246 |   0.000612127 | 0.474643 |      0.489745 |                    1 |\n",
            "| train_Stage1_tune_checkpoint_228f8_00003 | TERMINATED |       |     0.0001      |    32 |    64 |           64 |   9.28841 |   0.000287676 | 0.50005  |      0.499648 |                    1 |\n",
            "| train_Stage1_tune_checkpoint_228f8_00004 | TERMINATED |       |     0.0001      |    32 |    64 |           64 |   9.67261 |   0.000125631 | 0.4998   |      0.495837 |                    1 |\n",
            "| train_Stage1_tune_checkpoint_228f8_00005 | TERMINATED |       |     0.0001      |   128 |    64 |           64 |   0       | 100           | 0.502287 |      0.491941 |                    1 |\n",
            "| train_Stage1_tune_checkpoint_228f8_00006 | TERMINATED |       |     0.0001      |    32 |    64 |           64 |   8.94812 |   0.00027926  | 0.500835 |      0.499812 |                    1 |\n",
            "| train_Stage1_tune_checkpoint_228f8_00007 | TERMINATED |       |     0.0001      |   128 |    64 |           64 |   5.99285 |   0.0128835   | 0.554119 |      0.552053 |                    1 |\n",
            "| train_Stage1_tune_checkpoint_228f8_00008 | TERMINATED |       |     0.000413361 |    32 |    64 |          128 |   7.81701 |   0.000731912 | 0.465095 |      0.5      |                    1 |\n",
            "| train_Stage1_tune_checkpoint_228f8_00009 | TERMINATED |       |     0.00012     |   128 |    64 |           32 |   6.17881 |   0.00404684  | 0.436249 |      0.402722 |                    1 |\n",
            "| train_Stage1_tune_checkpoint_228f8_00001 | ERROR      |       |     0.0001      |   128 |   128 |           64 | 100       |  79.1561      | 0.309535 |      0.381914 |                    1 |\n",
            "+------------------------------------------+------------+-------+-----------------+-------+-------+--------------+-----------+---------------+----------+---------------+----------------------+\n",
            "Number of errored trials: 1\n",
            "+------------------------------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name                               |   # failures | error file                                                                                                                        |\n",
            "|------------------------------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------|\n",
            "| train_Stage1_tune_checkpoint_228f8_00001 |            1 | /content/drive/MyDrive/Logs/Stage1_pbt_F/train_Stage1_tune_checkpoint_228f8_00001_1_ndf=128,ngf=128_2021-07-11_05-30-43/error.txt |\n",
            "+------------------------------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TuneError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTuneError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-509686d32ba6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;31m# tune_Stage1_asha(num_samples=12, num_epochs=5, gpus_per_trial=1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;31m# Population based training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m         \u001b[0mtune_Stage1_pbt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpus_per_trial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-17-509686d32ba6>\u001b[0m in \u001b[0;36mtune_Stage1_pbt\u001b[0;34m(num_samples, num_epochs, gpus_per_trial)\u001b[0m\n\u001b[1;32m    108\u001b[0m             ),\n\u001b[1;32m    109\u001b[0m         \u001b[0mfail_fast\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0mnum_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0;31m# resume='PROMPT',\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ray/tune/tune.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(run_or_experiment, name, metric, mode, stop, time_budget_s, config, resources_per_trial, num_samples, local_dir, search_alg, scheduler, keep_checkpoints_num, checkpoint_score_attr, checkpoint_freq, checkpoint_at_end, verbose, progress_reporter, log_to_file, trial_name_creator, trial_dirname_creator, sync_config, export_formats, max_failures, fail_fast, restore, server_port, resume, queue_trials, reuse_actors, trial_executor, raise_on_failed_trial, callbacks, loggers, ray_auto_init, run_errored_only, global_checkpoint_period, with_server, upload_dir, sync_to_cloud, sync_to_driver, sync_on_checkpoint, _remote)\u001b[0m\n\u001b[1;32m    541\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mincomplete_trials\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mraise_on_failed_trial\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msignal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSIGINT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 543\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTuneError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Trials did not complete\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincomplete_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    544\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Trials did not complete: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincomplete_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTuneError\u001b[0m: ('Trials did not complete', [train_Stage1_tune_checkpoint_228f8_00001])"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fH13iVqaKgq",
        "outputId": "00636f93-649c-4362-b06e-c0d39e16f1fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!cat /content/drive/MyDrive/Logs/Stage1_pbt_F/train_Stage1_tune_checkpoint_228f8_00001_1_ndf=128,ngf=128_2021-07-11_05-30-43/error.txt"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Failure # 1 (occurred at 2021-07-11_07-10-39)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ray/tune/trial_runner.py\", line 718, in _process_trial\n",
            "    results = self.trial_executor.fetch_result(trial)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ray/tune/ray_trial_executor.py\", line 688, in fetch_result\n",
            "    result = ray.get(trial_future[0], timeout=DEFAULT_GET_TIMEOUT)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ray/_private/client_mode_hook.py\", line 62, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ray/worker.py\", line 1495, in get\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(TuneError): \u001b[36mray::ImplicitFunc.train_buffered()\u001b[39m (pid=2978, ip=172.28.0.2)\n",
            "  File \"python/ray/_raylet.pyx\", line 501, in ray._raylet.execute_task\n",
            "  File \"python/ray/_raylet.pyx\", line 451, in ray._raylet.execute_task.function_executor\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ray/_private/function_manager.py\", line 563, in actor_method_executor\n",
            "    return method(__ray_actor, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ray/tune/trainable.py\", line 173, in train_buffered\n",
            "    result = self.train()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ray/tune/trainable.py\", line 232, in train\n",
            "    result = self.step()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ray/tune/function_runner.py\", line 366, in step\n",
            "    self._report_thread_runner_error(block=True)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ray/tune/function_runner.py\", line 513, in _report_thread_runner_error\n",
            "    (\"Trial raised an exception. Traceback:\\n{}\".format(err_tb_str)\n",
            "ray.tune.error.TuneError: Trial raised an exception. Traceback:\n",
            "\u001b[36mray::ImplicitFunc.train_buffered()\u001b[39m (pid=2978, ip=172.28.0.2)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ray/tune/function_runner.py\", line 248, in run\n",
            "    self._entrypoint()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ray/tune/function_runner.py\", line 316, in entrypoint\n",
            "    self._status_reporter.get_checkpoint())\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ray/tune/function_runner.py\", line 580, in _trainable_func\n",
            "    output = fn()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ray/tune/utils/trainable.py\", line 331, in inner\n",
            "    trainable(config, **fn_kwargs)\n",
            "  File \"<ipython-input-17-509686d32ba6>\", line 37, in train_Stage1_tune_checkpoint\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/core/saving.py\", line 199, in _load_model_state\n",
            "    model = cls(**_cls_kwargs)\n",
            "  File \"<ipython-input-14-52f37f71553b>\", line 3, in __init__\n",
            "TypeError: __init__() got multiple values for keyword argument 'feature_maps_gen'\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrM5BifUgMgA"
      },
      "source": [
        "def pretrained_Stage1s():\n",
        "    ckptf = pl_load(os.path.join(\n",
        "        '/content/drive/MyDrive/Logs/Stage1_F/train_Stage1_tune_checkpoint_',\n",
        "        'checkpoint'), map_location=lambda storage, loc: storage)\n",
        "    f = Stage1._load_model_state(ckptf, config={'batch_size': , 'ngf': , 'ndf': , 'learning_rate': })\n",
        "    ckptj = pl_load(os.path.join(\n",
        "        '/content/drive/MyDrive/Logs/Stage1_J/train_Stage1_tune_checkpoint_',\n",
        "        'checkpoint'), map_location=lambda storage, loc: storage)\n",
        "    j = Stage1._load_model_state(ckptj, config={'batch_size': , 'ngf': , 'ndf': , 'learning_rate': })\n",
        "    return f, j"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMwzBib_zulo"
      },
      "source": [
        "class Generator2(nn.Module):\n",
        "    def __init__(self, ngf: int = 128, image_channels: int = 1):\n",
        "        super().__init__()\n",
        "\n",
        "        ker, strd = 4, 2\n",
        "        pad = int((ker - 2)/2)\n",
        "        res_ker, res_strd, res_pad = 3, 1, 1\n",
        "        \n",
        "        # 64 -> 32\n",
        "        self.preprocessing = nn.Sequential(\n",
        "            nn.Conv2d(image_channels, ngf, ker, strd, pad, bias=False),\n",
        "            nn.ReLU(True)\n",
        "        )\n",
        "        # residuals\n",
        "        self.residual = nn.Sequential(\n",
        "            BasicBlock(ngf, ngf),\n",
        "            BasicBlock(ngf, ngf),\n",
        "            BasicBlock(ngf, ngf),\n",
        "            BasicBlock(ngf, ngf),\n",
        "            BasicBlock(ngf, ngf),\n",
        "            BasicBlock(ngf, ngf),\n",
        "        )\n",
        "        self.ending_residual = nn.Sequential(\n",
        "            nn.Conv2d(ngf, ngf, res_ker, res_strd, res_pad, bias=False),\n",
        "            nn.BatchNorm2d(ngf),\n",
        "            nn.ReLU(True)\n",
        "        )\n",
        "\n",
        "        # at this part, add the residual inputs from after the preprocessing\n",
        "\n",
        "        image_width = 150 # upscaling should be factor of 2 increase\n",
        "        mode = 'nearest' # upscaling method is nearest-neighbour\n",
        "        self.main = nn.Sequential(\n",
        "            # 32 -> 64\n",
        "            nn.Upsample(image_width//2, mode=mode),\n",
        "            nn.Conv2d(ngf, ngf*4, res_ker, res_strd, res_pad, bias=False),\n",
        "            nn.BatchNorm2d(ngf*4),\n",
        "            nn.ReLU(True),\n",
        "            # 64 -> 128\n",
        "            nn.Upsample(image_width, mode=mode),\n",
        "            nn.Conv2d(ngf*4, image_channels, res_ker, res_strd, res_pad, bias=False),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, in_x):\n",
        "        x_p = self.preprocessing(in_x)\n",
        "        x_r = x_p\n",
        "        x_r = self.residual(x_r)\n",
        "        x_r = self.ending_residual(x_r)\n",
        "        # large residual connections\n",
        "        x_f = x_r + x_p\n",
        "        return self.main(x_f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snK9DNDARgi7"
      },
      "source": [
        "class Stage2(DCGAN):\n",
        "    def __init__(self, checkpoint_dir: str, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.save_hyperparameters()\n",
        "\n",
        "        self.generator = Generator2(self.hparams.feature_maps_gen, self.hparams.image_channels)\n",
        "        \n",
        "        extra = self.discriminator._make_disc_block(self.hparams.feature_maps_disc * 2, self.hparams.feature_maps_disc * 2)\n",
        "        l = list(self.discriminator.disc)\n",
        "        l.insert(2, extra)\n",
        "        self.discriminator.disc = nn.Sequential(*l)\n",
        "        self.discriminator.apply(self._weights_init)        \n",
        "\n",
        "    def forward(self, noise):\n",
        "        return self.generator(noise)\n",
        "\n",
        "    def _disc_step(self, real: torch.Tensor) -> torch.Tensor:\n",
        "        disc_loss = self._get_disc_loss(real)\n",
        "        self.log('Stage2/D/train/loss', disc_loss, on_epoch=True)\n",
        "        return disc_loss\n",
        "\n",
        "    def _gen_step(self, real: torch.Tensor) -> torch.Tensor:\n",
        "        gen_loss = self._get_gen_loss(real)\n",
        "        self.log('Stage2/G/train/loss', gen_loss, on_epoch=True)\n",
        "        return gen_loss\n",
        "\n",
        "    def _get_noise(self, n_samples: int, latent_dim: int):\n",
        "        # Currently, this leads to errors:\n",
        "        # model = StackGAN.load_from_checkpoint(\n",
        "        #     os.path.join(checkpoint, 'checkpoint'))\n",
        "        # Workaround:\n",
        "        self.lowres = getattr(self, 'lowres', pretrained_Stage1s()[0])          # Choose the data this was pre-trained on\n",
        "        return self.lowres(torch.randn(n_samples, latent_dim, device=self.device))\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        imgs, labels = batch\n",
        "        out = self(torch.randn(labels.shape[0], self.hparams.latent_dim).type_as(imgs), labels)\n",
        "        # out = F.interpolate(out_64, 150)\n",
        "        return {'predF': self.modelF(out), 'predJ': self.modelJ(out), 'target': labels}\n",
        "\n",
        "    def validation_epoch_end(self, listofDicts):\n",
        "        target = torch.cat([x['target'] for x in listofDicts])\n",
        "        f, ax = plt.subplots(1,2, subplot_kw={'xlim': [0,1], 'xlabel': 'False Positive Rate', \n",
        "                                              'ylim': [0,1.05], 'ylabel': 'True Positive Rate'},\n",
        "                             figsize=[11, 5])\n",
        "        letters = ['F', 'J']\n",
        "        for l in range(2):\n",
        "            prediction = torch.cat([x['pred' + str(letters[l])] for x in listofDicts])\n",
        "            aurocTensor = tm.functional.auroc(prediction, target, num_classes=self.hparams.num_classes, average=None)\n",
        "            self.log('Stage2/ResNet(' + str(letters[l]) + ')/val/auroc', aurocTensor.min())\n",
        "            fprList, tprList, _ = tm.functional.roc(prediction, target, num_classes=self.hparams.num_classes)\n",
        "            \n",
        "            colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n",
        "            for i, color in zip(range(self.hparams.num_classes), colors):\n",
        "                ax[l].plot(fprList[i].cpu(), tprList[i].cpu(), color=color,\n",
        "                        label='ROC curve of class {0} (area = {1:0.2f})'\n",
        "                        ''.format(i, aurocTensor[i].cpu()))\n",
        "            post_plotting(ax[l])\n",
        "            ax[l].set_title('Multi-class ROC (' + str(letters[l]) + ')')\n",
        "        \n",
        "        f.tight_layout()\n",
        "        self.logger.experiment.add_figure('Stage2/ResNet/val/ROC', f)\n",
        "        f.savefig(str(tune.get_trial_dir()) + 'ROC_epoch_' + str(self.current_epoch) + '.pdf')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50DJjk0VwNhd"
      },
      "source": [
        "# __tune_train_checkpoint_begin\n",
        "def train_Stage2_tune_checkpoint(config, checkpoint_dir=None, num_epochs=10, num_gpus=1):\n",
        "    # data_dir = os.path.expanduser('/content/images/')\n",
        "    trainer = pl.Trainer(\n",
        "        # accumulate_grad_batches=2,\n",
        "        # limit_train_batches=0.20,\n",
        "        # limit_val_batches=0.20,\n",
        "        # num_sanity_val_steps=-1,\n",
        "        max_epochs=num_epochs,\n",
        "        prepare_data_per_node = False,\n",
        "        # If fractional GPUs passed in, convert to int.\n",
        "        gpus=math.ceil(num_gpus),\n",
        "        # tpu_cores = 8,\n",
        "        logger=TensorBoardLogger(save_dir=tune.get_trial_dir(), name='', version='.'),\n",
        "        # progress_bar_refresh_rate=1,\n",
        "        callbacks=[\n",
        "                   TuneReportCheckpointCallback(\n",
        "                       {'lossG': 'Stage2/G/train/loss', \n",
        "                        'lossD': 'Stage2/D/train/loss', \n",
        "                        'auroc': 'Stage2/ResNet(F)/val/auroc', \n",
        "                        'auroc_cross': 'Stage2/ResNet(J)/val/auroc',\n",
        "                        },\n",
        "                   ),\n",
        "        ],\n",
        "        # stochastic_weight_avg=True,\n",
        "        # works with only one optimizer\n",
        "        # benchmark=True,\n",
        "    )\n",
        "    dm = NpyDataModule(config, 64)\n",
        "    if checkpoint_dir:\n",
        "        # Currently, this leads to errors:\n",
        "        # model = Stage2.load_from_checkpoint(\n",
        "        #     os.path.join(checkpoint, 'checkpoint'))\n",
        "        # Workaround:\n",
        "        ckpt = pl_load(os.path.join(checkpoint_dir, 'checkpoint'),\n",
        "                       map_location=lambda storage, loc: storage)\n",
        "        model = Stage2._load_model_state(ckpt, config=config)\n",
        "        trainer.current_epoch = ckpt['epoch']\n",
        "    else:\n",
        "        model = Stage2(config)\n",
        "\n",
        "    trainer.fit(model, dm)\n",
        "# __tune_train_checkpoint_end__\n",
        "\n",
        "\n",
        "# __tune_asha_begin__\n",
        "def tune_Stage2_asha(num_samples=10, num_epochs=10, gpus_per_trial=1):\n",
        "    analysis = tune.run(\n",
        "        tune.with_parameters(train_Stage2_tune_checkpoint,\n",
        "                             num_epochs=num_epochs,\n",
        "                             num_gpus=gpus_per_trial),\n",
        "        name='Stage2_F',\n",
        "        metric='auroc',\n",
        "        mode='max',\n",
        "        config={'learning_rate': tune.choice([1e-5, 1e-4, 1e-3]),\n",
        "                'ngf': tune.choice([128, 64, 32]),\n",
        "                'ndf': tune.choice([128, 64, 32]),\n",
        "                'batch_size': tune.choice([128, 64, 32]),\n",
        "                },\n",
        "        resources_per_trial={'cpu': 2,\n",
        "                             'gpu': gpus_per_trial,\n",
        "                             },\n",
        "        num_samples=num_samples,\n",
        "        local_dir='./drive/MyDrive/Logs',\n",
        "        scheduler = ASHAScheduler(max_t=num_epochs, grace_period=1,  reduction_factor=2),\n",
        "        progress_reporter=CLIReporter(\n",
        "            # overwrite=True,\n",
        "            parameter_columns=['learning_rate', 'ngf', 'ndf', 'batch_size'],\n",
        "            metric_columns=['lossG', 'lossD', 'auroc', 'auroc_cross', 'training_iteration'],\n",
        "            ),\n",
        "        fail_fast = True,\n",
        "        # resume='PROMPT',\n",
        "        )\n",
        "\n",
        "    print('Best hyperparameters found were: ', analysis.best_config)\n",
        "\n",
        "# __tune_asha_end__\n",
        "\n",
        "\n",
        "# __tune_pbt_begin__\n",
        "def tune_Stage2_pbt(num_samples=10, num_epochs=10, gpus_per_trial=1):\n",
        "    analysis = tune.run(\n",
        "        tune.with_parameters(train_Stage2_tune_checkpoint,\n",
        "                             num_epochs=num_epochs,\n",
        "                             num_gpus=gpus_per_trial),\n",
        "        name='Stage2_F',\n",
        "        metric='auroc',\n",
        "        mode='max',\n",
        "        config={'learning_rate': tune.choice([1e-5, 1e-4, 1e-3]),\n",
        "                'ngf': tune.choice([128, 64, 32]),\n",
        "                'ndf': tune.choice([128, 64, 32]),\n",
        "                'batch_size': tune.choice([128, 64, 32]),\n",
        "                },\n",
        "        resources_per_trial={'cpu': 2,\n",
        "                             'gpu': gpus_per_trial,\n",
        "                             },\n",
        "        num_samples=num_samples,\n",
        "        local_dir='./drive/MyDrive/Logs',\n",
        "        scheduler = PopulationBasedTraining(perturbation_interval=4,\n",
        "                                        hyperparam_mutations={\n",
        "                                            'learning_rate': tune.choice([1e-5, 1e-4, 1e-3]),\n",
        "                                            # 'ngf': tune.choice([128, 64, 32]),\n",
        "                                            # 'ndf': tune.choice([128, 64, 32]),\n",
        "                                            'batch_size': tune.choice([128, 64, 32]),\n",
        "                                            },\n",
        "                                        ),\n",
        "        progress_reporter=CLIReporter(\n",
        "            # overwrite=True,\n",
        "            parameter_columns=['learning_rate', 'ngf', 'ndf', 'batch_size'],\n",
        "            metric_columns=['lossG', 'lossD', 'auroc', 'auroc_cross', 'training_iteration'],\n",
        "            ),\n",
        "        fail_fast = True,\n",
        "        # resume='PROMPT',\n",
        "        )\n",
        "\n",
        "    print('Best hyperparameters found were: ', analysis.best_config)\n",
        "\n",
        "# __tune_pbt_end__\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    import argparse\n",
        "\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\n",
        "        '--smoke-test', action='store_true', help='Finish quickly for testing')\n",
        "    args, _ = parser.parse_known_args()\n",
        "\n",
        "    if args.smoke_test:\n",
        "        tune_Stage2_asha(num_samples=1, num_epochs=6, gpus_per_trial=1)\n",
        "        tune_Stage2_pbt(num_samples=1, num_epochs=6, gpus_per_trial=1)\n",
        "    else:\n",
        "        # ASHA scheduler\n",
        "        tune_Stage2_asha(num_samples=12, num_epochs=3, gpus_per_trial=1)\n",
        "        # Population based training\n",
        "        # tune_Stage2_pbt(num_samples=8, num_epochs=5, gpus_per_trial=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gM91irMo1uUR"
      },
      "source": [
        "# StackGAN:\n",
        "Here we define the GAN module, that we shall use to generate representative images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abXKx0xitvoK"
      },
      "source": [
        "class StackGAN(pl.LightningModule):\n",
        "    def __init__(self, config, noise_size: int = 100, image_width = 64,\n",
        "                    num_classes: int = 3, image_channels: int = 1, b1: float = 0.5, **kwargs):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters(ignore = config)\n",
        "        self.feature_maps = config['feature_maps']\n",
        "        self.lr = config['learning_rate']\n",
        "        # -------------------------------------\n",
        "        # Need to create a subclass because we couldn't simply add/remove a layer;\n",
        "        # there are two inputs of the superclas' forward method.\n",
        "        self.G1 = DCGANGenerator(self.hparams.noise_size, self.feature_maps, self.hparams.image_channels).apply(self._weights_init)\n",
        "        l = list(self.G1.gen[0])\n",
        "        del l[1]\n",
        "        self.G1.gen[0] = nn.Sequential(*l)\n",
        "        self.G1.add_module('label_emb', nn.Embedding(self.hparams.num_classes, self.hparams.noise_size))\n",
        "        # ------------------------------------\n",
        "        self.D1 = DCGANDiscriminator(self.feature_maps, self.hparams.image_channels).apply(self._weights_init)\n",
        "        # -------------------------------------\n",
        "        self.G2 = Generator2(self.hparams.image_channels, self.feature_maps).apply(self._weights_init)\n",
        "        # -------------------------------------\n",
        "        self.D2 = DCGANDiscriminator(self.feature_maps, self.hparams.image_channels)\n",
        "        #  steps to mutate the instance, not the class definition\n",
        "        extra = self.D2._make_disc_block(self.feature_maps * 2, self.feature_maps * 2)\n",
        "        l = list(self.D2.disc)\n",
        "        l.insert(2, extra)\n",
        "        self.D2.disc = nn.Sequential(*l)\n",
        "        self.D2.apply(self._weights_init)\n",
        "        # No need for subclassing as the forward method need not be modified.\n",
        "        # -------------------------------------\n",
        "        self.R = LensResnet(config, num_classes = 4).apply(self._weights_init)\n",
        "        # -------------------------------------\n",
        "        self.pretrained = LensResnet(config)\n",
        "        ckpt = pl_load(os.path.join(\n",
        "            '/content/drive/MyDrive/Logs/tune_LensResnet_asha_model_j/train_LensResnet_tune_checkpoint_e38cb_00000_0_batch_size=128,learning_rate=0.001_2021-07-06_17-52-11/checkpoint_epoch=17-step=1406',\n",
        "            # '/content/drive/MyDrive/Logs/tune_LensResnet_asha_model_f/train_LensResnet_tune_checkpoint_e32ba_00000_0_batch_size=64,learning_rate=0.0001_2021-07-06_03-33-10/checkpoint_epoch=14-step=4689',\n",
        "            'checkpoint'),\n",
        "            map_location=lambda storage, loc: storage)\n",
        "        self.pretrained._load_model_state(ckpt)\n",
        "        # -------------------------------------\n",
        "        self.criterion1 = nn.BCELoss()\n",
        "        self.criterion2 = nn.CrossEntropyLoss()\n",
        "\n",
        "    @staticmethod\n",
        "    def _weights_init(m):\n",
        "        classname = m.__class__.__name__\n",
        "        if classname.find('Conv') != -1:\n",
        "            torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
        "        elif classname.find('BatchNorm') != -1:\n",
        "            torch.nn.init.normal_(m.weight, 1.0, 0.02)\n",
        "            torch.nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, noise, labels = None):\n",
        "        if labels is None:\n",
        "            labels = torch.randint(self.hparams.num_classes, noise.shape[:-1])                           # last dimension is the hidden dimension\n",
        "        inp = torch.mul(noise, self.G1.label_emb(labels))\n",
        "        out1 = self.G1(inp.view(-1, inp.shape[-1], 1, 1))\n",
        "        out2 = self.G2(out1.detach())\n",
        "        return out2, out1\n",
        "\n",
        "    def training_step(self, batch, batch_idx, optimizer_idx):\n",
        "        imgs, labels = batch\n",
        "        temp2, temp1 = self(torch.randn(labels.shape[0], self.hparams.noise_size).type_as(imgs), labels)\n",
        "\n",
        "        if optimizer_idx == 0:\n",
        "            loss = self.criterion1(self.D1(temp1), torch.ones_like(labels, dtype=torch.float32))\n",
        "            self.log('G1/train/loss/disc', loss)\n",
        "            loss.add_(self.criterion2(self.R.backbone(self.G2(temp1)), labels))\n",
        "            self.log('G1/train/loss/full', loss)\n",
        "\n",
        "        elif optimizer_idx == 1:\n",
        "            real, fake = self.D1(F.interpolate(imgs, self.hparams.image_width, mode='nearest')), self.D1(temp1.detach())\n",
        "            prediction, target = torch.cat((real, fake)), torch.cat((torch.ones_like(real),torch.zeros_like(fake)))\n",
        "            loss = self.criterion1(prediction, target)\n",
        "            self.log('D1/train/loss', loss)\n",
        "\n",
        "        elif optimizer_idx == 2:\n",
        "            loss = self.criterion1(self.D2(temp2), torch.ones_like(labels, dtype=torch.float32))\n",
        "            self.log('G2/train/loss/disc', loss)\n",
        "            loss.add_(self.criterion2(self.R.backbone(temp2), labels))\n",
        "            self.log('G2/train/loss/full', loss)\n",
        "\n",
        "        elif optimizer_idx == 3:\n",
        "            real, fake = self.D2(imgs), self.D2(temp2.detach())\n",
        "            prediction, target = torch.cat((real, fake)), torch.cat((torch.ones_like(real),torch.zeros_like(fake)))\n",
        "            loss = self.criterion1(prediction, target)\n",
        "            self.log('D2/train/loss', loss)\n",
        "\n",
        "        elif optimizer_idx == 4:\n",
        "            real, fake = self.R.backbone(imgs), self.R.backbone(temp2.detach())\n",
        "            prediction, target = torch.cat((real, fake)), torch.cat((labels, self.hparams.num_classes * torch.ones_like(labels)))\n",
        "            loss = self.criterion2(prediction, target)\n",
        "            self.log('R/train/loss', loss)\n",
        "        \n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        opt_g1 = torch.optim.Adam(self.G1.parameters(), self.lr, (self.hparams.b1, 0.999))\n",
        "        opt_d1 = torch.optim.Adam(self.D1.parameters(), self.lr, (self.hparams.b1, 0.999))\n",
        "        opt_g2 = torch.optim.Adam(self.G2.parameters(), self.lr, (self.hparams.b1, 0.999))\n",
        "        opt_d2 = torch.optim.Adam(self.D2.parameters(), self.lr, (self.hparams.b1, 0.999))\n",
        "        opt_r = torch.optim.Adam(self.R.parameters(), self.lr, (self.hparams.b1, 0.999))\n",
        "        return opt_g1, opt_d1, opt_g2, opt_d2, opt_r\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        imgs, labels = batch\n",
        "        temp2, _ = self(torch.randn(labels.shape[0], self.hparams.noise_size).type_as(imgs), labels)\n",
        "        return {'pred': self.pretrained(temp2.detach()), 'target': labels}\n",
        "\n",
        "    def validation_epoch_end(self, listofDicts):\n",
        "        prediction, target = torch.cat([x['pred'] for x in listofDicts]), torch.cat([x['target'] for x in listofDicts])\n",
        "        aurocTensor = tm.functional.auroc(prediction, target, num_classes=self.hparams.num_classes, average=None)\n",
        "        self.log('Pre/val/auroc', aurocTensor.min())\n",
        "        fprList, tprList, _ = tm.functional.roc(prediction, target, num_classes=self.hparams.num_classes)\n",
        "        \n",
        "        f = plt.figure()\n",
        "        colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n",
        "        for i, color in zip(range(self.hparams.num_classes), colors):\n",
        "            plt.plot(fprList[i].cpu(), tprList[i].cpu(), color=color,\n",
        "                    label='ROC curve of class {0} (area = {1:0.2f})'\n",
        "                    ''.format(i, aurocTensor[i].cpu()))\n",
        "        plt.plot([0, 1], [0, 1], 'k--')\n",
        "        plt.xlim([0.0, 1.0])\n",
        "        plt.ylim([0.0, 1.05])\n",
        "        plt.xlabel('False Positive Rate')\n",
        "        plt.ylabel('True Positive Rate')\n",
        "        plt.title('Multi-class ROC')\n",
        "        plt.legend(loc='lower right')\n",
        "\n",
        "        self.logger.experiment.add_figure('StackGAN/val/ROC', f)\n",
        "        f.savefig(str(tune.get_trial_dir())+'ROC_epoch_'+str(self.current_epoch)+'.pdf')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfPgHHMdNOzQ"
      },
      "source": [
        "# Tune StackGAN:\n",
        "Here we tune hyperparameters for generating images that resemble the images from input."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdR_s_6zGtp4"
      },
      "source": [
        "# __tune_train_checkpoint_begin\n",
        "def train_StackGAN_tune_checkpoint(config,\n",
        "                                   checkpoint_dir=None,\n",
        "                                   num_epochs=10,\n",
        "                                   num_gpus=1):\n",
        "    data_dir = os.path.expanduser('/content/images/')\n",
        "    trainer = pl.Trainer(\n",
        "        # accumulate_grad_batches=2,\n",
        "        # limit_train_batches=0.20,\n",
        "        # limit_val_batches=0.20,\n",
        "        num_sanity_val_steps=-1,\n",
        "        max_epochs=num_epochs,\n",
        "        prepare_data_per_node = False,\n",
        "        # If fractional GPUs passed in, convert to int.\n",
        "        gpus=math.ceil(num_gpus),\n",
        "        # tpu_cores = 8,\n",
        "        logger=TensorBoardLogger(save_dir=tune.get_trial_dir(), name='', version='.'),\n",
        "        # progress_bar_refresh_rate=1,\n",
        "        callbacks=[\n",
        "            TuneReportCheckpointCallback(\n",
        "                metrics={\n",
        "                    'lossG1': 'G1/train/loss/full',\n",
        "                    'lossG2': 'G2/train/loss/full',\n",
        "                    'lossD1': 'D1/train/loss',\n",
        "                    'lossD2': 'D2/train/loss',\n",
        "                    'lossR': 'R/train/loss',\n",
        "                    'auroc': 'Pre/val/auroc',\n",
        "                },\n",
        "                filename='checkpoint',\n",
        "                # on='training_end'\n",
        "            )\n",
        "        ],\n",
        "        # stochastic_weight_avg=True,\n",
        "        # works with only one optimizer\n",
        "        )\n",
        "    dm = NpyDataModule(config, data_dir)\n",
        "    if checkpoint_dir:\n",
        "        # Currently, this leads to errors:\n",
        "        # model = StackGAN.load_from_checkpoint(\n",
        "        #     os.path.join(checkpoint, 'checkpoint'))\n",
        "        # Workaround:\n",
        "        ckpt = pl_load(\n",
        "            os.path.join(checkpoint_dir, 'checkpoint'),\n",
        "            map_location=lambda storage, loc: storage)\n",
        "        model = StackGAN._load_model_state(\n",
        "            ckpt, config=config, \n",
        "            # data_dir=data_dir\n",
        "            )\n",
        "        trainer.current_epoch = ckpt['epoch']\n",
        "    else:\n",
        "        model = StackGAN(config)\n",
        "\n",
        "    trainer.fit(model, dm)\n",
        "# __tune_train_checkpoint_end__\n",
        "\n",
        "\n",
        "# __tune_asha_begin__\n",
        "def tune_StackGAN_asha(num_samples=10, num_epochs=10, gpus_per_trial=1):\n",
        "    config = {\n",
        "        'learning_rate': tune.choice([1e-4]),\n",
        "        'feature_maps': tune.choice([64]),\n",
        "        'batch_size': tune.choice([128, 64]),\n",
        "    }\n",
        "\n",
        "    scheduler = ASHAScheduler(\n",
        "        max_t=num_epochs,\n",
        "        grace_period=1,\n",
        "        reduction_factor=2)\n",
        "\n",
        "    reporter = CLIReporter(\n",
        "        # overwrite=True,\n",
        "        parameter_columns=['learning_rate', 'feature_maps', 'batch_size'],\n",
        "        metric_columns=['lossG1', 'lossG2', 'lossD1', 'lossD2', 'lossR', 'auroc', 'training_iteration'],\n",
        "        )\n",
        "\n",
        "    analysis = tune.run(\n",
        "        tune.with_parameters(\n",
        "            train_StackGAN_tune_checkpoint,\n",
        "            num_epochs=num_epochs,\n",
        "            num_gpus=gpus_per_trial),\n",
        "        name='tune_StackGAN_asha_model_j',\n",
        "        metric='auroc',\n",
        "        mode='max',\n",
        "        config=config,\n",
        "        resources_per_trial={\n",
        "            'cpu': 2,\n",
        "            'gpu': gpus_per_trial,\n",
        "            # 'tpu': 8,\n",
        "        },\n",
        "        num_samples=num_samples,\n",
        "        local_dir='./drive/MyDrive/Logs',\n",
        "        scheduler=scheduler,\n",
        "        progress_reporter=reporter,\n",
        "        # restore='/content/drive/MyDrive/Logs/tune_StackGAN_1_asha_model_j/train_StackGAN_tune_checkpoint_fa25b_00000_0_batch_size=64,feature_maps=64,learning_rate=0.0001_2021-07-06_20-23-13/checkpoint_epoch=0-step=937',\n",
        "        fail_fast = True,\n",
        "        resume='PROMPT',\n",
        "        )\n",
        "\n",
        "    print('Best hyperparameters found were: ', analysis.best_config)\n",
        "\n",
        "# __tune_asha_end__\n",
        "\n",
        "\n",
        "# __tune_pbt_begin__\n",
        "def tune_StackGAN_pbt(num_samples=10, num_epochs=10, gpus_per_trial=1):\n",
        "    config = {\n",
        "        'learning_rate': 1e-4,\n",
        "        'feature_maps': 64,\n",
        "        'batch_size': 64,\n",
        "    }\n",
        "\n",
        "    scheduler = PopulationBasedTraining(\n",
        "        perturbation_interval=4,\n",
        "        hyperparam_mutations={\n",
        "            'learning_rate': [1e-4, 1e-3],\n",
        "            'feature_maps': [64, 128],\n",
        "            'batch_size': [32, 64, 128]\n",
        "        })\n",
        "\n",
        "    reporter = CLIReporter(\n",
        "        # overwrite=True,\n",
        "        parameter_columns=['learning_rate', 'feature_maps', 'batch_size'],\n",
        "        metric_columns=['lossG1', 'lossG2', 'lossD1', 'lossD2', 'lossR', 'auroc', 'training_iteration'],\n",
        "        )\n",
        "\n",
        "    analysis = tune.run(\n",
        "        # resume=True,\n",
        "        tune.with_parameters(\n",
        "            train_StackGAN_tune_checkpoint,\n",
        "            num_epochs=num_epochs,\n",
        "            num_gpus=gpus_per_trial),\n",
        "        name='tune_StackGAN_pbt_model_j',\n",
        "        metric='auroc',\n",
        "        mode='max',\n",
        "        resources_per_trial={\n",
        "            'cpu': 2,\n",
        "            'gpu': gpus_per_trial,\n",
        "            # 'tpu': 8,\n",
        "        },\n",
        "        config=config,\n",
        "        num_samples=num_samples,\n",
        "        scheduler=scheduler,\n",
        "        progress_reporter=reporter,\n",
        "        local_dir='./drive/MyDrive/Logs',\n",
        "        # restore='/content/drive/MyDrive/Logs/tune_StackGAN_1_asha_model_j/train_StackGAN_tune_checkpoint_fa25b_00000_0_batch_size=64,feature_maps=64,learning_rate=0.0001_2021-07-06_20-23-13/checkpoint_epoch=0-step=937',\n",
        "        fail_fast = True,\n",
        "        # resume='PROMPT',\n",
        "        )\n",
        "\n",
        "    print('Best hyperparameters found were: ', analysis.best_config)\n",
        "\n",
        "# __tune_pbt_end__\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    import argparse\n",
        "\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\n",
        "        '--smoke-test', action='store_true', help='Finish quickly for testing')\n",
        "    args, _ = parser.parse_known_args()\n",
        "\n",
        "    if args.smoke_test:\n",
        "        tune_StackGAN_asha(num_samples=1, num_epochs=6, gpus_per_trial=1)\n",
        "        tune_StackGAN_pbt(num_samples=1, num_epochs=6, gpus_per_trial=1)\n",
        "    else:\n",
        "        # ASHA scheduler\n",
        "        tune_StackGAN_asha(num_samples=2, num_epochs=1, gpus_per_trial=1)\n",
        "        # Population based training\n",
        "        # tune_StackGAN_pbt(num_samples=8, num_epochs=5, gpus_per_trial=1)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}