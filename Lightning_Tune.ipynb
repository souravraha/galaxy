{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lightning_Tune.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "ASOiXXn-36Wf",
        "FbXpnU7Y4Cr9",
        "6ZEFEbtu6y8f"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDzU0Yty6Qsw"
      },
      "source": [
        "# Install packages and import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aMwknUIDcTW1",
        "outputId": "47f6cb0a-a2a6-4090-90c9-2283f0dabd3c"
      },
      "source": [
        "# If you are running on Google Colab, uncomment below to install the necessary dependencies \n",
        "# before beginning the exercise.\n",
        "\n",
        "print(\"Setting up colab environment\")\n",
        "!pip uninstall -y -q pyarrow\n",
        "!pip install -q ray[debug] lightning-bolts\n",
        "!pip install -U -q ray[tune]\n",
        "# !pip install cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.8-cp37-cp37m-linux_x86_64.whl\n",
        "\n",
        "# # A hack to force the runtime to restart, needed to include the above dependencies.\n",
        "# print(\"Done installing! Restarting via forced crash (this is not an issue).\")\n",
        "# import os\n",
        "# os._exit(0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting up colab environment\n",
            "\u001b[K     |████████████████████████████████| 51.6MB 59kB/s \n",
            "\u001b[33m  WARNING: ray 1.4.1 does not provide the extra 'debug'\u001b[0m\n",
            "\u001b[K     |████████████████████████████████| 256kB 56.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 133kB 57.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.3MB 47.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.0MB 52.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 81kB 11.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 10.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 10.1MB 40.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.1MB 32.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 81kB 11.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 819kB 54.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 235kB 58.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 143kB 59.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 296kB 57.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 92kB 11.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 10.6MB 42.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 122kB 57.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 829kB 50.4MB/s \n",
            "\u001b[?25h  Building wheel for gpustat (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: tensorflow 2.5.0 has requirement tensorboard~=2.5, but you'll have tensorboard 2.4.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: pytorch-lightning 1.3.8 has requirement PyYAML<=5.4.1,>=5.1, but you'll have pyyaml 3.13 which is incompatible.\u001b[0m\n",
            "\u001b[K     |████████████████████████████████| 133kB 25.2MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNMd9sh02fG9"
      },
      "source": [
        "# If you are running on Google Colab, please install TensorFlow 2.0 by uncommenting below..\n",
        "\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cR6G_K6bWVqd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d347a77-69ce-4dd4-80b8-f1d06025ae4f"
      },
      "source": [
        "# __import_lightning_begin__\n",
        "import math\n",
        "import gdown, tarfile           #\n",
        "from zipfile import ZipFile\n",
        "import shutil\n",
        "import numpy as np              #\n",
        "from matplotlib import pyplot as plt\n",
        "from itertools import cycle\n",
        "import torch\n",
        "from torch import nn\n",
        "import pytorch_lightning as pl\n",
        "from filelock import FileLock\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torch.nn import functional as F\n",
        "import torchvision.datasets as datasets\n",
        "from torchvision.models import resnet18\n",
        "from pl_bolts.models.self_supervised.resnets import BasicBlock                  # problem with resnet18\n",
        "from pl_bolts.models.gans import DCGAN\n",
        "from pl_bolts.models.gans.dcgan.components import DCGANDiscriminator, DCGANGenerator\n",
        "import torchmetrics as tm\n",
        "from torchvision import transforms\n",
        "import os\n",
        "from os.path import basename\n",
        "# __import_lightning_end__\n",
        "\n",
        "# __import_tune_begin__\n",
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "from pytorch_lightning.utilities.cloud_io import load as pl_load\n",
        "from ray import tune\n",
        "from ray.tune import CLIReporter, JupyterNotebookReporter\n",
        "from ray.tune.schedulers import ASHAScheduler, PopulationBasedTraining\n",
        "from ray.tune.integration.pytorch_lightning import TuneReportCallback, \\\n",
        "    TuneReportCheckpointCallback\n",
        "# __import_tune_end__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ray/autoscaler/_private/cli_logger.py:61: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
            "  \"update your install command.\", FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WK9GeW6miiXr"
      },
      "source": [
        "# Download and extract data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HGZRwWHbXqLu",
        "outputId": "14c9f9dd-fda0-405b-ca96-114ecbaf614b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZUIrgOfbwPe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92be0ae5-c74a-4d74-bf8e-739a90260585"
      },
      "source": [
        "# 'a': 1Cjcw2EWorhdhJSGoWOdxsEUDxvl943dt, 'b': 15yXXC4h5VsytP3Ak1jfUSjQhdgP2s23K, 'c': 1vuQ-pLzoKT4Hd_V7949r9eND9E2fB_u_,\n",
        "# 'd': , 'e': 1wFuasvb7PthxXtMUlsD13uzYHWlWt06H, 'f': 17l6H61tLAu26zGuei38r_T5ssjbYUeaJ, \n",
        "# 'g': 1SxQVosWeEjY3Pyn8LRXA11rLnZ9HK_7B, 'h': 1Atau0RH4oyLAiYReW-G9a8l9pUNltglF, 'i': 15lEgsR1p00KSHieaT9a1nkbJ86pDxwgp, \n",
        "# 'j': 1m0EQUbqZZeyl76XsQIKWU5Qd7jGmmWhB, 'k': , 'l': 1meTDi4aeWfdChOiXeLtUOGhjVDVu000e\n",
        "\n",
        "# !rm -rf images\n",
        "!gdown --id 17l6H61tLAu26zGuei38r_T5ssjbYUeaJ\n",
        "!tar zxf ./model_f.tgz\n",
        "\n",
        "# def prepare_data(data_dir: str = '/content'):\n",
        "#     gdown.download('https://drive.google.com/uc?id=17l6H61tLAu26zGuei38r_T5ssjbYUeaJ', data_dir+'/model_f.tgz', quiet=True)\n",
        "    \n",
        "#     temp = tarfile.open(data_dir+'/model_f.tgz', 'r|gz')\n",
        "#     temp.extractall()\n",
        "#     temp.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=17l6H61tLAu26zGuei38r_T5ssjbYUeaJ\n",
            "To: /content/model_f.tgz\n",
            "2.34GB [00:27, 85.1MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Snbv_zoNiWfW"
      },
      "source": [
        "# DataModule\n",
        "This creates dataloaders which need to be supplied to train, validate or test the module we have."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yItuGxXmXzGr"
      },
      "source": [
        "class NpyDataModule(pl.LightningDataModule):\n",
        "\n",
        "    def __init__(self, config, data_dir: str = '/content/images/', img_width: int = 150):\n",
        "        super().__init__()\n",
        "        # This method is not implemented\n",
        "        # self.save_hyperparameters()\n",
        "        self.batch_size = config['batch_size']\n",
        "        self.data_dir = os.path.expanduser(data_dir)\n",
        "        \n",
        "        GLOBAL = np.load('/content/drive/MyDrive/git_repos/forging_new_worlds/GLOBAL_VALS_F.npz')\n",
        "        self.transform = transforms.Compose([\n",
        "            # transforms.ConvertImageDtype(torch.float32),\n",
        "            # Can't use this, divides values by dtype.max, use float() in npyloader instead\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.RandomVerticalFlip(),\n",
        "            transforms.Normalize(mean=(GLOBAL['VALS'][0],), std=(GLOBAL['VALS'][1],)),\n",
        "            # this shift-scales the pixel values, N(mu, sigma) -> N(0, 1)\n",
        "            transforms.Resize(img_width, transforms.InterpolationMode.NEAREST),\n",
        "        ])\n",
        "    \n",
        "    @staticmethod\n",
        "    def npy_loader(path):\n",
        "        # s=np.load(path).astype('float',copy=False)\n",
        "        return torch.from_numpy(np.load(path)).unsqueeze(0).float()\n",
        "        # Convert to tenssor first, and then to float, otherwise final dtype \n",
        "        # would be float64, which would raise errors in conv layers      ###### type as\n",
        "\n",
        "    def setup(self, stage: str = None):\n",
        "        if stage in ('fit', None):\n",
        "            self.full_set = datasets.DatasetFolder(os.path.join(self.data_dir,'train'),\n",
        "                                                   self.npy_loader, \n",
        "                                                   ('.npy'), \n",
        "                                                   self.transform,\n",
        "                                                   )\n",
        "            self.train_set, self.val_set = random_split(self.full_set, [60000, 15000])            \n",
        "            self.dims = tuple(self.train_set[0][0].shape)\n",
        "\n",
        "        if stage in ('test', None):\n",
        "            self.test_set = datasets.DatasetFolder(os.path.join(self.data_dir,'val'), \n",
        "                                                   self.npy_loader, \n",
        "                                                   ('.npy'), \n",
        "                                                   self.transform,\n",
        "                                                   )\n",
        "            self.dims = getattr(self, 'dims', self.test_set[0][0].shape)\n",
        "    \n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(self.train_set, self.batch_size, shuffle=True)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(self.val_set, self.batch_size, shuffle=True)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(self.test_set, self.batch_size, shuffle=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0bm1afc11hN"
      },
      "source": [
        "# ResNet:\n",
        "We modify a ResNet slightly for our purpose."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ojZ0yT4z168p"
      },
      "source": [
        "class LensResnet(pl.LightningModule):\n",
        "\n",
        "    def __init__(self, config, data_dir: str = '/content/images/', image_channels: int = 1, \n",
        "                 num_classes: int = 3, **kwargs):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters(ignore=config)\n",
        "        self.learning_rate = config['learning_rate']\n",
        "\n",
        "        # init a pretrained resnet\n",
        "        self.backbone = resnet18(num_classes = self.hparams.num_classes)\n",
        "        self.backbone.conv1 = nn.Conv2d(self.hparams.image_channels, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
        "        #  can't merely change the in_channels since weights have to changed as well\n",
        "        self.backbone.fc = nn.Sequential(\n",
        "            nn.Dropout(0.5),\n",
        "            self.backbone.fc\n",
        "        )\n",
        "        # self.backbone.\n",
        "        # metrics = tm.MetricCollection([\n",
        "        #     # tm.AUROC(self.hparams.num_classes, average='weighted'),\n",
        "        #     # tm.ROC(self.hparams.num_classes),\n",
        "        # #     tm.PrecisionRecallCurve(self.hparams.num_classes),\n",
        "        # ])\n",
        "        # self.train_metrics = metrics.clone(prefix='ResNet/train/')\n",
        "        # self.val_metrics = metrics.clone(prefix='ResNet/val/')\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.Adam(self.backbone.parameters(), self.learning_rate)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return F.softmax(self.backbone(x), 1)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        imgs, labels = batch\n",
        "        self.log('ResNet/train/auroc', tm.functional.auroc(self(imgs),labels, average='weighted', num_classes=self.hparams.num_classes))\n",
        "        loss = F.cross_entropy(self.backbone(imgs), labels)\n",
        "        self.log('ResNet/train/loss', loss)\n",
        "        #  keep only scalars here, for no errors\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        imgs, labels = batch\n",
        "        self.log('ResNet/val/loss', F.cross_entropy(self.backbone(imgs), labels))\n",
        "        #  keep only scalars here, for no errors\n",
        "        return {'pred': self(imgs), 'target': labels}\n",
        "\n",
        "    def validation_epoch_end(self, Listofdicts):\n",
        "        prediction, target = torch.cat([x[\"pred\"] for x in Listofdicts]), torch.cat([x[\"target\"] for x in Listofdicts])\n",
        "        aurocTensor = tm.functional.auroc(prediction, target, num_classes=self.hparams.num_classes, average=None)\n",
        "        self.log('ResNet/val/auroc', aurocTensor.min())\n",
        "        fprList, tprList, _ = tm.functional.roc(prediction, target, num_classes=self.hparams.num_classes)\n",
        "        \n",
        "        f = plt.figure()\n",
        "        colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n",
        "        for i, color in zip(range(self.hparams.num_classes), colors):\n",
        "            plt.plot(fprList[i].cpu(), tprList[i].cpu(), color=color,\n",
        "                    label='ROC curve of class {0} (area = {1:0.2f})'\n",
        "                    ''.format(i, aurocTensor[i].cpu()))\n",
        "        plt.plot([0, 1], [0, 1], 'k--')\n",
        "        plt.xlim([0.0, 1.0])\n",
        "        plt.ylim([0.0, 1.05])\n",
        "        plt.xlabel('False Positive Rate')\n",
        "        plt.ylabel('True Positive Rate')\n",
        "        plt.title('Multi-class ROC')\n",
        "        plt.legend(loc=\"lower right\")\n",
        "\n",
        "        self.logger.experiment.add_figure('ResNet/val/ROC', f)\n",
        "        f.savefig(str(tune.get_trial_dir())+'ROC_epoch_'+str(self.current_epoch)+'.pdf')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASOiXXn-36Wf"
      },
      "source": [
        "# Trying out Auto Tuning of learning rate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5FtDpDdAADLi",
        "outputId": "b6aad2ed-5af4-4348-a0dd-fc77b6d0a38f"
      },
      "source": [
        "# Can't work with multiple optimizers\n",
        "config = {\n",
        "    'learning_rate': 1e-4, 'batch_size': 128, 'feature_maps': 64,\n",
        "}\n",
        "dm = NpyDataModule(config)\n",
        "generator = StackGAN(config)\n",
        "\n",
        "trainer = pl.Trainer(\n",
        "    # logger=,\n",
        "    # checkpoint_callback=,\n",
        "    default_root_dir='./drive/MyDrive/Logs/', \n",
        "    gpus=1,\n",
        "    auto_select_gpus=True, \n",
        "    # tpu_cores=\n",
        "    progress_bar_refresh_rate=1,\n",
        "    # fast_dev_run=,\n",
        "    max_epochs=5,\n",
        "    # max_time=,\n",
        "    # limit_train_batches=,\n",
        "    # flush_logs_every_n_steps=,\n",
        "    # log_every_n_steps=,\n",
        "    # resume_from_checkpoint='./drive/MyDrive/Logs/lr_find_temp_model.ckpt',\n",
        "    auto_lr_find = True,\n",
        "    # auto_scale_batch_size=True,\n",
        "    # prepare_data_per_node=,\n",
        "    )\n",
        "\n",
        "# Run learning rate finder\n",
        "lr_finder = trainer.tuner.lr_find(generator, dm)\n",
        "\n",
        "# # Results can be found in\n",
        "# # lr_finder.results\n",
        "\n",
        "# Plot with\n",
        "fig = lr_finder.plot(suggest=True)\n",
        "fig.show()\n",
        "\n",
        "# Pick point based on plot, or get suggestion\n",
        "new_lr = lr_finder.suggestion()\n",
        "\n",
        "# # update hparams of the model\n",
        "# model.hparams.lr = new_lr\n",
        "\n",
        "# # Fit model\n",
        "# trainer.fit(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-bf2b0c79af82>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# Run learning rate finder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mlr_finder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtuner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_find\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m# # Results can be found in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/tuner/tuning.py\u001b[0m in \u001b[0;36mlr_find\u001b[0;34m(self, model, train_dataloader, val_dataloaders, datamodule, min_lr, max_lr, num_training, mode, early_stop_threshold, update_attr)\u001b[0m\n\u001b[1;32m    195\u001b[0m                 \u001b[0;34m'mode'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0;34m'early_stop_threshold'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mearly_stop_threshold\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m                 \u001b[0;34m'update_attr'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mupdate_attr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m             }\n\u001b[1;32m    199\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mtune\u001b[0;34m(self, model, train_dataloader, val_dataloaders, datamodule, scale_batch_size_kwargs, lr_find_kwargs)\u001b[0m\n\u001b[1;32m    686\u001b[0m         )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtuner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tune\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_batch_size_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscale_batch_size_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_find_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr_find_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/tuner/tuning.py\u001b[0m in \u001b[0;36m_tune\u001b[0;34m(self, model, scale_batch_size_kwargs, lr_find_kwargs)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_lr_find\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mlr_find_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'update_attr'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m             \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr_find'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr_find\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mlr_find_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainerStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFINISHED\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/tuner/lr_finder.py\u001b[0m in \u001b[0;36mlr_find\u001b[0;34m(trainer, model, min_lr, max_lr, num_training, mode, early_stop_threshold, update_attr)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;31m# Fit, lr & loss logged in callback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtuner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m     \u001b[0;31m# Prompt if we stopped early\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/tuner/tuning.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainerStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRUNNING\u001b[0m  \u001b[0;31m# last `_run` call might have set it to `FINISHED`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtuning\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    715\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_setup_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# allow user to setup lightning_module in accelerator environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_configure_sharded_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# allow user to setup in model sharded environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 717\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# note: this sets up self.lightning_module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    718\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m         \u001b[0;31m# ----------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/accelerators/gpu.py\u001b[0m in \u001b[0;36msetup\u001b[0;34m(self, trainer, model)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_nvidia_flags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocal_rank\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_train_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/accelerators/accelerator.py\u001b[0m in \u001b[0;36msetup\u001b[0;34m(self, trainer, model)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_training_type_plugin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_type_plugin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_type_plugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_optimizers_in_pre_dispatch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_optimizers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_precision_plugin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprecision_plugin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/accelerators/accelerator.py\u001b[0m in \u001b[0;36msetup_optimizers\u001b[0;34m(self, trainer)\u001b[0m\n\u001b[1;32m    373\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m         optimizers, lr_schedulers, optimizer_frequencies = self.training_type_plugin.init_optimizers(\n\u001b[0;32m--> 375\u001b[0;31m             \u001b[0mtrainer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    376\u001b[0m         )\n\u001b[1;32m    377\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\u001b[0m in \u001b[0;36minit_optimizers\u001b[0;34m(self, trainer, model)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minit_optimizers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'pl.Trainer'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'pl.LightningModule'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_optimizers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0moptimizer_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_closure\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/optimizers.py\u001b[0m in \u001b[0;36minit_optimizers\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minit_optimizers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mLightningModule\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lightning_optimizers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0moptim_conf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfigure_optimizers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moptim_conf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             rank_zero_warn(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/tuner/lr_finder.py\u001b[0m in \u001b[0;36mfunc\u001b[0;34m()\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m             \u001b[0mnew_lrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_min\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mparam_group\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_lr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_lrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m                 \u001b[0mparam_group\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"lr\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_lr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'param_groups'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbXpnU7Y4Cr9"
      },
      "source": [
        "# Tune ResNet hyperparameters:\n",
        "Here we tune hyperparameters as we train our modified ResNet."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33OXrLjWn0II",
        "outputId": "8999ae13-ac1b-4588-ffc0-d7d87f64fa76"
      },
      "source": [
        "# __tune_train_checkpoint_begin\n",
        "def train_LensResnet_tune_checkpoint(config,\n",
        "                                    checkpoint_dir=None,\n",
        "                                    num_epochs=10,\n",
        "                                    num_gpus=1):\n",
        "    data_dir = os.path.expanduser(\"/content/images/\")\n",
        "\n",
        "    trainer = pl.Trainer(\n",
        "        max_epochs=num_epochs,\n",
        "        prepare_data_per_node = False,\n",
        "        num_sanity_val_steps=0,\n",
        "        # If fractional GPUs passed in, convert to int.\n",
        "        gpus=math.ceil(num_gpus),\n",
        "        # tpu_cores = 8,\n",
        "        logger=TensorBoardLogger(\n",
        "            save_dir=tune.get_trial_dir(), name=\"\", version=\".\"),\n",
        "        # progress_bar_refresh_rate=1,\n",
        "        callbacks=[\n",
        "            TuneReportCheckpointCallback(\n",
        "                metrics={\n",
        "                    \"loss\": \"ResNet/val/loss\",\n",
        "                    \"auroc\": \"ResNet/val/auroc\",\n",
        "                },\n",
        "                filename=\"checkpoint\",\n",
        "                # on=\"validation_end\"\n",
        "            )\n",
        "        ],\n",
        "        stochastic_weight_avg=True,\n",
        "    )\n",
        "\n",
        "    dm = NpyDataModule(config, data_dir)\n",
        "    \n",
        "    if checkpoint_dir:\n",
        "        # Currently, this leads to errors:\n",
        "        # model = LensResnet.load_from_checkpoint(\n",
        "        #     os.path.join(checkpoint, \"checkpoint\"))\n",
        "        # Workaround:\n",
        "        ckpt = pl_load(\n",
        "            os.path.join(checkpoint_dir, \"checkpoint\"),\n",
        "            map_location=lambda storage, loc: storage)\n",
        "        model = LensResnet._load_model_state(\n",
        "            ckpt, config=config, \n",
        "            # data_dir=data_dir\n",
        "            )\n",
        "        trainer.current_epoch = ckpt[\"epoch\"]\n",
        "    else:\n",
        "        model = LensResnet(config, \n",
        "                        #  data_dir\n",
        "                         )\n",
        "\n",
        "    trainer.fit(model, dm)\n",
        "\n",
        "# __tune_train_checkpoint_end__\n",
        "\n",
        "\n",
        "# __tune_asha_begin__\n",
        "def tune_LensResnet_asha(num_samples=10, num_epochs=10, gpus_per_trial=1):\n",
        "    config = {\n",
        "        \"learning_rate\": tune.choice([1e-5, 1e-4, 1e-3, 1e-2]),\n",
        "        \"batch_size\": tune.choice([128, 64, 32]),\n",
        "    }\n",
        "\n",
        "    scheduler = ASHAScheduler(\n",
        "        max_t=num_epochs,\n",
        "        grace_period=1,\n",
        "        reduction_factor=2)\n",
        "\n",
        "    reporter = CLIReporter(\n",
        "        # overwrite=True,\n",
        "        parameter_columns=[\"learning_rate\", \"batch_size\"],\n",
        "        metric_columns=[\"loss\", \"auroc\", \"training_iteration\"])\n",
        "\n",
        "    analysis = tune.run(\n",
        "        tune.with_parameters(\n",
        "            train_LensResnet_tune_checkpoint,\n",
        "            num_epochs=num_epochs,\n",
        "            num_gpus=gpus_per_trial),\n",
        "        name=\"LensResNet_F\",\n",
        "        metric=\"auroc\",\n",
        "        mode=\"max\",\n",
        "        config=config,\n",
        "        resources_per_trial={\n",
        "            \"cpu\": 2,\n",
        "            \"gpu\": gpus_per_trial,\n",
        "            # \"tpu\": 8,\n",
        "        },\n",
        "        # num_samples=num_samples,\n",
        "        local_dir='./drive/MyDrive/Logs',\n",
        "        scheduler=scheduler,\n",
        "        progress_reporter=reporter,\n",
        "        fail_fast = True,\n",
        "        # restore = '/content/drive/MyDrive/Logs/tune_LensResnet_asha_model_j/train_LensResnet_tune_checkpoint_e38cb_00000_0_batch_size=128,learning_rate=0.001_2021-07-06_17-52-11/checkpoint_tmp208560',\n",
        "        # '/content/drive/MyDrive/Logs/tune_LensResnet_asha_model_f/train_LensResnet_tune_checkpoint_e32ba_00000_0_batch_size=64,learning_rate=0.0001_2021-07-06_03-33-10/checkpoint_epoch=14-step=4689',\n",
        "        resume='PROMPT',\n",
        "        )\n",
        "\n",
        "    print(\"Best hyperparameters found were: \", analysis.best_config)\n",
        "# __tune_asha_end__\n",
        "\n",
        "\n",
        "# __tune_pbt_begin__\n",
        "def tune_LensResnet_pbt(num_samples=10, num_epochs=10, gpus_per_trial=1):\n",
        "    config = {\n",
        "        \"learning_rate\": 1e-3,\n",
        "        \"batch_size\": 64,\n",
        "    }\n",
        "\n",
        "    scheduler = PopulationBasedTraining(\n",
        "        perturbation_interval=4,\n",
        "        hyperparam_mutations={\n",
        "            \"learning_rate\": [1e-5, 1e-4, 1e-3, 1e-2],\n",
        "            \"batch_size\": [32, 64, 128]\n",
        "        })\n",
        "\n",
        "    reporter = CLIReporter(\n",
        "        # overwrite=True,\n",
        "        parameter_columns=[\"learning_rate\", \"batch_size\"],\n",
        "        metric_columns=[\"loss\", \"auroc\", \"training_iteration\"])\n",
        "\n",
        "    analysis = tune.run(\n",
        "        # resume=True,\n",
        "        tune.with_parameters(\n",
        "            train_LensResnet_tune_checkpoint,\n",
        "            num_epochs=num_epochs,\n",
        "            num_gpus=gpus_per_trial),\n",
        "        metric=\"auroc\",\n",
        "        mode=\"max\",\n",
        "        resources_per_trial={\n",
        "            \"cpu\": 2,\n",
        "            \"gpu\": gpus_per_trial,\n",
        "            # \"tpu\": 8,\n",
        "        },\n",
        "        fail_fast = True,\n",
        "        config=config,\n",
        "        num_samples=num_samples,\n",
        "        scheduler=scheduler,\n",
        "        progress_reporter=reporter,\n",
        "        local_dir='./drive/MyDrive/Logs' ,\n",
        "        name=\"tune_LensResnet_pbt\")\n",
        "\n",
        "    print(\"Best hyperparameters found were: \", analysis.best_config)\n",
        "\n",
        "# __tune_pbt_end__\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import argparse\n",
        "\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\n",
        "        \"--smoke-test\", action=\"store_true\", help=\"Finish quickly for testing\")\n",
        "    args, _ = parser.parse_known_args()\n",
        "\n",
        "    if args.smoke_test:\n",
        "        tune_LensResnet_asha(num_samples=1, num_epochs=6, gpus_per_trial=1)\n",
        "        tune_LensResnet_pbt(num_samples=1, num_epochs=6, gpus_per_trial=1)\n",
        "    else:\n",
        "        # ASHA scheduler\n",
        "        tune_LensResnet_asha(num_samples=12, num_epochs=3, gpus_per_trial=1)\n",
        "        # Population based training\n",
        "        # tune_LensResnet_pbt(num_samples=10, num_epochs=10, gpus_per_trial=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "== Status ==\n",
            "Memory usage on this node: 1.5/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 2.000: None | Iter 1.000: None\n",
            "Resources requested: 0/2 CPUs, 0/1 GPUs, 0.0/7.48 GiB heap, 0.0/3.74 GiB objects (0.0/1.0 accelerator_type:T4)\n",
            "Result logdir: /content/drive/MyDrive/Logs/LensResNet_F\n",
            "Number of trials: 12/12 (12 PENDING)\n",
            "+----------------------------------------------+----------+-------+-----------------+--------------+\n",
            "| Trial name                                   | status   | loc   |   learning_rate |   batch_size |\n",
            "|----------------------------------------------+----------+-------+-----------------+--------------|\n",
            "| train_LensResnet_tune_checkpoint_83150_00000 | PENDING  |       |          0.01   |           64 |\n",
            "| train_LensResnet_tune_checkpoint_83150_00001 | PENDING  |       |          0.001  |           64 |\n",
            "| train_LensResnet_tune_checkpoint_83150_00002 | PENDING  |       |          0.001  |          128 |\n",
            "| train_LensResnet_tune_checkpoint_83150_00003 | PENDING  |       |          0.01   |           32 |\n",
            "| train_LensResnet_tune_checkpoint_83150_00004 | PENDING  |       |          1e-05  |          128 |\n",
            "| train_LensResnet_tune_checkpoint_83150_00005 | PENDING  |       |          0.0001 |           64 |\n",
            "| train_LensResnet_tune_checkpoint_83150_00006 | PENDING  |       |          0.0001 |           32 |\n",
            "| train_LensResnet_tune_checkpoint_83150_00007 | PENDING  |       |          1e-05  |           32 |\n",
            "| train_LensResnet_tune_checkpoint_83150_00008 | PENDING  |       |          1e-05  |           32 |\n",
            "| train_LensResnet_tune_checkpoint_83150_00009 | PENDING  |       |          0.0001 |          128 |\n",
            "| train_LensResnet_tune_checkpoint_83150_00010 | PENDING  |       |          0.0001 |           32 |\n",
            "| train_LensResnet_tune_checkpoint_83150_00011 | PENDING  |       |          0.01   |           32 |\n",
            "+----------------------------------------------+----------+-------+-----------------+--------------+\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=623)\u001b[0m GPU available: True, used: True\n",
            "\u001b[2m\u001b[36m(pid=623)\u001b[0m TPU available: False, using: 0 TPU cores\n",
            "\u001b[2m\u001b[36m(pid=623)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "== Status ==\n",
            "Memory usage on this node: 1.9/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 2.000: None | Iter 1.000: None\n",
            "Resources requested: 2.0/2 CPUs, 1.0/1 GPUs, 0.0/7.48 GiB heap, 0.0/3.74 GiB objects (0.0/1.0 accelerator_type:T4)\n",
            "Result logdir: /content/drive/MyDrive/Logs/LensResNet_F\n",
            "Number of trials: 12/12 (11 PENDING, 1 RUNNING)\n",
            "+----------------------------------------------+----------+-------+-----------------+--------------+\n",
            "| Trial name                                   | status   | loc   |   learning_rate |   batch_size |\n",
            "|----------------------------------------------+----------+-------+-----------------+--------------|\n",
            "| train_LensResnet_tune_checkpoint_83150_00000 | RUNNING  |       |          0.01   |           64 |\n",
            "| train_LensResnet_tune_checkpoint_83150_00001 | PENDING  |       |          0.001  |           64 |\n",
            "| train_LensResnet_tune_checkpoint_83150_00002 | PENDING  |       |          0.001  |          128 |\n",
            "| train_LensResnet_tune_checkpoint_83150_00003 | PENDING  |       |          0.01   |           32 |\n",
            "| train_LensResnet_tune_checkpoint_83150_00004 | PENDING  |       |          1e-05  |          128 |\n",
            "| train_LensResnet_tune_checkpoint_83150_00005 | PENDING  |       |          0.0001 |           64 |\n",
            "| train_LensResnet_tune_checkpoint_83150_00006 | PENDING  |       |          0.0001 |           32 |\n",
            "| train_LensResnet_tune_checkpoint_83150_00007 | PENDING  |       |          1e-05  |           32 |\n",
            "| train_LensResnet_tune_checkpoint_83150_00008 | PENDING  |       |          1e-05  |           32 |\n",
            "| train_LensResnet_tune_checkpoint_83150_00009 | PENDING  |       |          0.0001 |          128 |\n",
            "| train_LensResnet_tune_checkpoint_83150_00010 | PENDING  |       |          0.0001 |           32 |\n",
            "| train_LensResnet_tune_checkpoint_83150_00011 | PENDING  |       |          0.01   |           32 |\n",
            "+----------------------------------------------+----------+-------+-----------------+--------------+\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=623)\u001b[0m 2021-07-08 04:40:01.526809: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "\u001b[2m\u001b[36m(pid=623)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=623)\u001b[0m   | Name     | Type   | Params\n",
            "\u001b[2m\u001b[36m(pid=623)\u001b[0m ------------------------------------\n",
            "\u001b[2m\u001b[36m(pid=623)\u001b[0m 0 | backbone | ResNet | 11.2 M\n",
            "\u001b[2m\u001b[36m(pid=623)\u001b[0m ------------------------------------\n",
            "\u001b[2m\u001b[36m(pid=623)\u001b[0m 11.2 M    Trainable params\n",
            "\u001b[2m\u001b[36m(pid=623)\u001b[0m 0         Non-trainable params\n",
            "\u001b[2m\u001b[36m(pid=623)\u001b[0m 11.2 M    Total params\n",
            "\u001b[2m\u001b[36m(pid=623)\u001b[0m 44.687    Total estimated model params size (MB)\n",
            "\u001b[2m\u001b[36m(pid=623)\u001b[0m /usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/data_loading.py:349: UserWarning: Your val_dataloader has `shuffle=True`, it is best practice to turn this off for val/test/predict dataloaders.\n",
            "\u001b[2m\u001b[36m(pid=623)\u001b[0m   f'Your {mode}_dataloader has `shuffle=True`, it is best practice to turn'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=623)\u001b[0m \rTraining: 0it [00:00, ?it/s]\rTraining:   0%|          | 0/1173 [00:00<?, ?it/s]\rEpoch 0:   0%|          | 0/1173 [00:00<?, ?it/s] \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=623)\u001b[0m /usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "\u001b[2m\u001b[36m(pid=623)\u001b[0m   return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0:   2%|▏         | 20/1173 [00:09<09:23,  2.05it/s, loss=1.4, v_num=.]\n",
            "Epoch 0:   3%|▎         | 40/1173 [00:18<08:49,  2.14it/s, loss=1.26, v_num=.]\n",
            "Epoch 0:   5%|▌         | 60/1173 [00:27<08:34,  2.16it/s, loss=1.21, v_num=.]\n",
            "Epoch 0:   7%|▋         | 80/1173 [00:36<08:21,  2.18it/s, loss=1.2, v_num=.] \n",
            "Epoch 0:   9%|▊         | 100/1173 [00:45<08:09,  2.19it/s, loss=1.21, v_num=.]\n",
            "Epoch 0:  10%|█         | 120/1173 [00:54<07:58,  2.20it/s, loss=1.14, v_num=.]\n",
            "Epoch 0:  12%|█▏        | 140/1173 [01:03<07:48,  2.21it/s, loss=1.18, v_num=.]\n",
            "Epoch 0:  14%|█▎        | 160/1173 [01:12<07:38,  2.21it/s, loss=1.14, v_num=.]\n",
            "Epoch 0:  15%|█▌        | 180/1173 [01:21<07:29,  2.21it/s, loss=1.13, v_num=.]\n",
            "Epoch 0:  17%|█▋        | 200/1173 [01:30<07:19,  2.21it/s, loss=1.12, v_num=.]\n",
            "Epoch 0:  19%|█▉        | 220/1173 [01:39<07:10,  2.22it/s, loss=1.12, v_num=.]\n",
            "Epoch 0:  20%|██        | 240/1173 [01:48<07:01,  2.21it/s, loss=1.12, v_num=.]\n",
            "Epoch 0:  22%|██▏       | 260/1173 [01:57<06:51,  2.22it/s, loss=1.12, v_num=.]\n",
            "Epoch 0:  24%|██▍       | 280/1173 [02:06<06:43,  2.22it/s, loss=1.12, v_num=.]\n",
            "Epoch 0:  26%|██▌       | 300/1173 [02:15<06:34,  2.21it/s, loss=1.11, v_num=.]\n",
            "Epoch 0:  27%|██▋       | 320/1173 [02:24<06:25,  2.21it/s, loss=1.11, v_num=.]\n",
            "Epoch 0:  29%|██▉       | 340/1173 [02:33<06:16,  2.21it/s, loss=1.1, v_num=.] \n",
            "Epoch 0:  31%|███       | 360/1173 [02:42<06:07,  2.21it/s, loss=1.11, v_num=.]\n",
            "Epoch 0:  32%|███▏      | 380/1173 [02:52<05:59,  2.21it/s, loss=1.12, v_num=.]\n",
            "Epoch 0:  34%|███▍      | 400/1173 [03:01<05:51,  2.20it/s, loss=1.1, v_num=.] \n",
            "Epoch 0:  36%|███▌      | 420/1173 [03:10<05:42,  2.20it/s, loss=1.11, v_num=.]\n",
            "Epoch 0:  38%|███▊      | 440/1173 [03:19<05:33,  2.20it/s, loss=1.11, v_num=.]\n",
            "Epoch 0:  39%|███▉      | 460/1173 [03:29<05:24,  2.20it/s, loss=1.11, v_num=.]\n",
            "Epoch 0:  41%|████      | 480/1173 [03:38<05:15,  2.20it/s, loss=1.1, v_num=.] \n",
            "Epoch 0:  43%|████▎     | 500/1173 [03:47<05:06,  2.20it/s, loss=1.11, v_num=.]\n",
            "Epoch 0:  44%|████▍     | 520/1173 [03:56<04:57,  2.19it/s, loss=1.1, v_num=.] \n",
            "Epoch 0:  46%|████▌     | 540/1173 [04:06<04:48,  2.19it/s, loss=1.1, v_num=.]\n",
            "Epoch 0:  48%|████▊     | 560/1173 [04:15<04:39,  2.19it/s, loss=1.1, v_num=.]\n",
            "Epoch 0:  49%|████▉     | 580/1173 [04:24<04:30,  2.19it/s, loss=1.1, v_num=.]\n",
            "Epoch 0:  51%|█████     | 600/1173 [04:34<04:21,  2.19it/s, loss=1.1, v_num=.]\n",
            "Epoch 0:  53%|█████▎    | 620/1173 [04:43<04:12,  2.19it/s, loss=1.1, v_num=.]\n",
            "Epoch 0:  55%|█████▍    | 640/1173 [04:52<04:03,  2.19it/s, loss=1.1, v_num=.]\n",
            "Epoch 0:  56%|█████▋    | 660/1173 [05:01<03:54,  2.19it/s, loss=1.11, v_num=.]\n",
            "Epoch 0:  58%|█████▊    | 680/1173 [05:11<03:45,  2.19it/s, loss=1.1, v_num=.] \n",
            "Epoch 0:  60%|█████▉    | 700/1173 [05:20<03:36,  2.19it/s, loss=1.1, v_num=.]\n",
            "Epoch 0:  61%|██████▏   | 720/1173 [05:29<03:27,  2.19it/s, loss=1.1, v_num=.]\n",
            "Epoch 0:  63%|██████▎   | 740/1173 [05:38<03:18,  2.19it/s, loss=1.1, v_num=.]\n",
            "Epoch 0:  65%|██████▍   | 760/1173 [05:47<03:08,  2.19it/s, loss=1.1, v_num=.]\n",
            "Epoch 0:  66%|██████▋   | 780/1173 [05:56<02:59,  2.19it/s, loss=1.11, v_num=.]\n",
            "Epoch 0:  68%|██████▊   | 800/1173 [06:06<02:50,  2.19it/s, loss=1.1, v_num=.] \n",
            "Epoch 0:  70%|██████▉   | 820/1173 [06:15<02:41,  2.18it/s, loss=1.1, v_num=.]\n",
            "Epoch 0:  72%|███████▏  | 840/1173 [06:24<02:32,  2.18it/s, loss=1.1, v_num=.]\n",
            "Epoch 0:  73%|███████▎  | 860/1173 [06:33<02:23,  2.18it/s, loss=1.1, v_num=.]\n",
            "Epoch 0:  75%|███████▌  | 880/1173 [06:42<02:14,  2.18it/s, loss=1.1, v_num=.]\n",
            "Epoch 0:  77%|███████▋  | 900/1173 [06:52<02:05,  2.18it/s, loss=1.1, v_num=.]\n",
            "Epoch 0:  78%|███████▊  | 920/1173 [07:01<01:55,  2.18it/s, loss=1.1, v_num=.]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=623)\u001b[0m /usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/callback_hook.py:101: LightningDeprecationWarning: The signature of `Callback.on_train_epoch_end` has changed in v1.3. `outputs` parameter has been removed. Support for the old signature will be removed in v1.5\n",
            "\u001b[2m\u001b[36m(pid=623)\u001b[0m   \"The signature of `Callback.on_train_epoch_end` has changed in v1.3.\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=623)\u001b[0m \rEpoch 0:  80%|████████  | 940/1173 [07:09<01:46,  2.19it/s, loss=1.1, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=623)\u001b[0m \rValidating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/235 [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[2m\u001b[36m(pid=623)\u001b[0m \n",
            "Epoch 0:  82%|████████▏ | 960/1173 [07:17<01:37,  2.19it/s, loss=1.1, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=623)\u001b[0m \n",
            "Epoch 0:  84%|████████▎ | 980/1173 [07:26<01:28,  2.19it/s, loss=1.1, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=623)\u001b[0m \n",
            "Epoch 0:  85%|████████▌ | 1000/1173 [07:35<01:18,  2.19it/s, loss=1.1, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=623)\u001b[0m \n",
            "Epoch 0:  87%|████████▋ | 1020/1173 [07:45<01:09,  2.19it/s, loss=1.1, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=623)\u001b[0m \n",
            "Epoch 0:  89%|████████▊ | 1040/1173 [07:54<01:00,  2.19it/s, loss=1.1, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=623)\u001b[0m \n",
            "Epoch 0:  90%|█████████ | 1060/1173 [08:03<00:51,  2.19it/s, loss=1.1, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=623)\u001b[0m \n",
            "Epoch 0:  92%|█████████▏| 1080/1173 [08:12<00:42,  2.19it/s, loss=1.1, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=623)\u001b[0m \n",
            "Epoch 0:  94%|█████████▍| 1100/1173 [08:21<00:33,  2.19it/s, loss=1.1, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=623)\u001b[0m \n",
            "Epoch 0:  95%|█████████▌| 1120/1173 [08:30<00:24,  2.19it/s, loss=1.1, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=623)\u001b[0m \n",
            "Epoch 0:  97%|█████████▋| 1140/1173 [08:39<00:15,  2.19it/s, loss=1.1, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=623)\u001b[0m \n",
            "Epoch 0:  99%|█████████▉| 1160/1173 [08:48<00:05,  2.19it/s, loss=1.1, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=623)\u001b[0m \n",
            "Validating: 100%|██████████| 235/235 [01:46<00:00,  2.23it/s]\u001b[A\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=623)\u001b[0m /usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/deprecated_api.py:152: LightningDeprecationWarning: `Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\n",
            "\u001b[2m\u001b[36m(pid=623)\u001b[0m   \"`Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for train_LensResnet_tune_checkpoint_83150_00000:\n",
            "  auroc: 0.5039865970611572\n",
            "  date: 2021-07-08_04-49-03\n",
            "  done: false\n",
            "  experiment_id: a2aedbaa924043a5b44630f25b707c69\n",
            "  hostname: 0b93bd12a956\n",
            "  iterations_since_restore: 1\n",
            "  loss: 1.11496901512146\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 623\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 557.7166030406952\n",
            "  time_this_iter_s: 557.7166030406952\n",
            "  time_total_s: 557.7166030406952\n",
            "  timestamp: 1625719743\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 1\n",
            "  trial_id: '83150_00000'\n",
            "  \n",
            "== Status ==\n",
            "Memory usage on this node: 4.2/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 2.000: None | Iter 1.000: 0.5039865970611572\n",
            "Resources requested: 2.0/2 CPUs, 1.0/1 GPUs, 0.0/7.48 GiB heap, 0.0/3.74 GiB objects (0.0/1.0 accelerator_type:T4, 0.0/1.0 GPU_group_7647334c6aa32c88cd055440d9a148be, 0.0/1.0 GPU_group_0_7647334c6aa32c88cd055440d9a148be, 0.0/2.0 CPU_group_0_7647334c6aa32c88cd055440d9a148be, 0.0/2.0 CPU_group_7647334c6aa32c88cd055440d9a148be)\n",
            "Current best trial: 83150_00000 with auroc=0.5039865970611572 and parameters={'learning_rate': 0.01, 'batch_size': 64}\n",
            "Result logdir: /content/drive/MyDrive/Logs/LensResNet_F\n",
            "Number of trials: 12/12 (11 PENDING, 1 RUNNING)\n",
            "+----------------------------------------------+----------+----------------+-----------------+--------------+---------+----------+----------------------+\n",
            "| Trial name                                   | status   | loc            |   learning_rate |   batch_size |    loss |    auroc |   training_iteration |\n",
            "|----------------------------------------------+----------+----------------+-----------------+--------------+---------+----------+----------------------|\n",
            "| train_LensResnet_tune_checkpoint_83150_00000 | RUNNING  | 172.28.0.2:623 |          0.01   |           64 | 1.11497 | 0.503987 |                    1 |\n",
            "| train_LensResnet_tune_checkpoint_83150_00001 | PENDING  |                |          0.001  |           64 |         |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00002 | PENDING  |                |          0.001  |          128 |         |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00003 | PENDING  |                |          0.01   |           32 |         |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00004 | PENDING  |                |          1e-05  |          128 |         |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00005 | PENDING  |                |          0.0001 |           64 |         |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00006 | PENDING  |                |          0.0001 |           32 |         |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00007 | PENDING  |                |          1e-05  |           32 |         |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00008 | PENDING  |                |          1e-05  |           32 |         |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00009 | PENDING  |                |          0.0001 |          128 |         |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00010 | PENDING  |                |          0.0001 |           32 |         |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00011 | PENDING  |                |          0.01   |           32 |         |          |                      |\n",
            "+----------------------------------------------+----------+----------------+-----------------+--------------+---------+----------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(pid=623)\u001b[0m \rEpoch 0: 100%|██████████| 1173/1173 [08:57<00:00,  2.18it/s, loss=1.1, v_num=.]\n",
            "                                                             \u001b[A\n",
            "Epoch 1:   0%|          | 0/1173 [00:00<?, ?it/s, loss=1.1, v_num=.]\n",
            "Epoch 1:   2%|▏         | 20/1173 [00:09<09:16,  2.07it/s, loss=1.1, v_num=.]\n",
            "Epoch 1:   3%|▎         | 40/1173 [00:19<09:00,  2.10it/s, loss=1.1, v_num=.]\n",
            "Epoch 1:   5%|▌         | 60/1173 [00:28<08:46,  2.11it/s, loss=1.1, v_num=.]\n",
            "Epoch 1:   7%|▋         | 80/1173 [00:37<08:38,  2.11it/s, loss=1.1, v_num=.]\n",
            "Epoch 1:   9%|▊         | 100/1173 [00:47<08:28,  2.11it/s, loss=1.1, v_num=.]\n",
            "Epoch 1:  10%|█         | 120/1173 [00:56<08:19,  2.11it/s, loss=1.1, v_num=.]\n",
            "Epoch 1:  12%|█▏        | 140/1173 [01:06<08:10,  2.11it/s, loss=1.1, v_num=.]\n",
            "Epoch 1:  14%|█▎        | 160/1173 [01:15<08:00,  2.11it/s, loss=1.1, v_num=.]\n",
            "Epoch 1:  15%|█▌        | 180/1173 [01:25<07:50,  2.11it/s, loss=1.1, v_num=.]\n",
            "Epoch 1:  17%|█▋        | 200/1173 [01:34<07:41,  2.11it/s, loss=1.1, v_num=.]\n",
            "Epoch 1:  19%|█▉        | 220/1173 [01:44<07:31,  2.11it/s, loss=1.1, v_num=.]\n",
            "Epoch 1:  20%|██        | 240/1173 [01:53<07:20,  2.12it/s, loss=1.1, v_num=.]\n",
            "Epoch 1:  22%|██▏       | 260/1173 [02:02<07:10,  2.12it/s, loss=1.1, v_num=.]\n",
            "Epoch 1:  24%|██▍       | 280/1173 [02:11<07:00,  2.13it/s, loss=1.1, v_num=.]\n",
            "Epoch 1:  26%|██▌       | 300/1173 [02:20<06:50,  2.13it/s, loss=1.1, v_num=.]\n",
            "Epoch 1:  27%|██▋       | 320/1173 [02:30<06:40,  2.13it/s, loss=1.1, v_num=.]\n",
            "Epoch 1:  29%|██▉       | 340/1173 [02:39<06:30,  2.13it/s, loss=1.1, v_num=.]\n",
            "Epoch 1:  31%|███       | 360/1173 [02:48<06:20,  2.14it/s, loss=1.1, v_num=.]\n",
            "Epoch 1:  32%|███▏      | 380/1173 [02:57<06:10,  2.14it/s, loss=1.1, v_num=.]\n",
            "Epoch 1:  34%|███▍      | 400/1173 [03:06<06:00,  2.14it/s, loss=1.1, v_num=.]\n",
            "Epoch 1:  36%|███▌      | 420/1173 [03:15<05:51,  2.14it/s, loss=1.1, v_num=.]\n",
            "Epoch 1:  38%|███▊      | 440/1173 [03:24<05:41,  2.15it/s, loss=1.1, v_num=.]\n",
            "Epoch 1:  39%|███▉      | 460/1173 [03:33<05:31,  2.15it/s, loss=1.1, v_num=.]\n",
            "Epoch 1:  41%|████      | 480/1173 [03:43<05:21,  2.15it/s, loss=1.1, v_num=.]\n",
            "Epoch 1:  43%|████▎     | 500/1173 [03:52<05:12,  2.15it/s, loss=1.1, v_num=.]\n",
            "Epoch 1:  44%|████▍     | 520/1173 [04:01<05:03,  2.15it/s, loss=1.1, v_num=.]\n",
            "Epoch 1:  46%|████▌     | 540/1173 [04:10<04:53,  2.16it/s, loss=1.1, v_num=.]\n",
            "Epoch 1:  48%|████▊     | 560/1173 [04:19<04:43,  2.16it/s, loss=1.1, v_num=.]\n",
            "Epoch 1:  49%|████▉     | 580/1173 [04:28<04:34,  2.16it/s, loss=1.09, v_num=.]\n",
            "Epoch 1:  51%|█████     | 600/1173 [04:37<04:24,  2.16it/s, loss=1.1, v_num=.] \n",
            "Epoch 1:  53%|█████▎    | 620/1173 [04:46<04:15,  2.17it/s, loss=1.1, v_num=.]\n",
            "Epoch 1:  55%|█████▍    | 640/1173 [04:55<04:05,  2.17it/s, loss=1.1, v_num=.]\n",
            "Epoch 1:  56%|█████▋    | 660/1173 [05:04<03:56,  2.17it/s, loss=1.1, v_num=.]\n",
            "Epoch 1:  58%|█████▊    | 680/1173 [05:13<03:47,  2.17it/s, loss=1.1, v_num=.]\n",
            "Epoch 1:  60%|█████▉    | 700/1173 [05:22<03:38,  2.17it/s, loss=1.1, v_num=.]\n",
            "Epoch 1:  61%|██████▏   | 720/1173 [05:31<03:28,  2.17it/s, loss=1.1, v_num=.]\n",
            "Epoch 1:  63%|██████▎   | 740/1173 [05:40<03:19,  2.17it/s, loss=1.1, v_num=.]\n",
            "Epoch 1:  65%|██████▍   | 760/1173 [05:49<03:10,  2.17it/s, loss=1.1, v_num=.]\n",
            "Epoch 1:  66%|██████▋   | 780/1173 [05:58<03:00,  2.17it/s, loss=1.1, v_num=.]\n",
            "Epoch 1:  68%|██████▊   | 800/1173 [06:08<02:51,  2.17it/s, loss=1.09, v_num=.]\n",
            "Epoch 1:  70%|██████▉   | 820/1173 [06:17<02:42,  2.17it/s, loss=1.1, v_num=.] \n",
            "Epoch 1:  72%|███████▏  | 840/1173 [06:26<02:33,  2.17it/s, loss=1.1, v_num=.]\n",
            "Epoch 1:  73%|███████▎  | 860/1173 [06:35<02:23,  2.18it/s, loss=1.09, v_num=.]\n",
            "Epoch 1:  75%|███████▌  | 880/1173 [06:44<02:14,  2.18it/s, loss=1.09, v_num=.]\n",
            "Epoch 1:  77%|███████▋  | 900/1173 [06:53<02:05,  2.18it/s, loss=1.1, v_num=.] \n",
            "Epoch 1:  78%|███████▊  | 920/1173 [07:02<01:56,  2.18it/s, loss=1.09, v_num=.]\n",
            "Epoch 1:  80%|████████  | 940/1173 [07:10<01:46,  2.18it/s, loss=1.09, v_num=.]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/235 [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[2m\u001b[36m(pid=623)\u001b[0m \n",
            "Epoch 1:  82%|████████▏ | 960/1173 [07:19<01:37,  2.19it/s, loss=1.09, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=623)\u001b[0m \n",
            "Epoch 1:  84%|████████▎ | 980/1173 [07:28<01:28,  2.19it/s, loss=1.09, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=623)\u001b[0m \n",
            "Epoch 1:  85%|████████▌ | 1000/1173 [07:37<01:19,  2.19it/s, loss=1.09, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=623)\u001b[0m \n",
            "Epoch 1:  87%|████████▋ | 1020/1173 [07:46<01:09,  2.19it/s, loss=1.09, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=623)\u001b[0m \n",
            "Epoch 1:  89%|████████▊ | 1040/1173 [07:54<01:00,  2.19it/s, loss=1.09, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=623)\u001b[0m \n",
            "Epoch 1:  90%|█████████ | 1060/1173 [08:04<00:51,  2.19it/s, loss=1.09, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=623)\u001b[0m \n",
            "Epoch 1:  92%|█████████▏| 1080/1173 [08:13<00:42,  2.19it/s, loss=1.09, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=623)\u001b[0m \n",
            "Epoch 1:  94%|█████████▍| 1100/1173 [08:22<00:33,  2.19it/s, loss=1.09, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=623)\u001b[0m \n",
            "Epoch 1:  95%|█████████▌| 1120/1173 [08:31<00:24,  2.19it/s, loss=1.09, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=623)\u001b[0m \n",
            "Epoch 1:  97%|█████████▋| 1140/1173 [08:40<00:15,  2.19it/s, loss=1.09, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=623)\u001b[0m \n",
            "Epoch 1:  99%|█████████▉| 1160/1173 [08:49<00:05,  2.19it/s, loss=1.09, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=623)\u001b[0m \n",
            "Validating: 100%|██████████| 235/235 [01:45<00:00,  2.23it/s]\u001b[A\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=623)\u001b[0m /usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/deprecated_api.py:152: LightningDeprecationWarning: `Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\n",
            "\u001b[2m\u001b[36m(pid=623)\u001b[0m   \"`Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for train_LensResnet_tune_checkpoint_83150_00000:\n",
            "  auroc: 0.547535240650177\n",
            "  date: 2021-07-08_04-58-02\n",
            "  done: false\n",
            "  experiment_id: a2aedbaa924043a5b44630f25b707c69\n",
            "  hostname: 0b93bd12a956\n",
            "  iterations_since_restore: 2\n",
            "  loss: 1.2411631345748901\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 623\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 1096.8282725811005\n",
            "  time_this_iter_s: 539.1116695404053\n",
            "  time_total_s: 1096.8282725811005\n",
            "  timestamp: 1625720282\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 2\n",
            "  trial_id: '83150_00000'\n",
            "  \n",
            "== Status ==\n",
            "Memory usage on this node: 4.3/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 2.000: 0.547535240650177 | Iter 1.000: 0.5039865970611572\n",
            "Resources requested: 2.0/2 CPUs, 1.0/1 GPUs, 0.0/7.48 GiB heap, 0.0/3.74 GiB objects (0.0/2.0 CPU_group_0_7647334c6aa32c88cd055440d9a148be, 0.0/1.0 GPU_group_0_7647334c6aa32c88cd055440d9a148be, 0.0/1.0 GPU_group_7647334c6aa32c88cd055440d9a148be, 0.0/1.0 accelerator_type:T4, 0.0/2.0 CPU_group_7647334c6aa32c88cd055440d9a148be)\n",
            "Current best trial: 83150_00000 with auroc=0.547535240650177 and parameters={'learning_rate': 0.01, 'batch_size': 64}\n",
            "Result logdir: /content/drive/MyDrive/Logs/LensResNet_F\n",
            "Number of trials: 12/12 (11 PENDING, 1 RUNNING)\n",
            "+----------------------------------------------+----------+----------------+-----------------+--------------+---------+----------+----------------------+\n",
            "| Trial name                                   | status   | loc            |   learning_rate |   batch_size |    loss |    auroc |   training_iteration |\n",
            "|----------------------------------------------+----------+----------------+-----------------+--------------+---------+----------+----------------------|\n",
            "| train_LensResnet_tune_checkpoint_83150_00000 | RUNNING  | 172.28.0.2:623 |          0.01   |           64 | 1.24116 | 0.547535 |                    2 |\n",
            "| train_LensResnet_tune_checkpoint_83150_00001 | PENDING  |                |          0.001  |           64 |         |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00002 | PENDING  |                |          0.001  |          128 |         |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00003 | PENDING  |                |          0.01   |           32 |         |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00004 | PENDING  |                |          1e-05  |          128 |         |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00005 | PENDING  |                |          0.0001 |           64 |         |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00006 | PENDING  |                |          0.0001 |           32 |         |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00007 | PENDING  |                |          1e-05  |           32 |         |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00008 | PENDING  |                |          1e-05  |           32 |         |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00009 | PENDING  |                |          0.0001 |          128 |         |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00010 | PENDING  |                |          0.0001 |           32 |         |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00011 | PENDING  |                |          0.01   |           32 |         |          |                      |\n",
            "+----------------------------------------------+----------+----------------+-----------------+--------------+---------+----------+----------------------+\n",
            "\n",
            "\n",
            "Epoch 1: 100%|██████████| 1173/1173 [08:58<00:00,  2.18it/s, loss=1.06, v_num=.]\n",
            "                                                             \u001b[A\n",
            "Epoch 2:   0%|          | 0/1173 [00:00<?, ?it/s, loss=1.06, v_num=.]\n",
            "Epoch 2:   2%|▏         | 20/1173 [00:09<09:14,  2.08it/s, loss=1.07, v_num=.]\n",
            "Epoch 2:   3%|▎         | 40/1173 [00:18<08:52,  2.13it/s, loss=1.07, v_num=.]\n",
            "Epoch 2:   5%|▌         | 60/1173 [00:28<08:39,  2.14it/s, loss=1.04, v_num=.]\n",
            "Epoch 2:   7%|▋         | 80/1173 [00:37<08:28,  2.15it/s, loss=1.03, v_num=.]\n",
            "Epoch 2:   9%|▊         | 100/1173 [00:46<08:19,  2.15it/s, loss=1, v_num=.]   \n",
            "Epoch 2:  10%|█         | 120/1173 [00:56<08:12,  2.14it/s, loss=1.01, v_num=.]\n",
            "Epoch 2:  12%|█▏        | 140/1173 [01:05<08:03,  2.14it/s, loss=1.02, v_num=.]\n",
            "Epoch 2:  14%|█▎        | 160/1173 [01:14<07:53,  2.14it/s, loss=1.03, v_num=.]\n",
            "Epoch 2:  15%|█▌        | 180/1173 [01:24<07:44,  2.14it/s, loss=0.974, v_num=.]\n",
            "Epoch 2:  17%|█▋        | 200/1173 [01:33<07:35,  2.14it/s, loss=0.968, v_num=.]\n",
            "Epoch 2:  19%|█▉        | 220/1173 [01:43<07:26,  2.13it/s, loss=0.968, v_num=.]\n",
            "Epoch 2:  19%|█▉        | 220/1173 [01:43<07:26,  2.13it/s, loss=0.962, v_num=.]\n",
            "Epoch 2:  20%|██        | 240/1173 [01:52<07:17,  2.13it/s, loss=0.997, v_num=.]\n",
            "Epoch 2:  22%|██▏       | 260/1173 [02:02<07:08,  2.13it/s, loss=0.985, v_num=.]\n",
            "Epoch 2:  24%|██▍       | 280/1173 [02:11<06:58,  2.13it/s, loss=0.948, v_num=.]\n",
            "Epoch 2:  26%|██▌       | 300/1173 [02:20<06:49,  2.13it/s, loss=0.968, v_num=.]\n",
            "Epoch 2:  27%|██▋       | 320/1173 [02:29<06:39,  2.14it/s, loss=0.967, v_num=.]\n",
            "Epoch 2:  29%|██▉       | 340/1173 [02:38<06:29,  2.14it/s, loss=0.966, v_num=.]\n",
            "Epoch 2:  31%|███       | 360/1173 [02:47<06:18,  2.15it/s, loss=0.945, v_num=.]\n",
            "Epoch 2:  32%|███▏      | 380/1173 [02:56<06:09,  2.15it/s, loss=0.985, v_num=.]\n",
            "Epoch 2:  34%|███▍      | 400/1173 [03:06<05:59,  2.15it/s, loss=0.984, v_num=.]\n",
            "Epoch 2:  36%|███▌      | 420/1173 [03:15<05:50,  2.15it/s, loss=0.949, v_num=.]\n",
            "Epoch 2:  38%|███▊      | 440/1173 [03:24<05:40,  2.15it/s, loss=0.916, v_num=.]\n",
            "Epoch 2:  39%|███▉      | 460/1173 [03:33<05:30,  2.16it/s, loss=0.937, v_num=.]\n",
            "Epoch 2:  41%|████      | 480/1173 [03:42<05:20,  2.16it/s, loss=0.914, v_num=.]\n",
            "Epoch 2:  43%|████▎     | 500/1173 [03:51<05:11,  2.16it/s, loss=0.914, v_num=.]\n",
            "Epoch 2:  44%|████▍     | 520/1173 [04:00<05:01,  2.16it/s, loss=0.921, v_num=.]\n",
            "Epoch 2:  46%|████▌     | 540/1173 [04:09<04:52,  2.17it/s, loss=0.93, v_num=.] \n",
            "Epoch 2:  48%|████▊     | 560/1173 [04:18<04:42,  2.17it/s, loss=0.956, v_num=.]\n",
            "Epoch 2:  49%|████▉     | 580/1173 [04:27<04:33,  2.17it/s, loss=0.942, v_num=.]\n",
            "Epoch 2:  51%|█████     | 600/1173 [04:36<04:23,  2.17it/s, loss=0.897, v_num=.]\n",
            "Epoch 2:  53%|█████▎    | 620/1173 [04:45<04:14,  2.17it/s, loss=0.891, v_num=.]\n",
            "Epoch 2:  55%|█████▍    | 640/1173 [04:54<04:05,  2.17it/s, loss=0.965, v_num=.]\n",
            "Epoch 2:  56%|█████▋    | 660/1173 [05:03<03:55,  2.18it/s, loss=0.907, v_num=.]\n",
            "Epoch 2:  58%|█████▊    | 680/1173 [05:12<03:46,  2.18it/s, loss=0.907, v_num=.]\n",
            "Epoch 2:  58%|█████▊    | 680/1173 [05:12<03:46,  2.18it/s, loss=0.929, v_num=.]\n",
            "Epoch 2:  60%|█████▉    | 700/1173 [05:21<03:37,  2.18it/s, loss=0.897, v_num=.]\n",
            "Epoch 2:  61%|██████▏   | 720/1173 [05:30<03:27,  2.18it/s, loss=0.902, v_num=.]\n",
            "Epoch 2:  63%|██████▎   | 740/1173 [05:39<03:18,  2.18it/s, loss=0.879, v_num=.]\n",
            "Epoch 2:  65%|██████▍   | 760/1173 [05:48<03:09,  2.18it/s, loss=0.872, v_num=.]\n",
            "Epoch 2:  66%|██████▋   | 780/1173 [05:57<03:00,  2.18it/s, loss=0.882, v_num=.]\n",
            "Epoch 2:  68%|██████▊   | 800/1173 [06:06<02:50,  2.18it/s, loss=0.91, v_num=.] \n",
            "Epoch 2:  70%|██████▉   | 820/1173 [06:15<02:41,  2.19it/s, loss=0.901, v_num=.]\n",
            "Epoch 2:  72%|███████▏  | 840/1173 [06:24<02:32,  2.19it/s, loss=0.937, v_num=.]\n",
            "Epoch 2:  73%|███████▎  | 860/1173 [06:33<02:23,  2.19it/s, loss=0.89, v_num=.] \n",
            "Epoch 2:  75%|███████▌  | 880/1173 [06:42<02:13,  2.19it/s, loss=0.882, v_num=.]\n",
            "Epoch 2:  77%|███████▋  | 900/1173 [06:51<02:04,  2.19it/s, loss=0.892, v_num=.]\n",
            "Epoch 2:  78%|███████▊  | 920/1173 [07:00<01:55,  2.19it/s, loss=0.88, v_num=.] \n",
            "Epoch 2:  80%|████████  | 940/1173 [07:07<01:46,  2.20it/s, loss=0.88, v_num=.]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/235 [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[2m\u001b[36m(pid=623)\u001b[0m \n",
            "Epoch 2:  82%|████████▏ | 960/1173 [07:16<01:36,  2.20it/s, loss=0.88, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=623)\u001b[0m \n",
            "Epoch 2:  84%|████████▎ | 980/1173 [07:25<01:27,  2.20it/s, loss=0.88, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=623)\u001b[0m \n",
            "Epoch 2:  85%|████████▌ | 1000/1173 [07:34<01:18,  2.20it/s, loss=0.88, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=623)\u001b[0m \n",
            "Epoch 2:  87%|████████▋ | 1020/1173 [07:43<01:09,  2.20it/s, loss=0.88, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=623)\u001b[0m \n",
            "Epoch 2:  89%|████████▊ | 1040/1173 [07:52<01:00,  2.20it/s, loss=0.88, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=623)\u001b[0m \n",
            "Epoch 2:  90%|█████████ | 1060/1173 [08:01<00:51,  2.20it/s, loss=0.88, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=623)\u001b[0m \n",
            "Epoch 2:  92%|█████████▏| 1080/1173 [08:10<00:42,  2.20it/s, loss=0.88, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=623)\u001b[0m \n",
            "Epoch 2:  94%|█████████▍| 1100/1173 [08:18<00:33,  2.20it/s, loss=0.88, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=623)\u001b[0m \n",
            "Epoch 2:  95%|█████████▌| 1120/1173 [08:28<00:24,  2.20it/s, loss=0.88, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=623)\u001b[0m \n",
            "Epoch 2:  97%|█████████▋| 1140/1173 [08:37<00:14,  2.20it/s, loss=0.88, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=623)\u001b[0m \n",
            "Epoch 2:  99%|█████████▉| 1160/1173 [08:45<00:05,  2.21it/s, loss=0.88, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=623)\u001b[0m \n",
            "Validating: 100%|██████████| 235/235 [01:44<00:00,  2.26it/s]\u001b[A\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=623)\u001b[0m /usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/deprecated_api.py:152: LightningDeprecationWarning: `Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\n",
            "\u001b[2m\u001b[36m(pid=623)\u001b[0m   \"`Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for train_LensResnet_tune_checkpoint_83150_00000:\n",
            "  auroc: 0.6731033325195312\n",
            "  date: 2021-07-08_05-06-57\n",
            "  done: true\n",
            "  experiment_id: a2aedbaa924043a5b44630f25b707c69\n",
            "  hostname: 0b93bd12a956\n",
            "  iterations_since_restore: 3\n",
            "  loss: 9.628469467163086\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 623\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 1632.327488899231\n",
            "  time_this_iter_s: 535.4992163181305\n",
            "  time_total_s: 1632.327488899231\n",
            "  timestamp: 1625720817\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 3\n",
            "  trial_id: '83150_00000'\n",
            "  \n",
            "== Status ==\n",
            "Memory usage on this node: 4.3/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=1\n",
            "Bracket: Iter 2.000: 0.547535240650177 | Iter 1.000: 0.5039865970611572\n",
            "Resources requested: 2.0/2 CPUs, 1.0/1 GPUs, 0.0/7.48 GiB heap, 0.0/3.74 GiB objects (0.0/2.0 CPU_group_7647334c6aa32c88cd055440d9a148be, 0.0/1.0 accelerator_type:T4, 0.0/2.0 CPU_group_0_7647334c6aa32c88cd055440d9a148be, 0.0/1.0 GPU_group_0_7647334c6aa32c88cd055440d9a148be, 0.0/1.0 GPU_group_7647334c6aa32c88cd055440d9a148be)\n",
            "Current best trial: 83150_00000 with auroc=0.6731033325195312 and parameters={'learning_rate': 0.01, 'batch_size': 64}\n",
            "Result logdir: /content/drive/MyDrive/Logs/LensResNet_F\n",
            "Number of trials: 12/12 (11 PENDING, 1 RUNNING)\n",
            "+----------------------------------------------+----------+----------------+-----------------+--------------+---------+----------+----------------------+\n",
            "| Trial name                                   | status   | loc            |   learning_rate |   batch_size |    loss |    auroc |   training_iteration |\n",
            "|----------------------------------------------+----------+----------------+-----------------+--------------+---------+----------+----------------------|\n",
            "| train_LensResnet_tune_checkpoint_83150_00000 | RUNNING  | 172.28.0.2:623 |          0.01   |           64 | 9.62847 | 0.673103 |                    3 |\n",
            "| train_LensResnet_tune_checkpoint_83150_00001 | PENDING  |                |          0.001  |           64 |         |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00002 | PENDING  |                |          0.001  |          128 |         |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00003 | PENDING  |                |          0.01   |           32 |         |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00004 | PENDING  |                |          1e-05  |          128 |         |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00005 | PENDING  |                |          0.0001 |           64 |         |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00006 | PENDING  |                |          0.0001 |           32 |         |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00007 | PENDING  |                |          1e-05  |           32 |         |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00008 | PENDING  |                |          1e-05  |           32 |         |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00009 | PENDING  |                |          0.0001 |          128 |         |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00010 | PENDING  |                |          0.0001 |           32 |         |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00011 | PENDING  |                |          0.01   |           32 |         |          |                      |\n",
            "+----------------------------------------------+----------+----------------+-----------------+--------------+---------+----------+----------------------+\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2021-07-08 05:06:58,595\tWARNING util.py:162 -- The `start_trial` operation took 0.638 s, which may be a performance bottleneck.\n",
            "\u001b[2m\u001b[36m(pid=624)\u001b[0m GPU available: True, used: True\n",
            "\u001b[2m\u001b[36m(pid=624)\u001b[0m TPU available: False, using: 0 TPU cores\n",
            "\u001b[2m\u001b[36m(pid=624)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\u001b[2m\u001b[36m(pid=624)\u001b[0m 2021-07-08 05:07:11.346080: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "\u001b[2m\u001b[36m(pid=624)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=624)\u001b[0m   | Name     | Type   | Params\n",
            "\u001b[2m\u001b[36m(pid=624)\u001b[0m ------------------------------------\n",
            "\u001b[2m\u001b[36m(pid=624)\u001b[0m 0 | backbone | ResNet | 11.2 M\n",
            "\u001b[2m\u001b[36m(pid=624)\u001b[0m ------------------------------------\n",
            "\u001b[2m\u001b[36m(pid=624)\u001b[0m 11.2 M    Trainable params\n",
            "\u001b[2m\u001b[36m(pid=624)\u001b[0m 0         Non-trainable params\n",
            "\u001b[2m\u001b[36m(pid=624)\u001b[0m 11.2 M    Total params\n",
            "\u001b[2m\u001b[36m(pid=624)\u001b[0m 44.687    Total estimated model params size (MB)\n",
            "\u001b[2m\u001b[36m(pid=624)\u001b[0m /usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/data_loading.py:349: UserWarning: Your val_dataloader has `shuffle=True`, it is best practice to turn this off for val/test/predict dataloaders.\n",
            "\u001b[2m\u001b[36m(pid=624)\u001b[0m   f'Your {mode}_dataloader has `shuffle=True`, it is best practice to turn'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=624)\u001b[0m \rTraining: 0it [00:00, ?it/s]\rTraining:   0%|          | 0/1173 [00:00<?, ?it/s]\rEpoch 0:   0%|          | 0/1173 [00:00<?, ?it/s] \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=624)\u001b[0m /usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "\u001b[2m\u001b[36m(pid=624)\u001b[0m   return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0:   2%|▏         | 20/1173 [00:08<07:56,  2.42it/s, loss=1.25, v_num=.]\n",
            "Epoch 0:   3%|▎         | 40/1173 [00:16<07:41,  2.46it/s, loss=1.17, v_num=.]\n",
            "Epoch 0:   5%|▌         | 60/1173 [00:24<07:30,  2.47it/s, loss=1.16, v_num=.]\n",
            "Epoch 0:   7%|▋         | 80/1173 [00:32<07:21,  2.48it/s, loss=1.17, v_num=.]\n",
            "Epoch 0:   9%|▊         | 100/1173 [00:40<07:12,  2.48it/s, loss=1.15, v_num=.]\n",
            "Epoch 0:  10%|█         | 120/1173 [00:48<07:04,  2.48it/s, loss=1.15, v_num=.]\n",
            "Epoch 0:  12%|█▏        | 140/1173 [00:56<06:58,  2.47it/s, loss=1.15, v_num=.]\n",
            "Epoch 0:  14%|█▎        | 160/1173 [01:05<06:52,  2.45it/s, loss=1.13, v_num=.]\n",
            "Epoch 0:  15%|█▌        | 180/1173 [01:14<06:48,  2.43it/s, loss=1.12, v_num=.]\n",
            "Epoch 0:  17%|█▋        | 200/1173 [01:23<06:43,  2.41it/s, loss=1.13, v_num=.]\n",
            "Epoch 0:  19%|█▉        | 220/1173 [01:32<06:38,  2.39it/s, loss=1.11, v_num=.]\n",
            "Epoch 0:  20%|██        | 240/1173 [01:41<06:32,  2.38it/s, loss=1.11, v_num=.]\n",
            "Epoch 0:  22%|██▏       | 260/1173 [01:50<06:27,  2.36it/s, loss=1.11, v_num=.]\n",
            "Epoch 0:  24%|██▍       | 280/1173 [01:59<06:22,  2.34it/s, loss=1.11, v_num=.]\n",
            "Epoch 0:  26%|██▌       | 300/1173 [02:09<06:15,  2.32it/s, loss=1.12, v_num=.]\n",
            "Epoch 0:  27%|██▋       | 320/1173 [02:18<06:09,  2.31it/s, loss=1.12, v_num=.]\n",
            "Epoch 0:  29%|██▉       | 340/1173 [02:28<06:02,  2.30it/s, loss=1.11, v_num=.]\n",
            "Epoch 0:  31%|███       | 360/1173 [02:37<05:55,  2.29it/s, loss=1.12, v_num=.]\n",
            "Epoch 0:  32%|███▏      | 380/1173 [02:46<05:47,  2.28it/s, loss=1.11, v_num=.]\n",
            "Epoch 0:  34%|███▍      | 400/1173 [02:55<05:38,  2.28it/s, loss=1.11, v_num=.]\n",
            "Epoch 0:  36%|███▌      | 420/1173 [03:04<05:30,  2.28it/s, loss=1.11, v_num=.]\n",
            "Epoch 0:  38%|███▊      | 440/1173 [03:13<05:22,  2.28it/s, loss=1.12, v_num=.]\n",
            "Epoch 0:  39%|███▉      | 460/1173 [03:22<05:13,  2.27it/s, loss=1.11, v_num=.]\n",
            "Epoch 0:  41%|████      | 480/1173 [03:31<05:05,  2.27it/s, loss=1.11, v_num=.]\n",
            "Epoch 0:  43%|████▎     | 500/1173 [03:40<04:56,  2.27it/s, loss=1.11, v_num=.]\n",
            "Epoch 0:  44%|████▍     | 520/1173 [03:49<04:48,  2.27it/s, loss=1.1, v_num=.] \n",
            "Epoch 0:  46%|████▌     | 540/1173 [03:58<04:39,  2.27it/s, loss=1.11, v_num=.]\n",
            "Epoch 0:  48%|████▊     | 560/1173 [04:07<04:30,  2.26it/s, loss=1.1, v_num=.] \n",
            "Epoch 0:  49%|████▉     | 580/1173 [04:16<04:22,  2.26it/s, loss=1.11, v_num=.]\n",
            "Epoch 0:  51%|█████     | 600/1173 [04:25<04:13,  2.26it/s, loss=1.11, v_num=.]\n",
            "Epoch 0:  53%|█████▎    | 620/1173 [04:34<04:04,  2.26it/s, loss=1.11, v_num=.]\n",
            "Epoch 0:  55%|█████▍    | 640/1173 [04:43<03:56,  2.26it/s, loss=1.11, v_num=.]\n",
            "Epoch 0:  56%|█████▋    | 660/1173 [04:52<03:47,  2.25it/s, loss=1.11, v_num=.]\n",
            "Epoch 0:  58%|█████▊    | 680/1173 [05:01<03:38,  2.25it/s, loss=1.1, v_num=.] \n",
            "Epoch 0:  60%|█████▉    | 700/1173 [05:10<03:29,  2.25it/s, loss=1.1, v_num=.]\n",
            "Epoch 0:  61%|██████▏   | 720/1173 [05:19<03:21,  2.25it/s, loss=1.1, v_num=.]\n",
            "Epoch 0:  63%|██████▎   | 740/1173 [05:28<03:12,  2.25it/s, loss=1.11, v_num=.]\n",
            "Epoch 0:  65%|██████▍   | 760/1173 [05:37<03:03,  2.25it/s, loss=1.11, v_num=.]\n",
            "Epoch 0:  66%|██████▋   | 780/1173 [05:46<02:54,  2.25it/s, loss=1.09, v_num=.]\n",
            "Epoch 0:  68%|██████▊   | 800/1173 [05:55<02:45,  2.25it/s, loss=1.11, v_num=.]\n",
            "Epoch 0:  70%|██████▉   | 820/1173 [06:04<02:37,  2.25it/s, loss=1.1, v_num=.] \n",
            "Epoch 0:  72%|███████▏  | 840/1173 [06:14<02:28,  2.25it/s, loss=1.11, v_num=.]\n",
            "Epoch 0:  73%|███████▎  | 860/1173 [06:23<02:19,  2.25it/s, loss=1.1, v_num=.] \n",
            "Epoch 0:  75%|███████▌  | 880/1173 [06:32<02:10,  2.24it/s, loss=1.09, v_num=.]\n",
            "Epoch 0:  77%|███████▋  | 900/1173 [06:41<02:01,  2.24it/s, loss=1.09, v_num=.]\n",
            "Epoch 0:  78%|███████▊  | 920/1173 [06:50<01:52,  2.24it/s, loss=1.09, v_num=.]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=624)\u001b[0m /usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/callback_hook.py:101: LightningDeprecationWarning: The signature of `Callback.on_train_epoch_end` has changed in v1.3. `outputs` parameter has been removed. Support for the old signature will be removed in v1.5\n",
            "\u001b[2m\u001b[36m(pid=624)\u001b[0m   \"The signature of `Callback.on_train_epoch_end` has changed in v1.3.\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=624)\u001b[0m \rEpoch 0:  80%|████████  | 940/1173 [06:57<01:43,  2.25it/s, loss=1.09, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=624)\u001b[0m \rValidating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/235 [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[2m\u001b[36m(pid=624)\u001b[0m \n",
            "Epoch 0:  82%|████████▏ | 960/1173 [07:05<01:34,  2.26it/s, loss=1.09, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=624)\u001b[0m \n",
            "Epoch 0:  84%|████████▎ | 980/1173 [07:14<01:25,  2.26it/s, loss=1.09, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=624)\u001b[0m \n",
            "Epoch 0:  85%|████████▌ | 1000/1173 [07:22<01:16,  2.26it/s, loss=1.09, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=624)\u001b[0m \n",
            "Epoch 0:  87%|████████▋ | 1020/1173 [07:31<01:07,  2.26it/s, loss=1.09, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=624)\u001b[0m \n",
            "Epoch 0:  89%|████████▊ | 1040/1173 [07:40<00:58,  2.26it/s, loss=1.09, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=624)\u001b[0m \n",
            "Epoch 0:  90%|█████████ | 1060/1173 [07:49<00:50,  2.26it/s, loss=1.09, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=624)\u001b[0m \n",
            "Epoch 0:  92%|█████████▏| 1080/1173 [07:58<00:41,  2.26it/s, loss=1.09, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=624)\u001b[0m \n",
            "Epoch 0:  94%|█████████▍| 1100/1173 [08:07<00:32,  2.26it/s, loss=1.09, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=624)\u001b[0m \n",
            "Epoch 0:  95%|█████████▌| 1120/1173 [08:16<00:23,  2.26it/s, loss=1.09, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=624)\u001b[0m \n",
            "Epoch 0:  97%|█████████▋| 1140/1173 [08:25<00:14,  2.26it/s, loss=1.09, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=624)\u001b[0m \n",
            "Epoch 0:  99%|█████████▉| 1160/1173 [08:34<00:05,  2.26it/s, loss=1.09, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=624)\u001b[0m \n",
            "Validating: 100%|██████████| 235/235 [01:42<00:00,  2.28it/s]\u001b[A\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=624)\u001b[0m /usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/deprecated_api.py:152: LightningDeprecationWarning: `Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\n",
            "\u001b[2m\u001b[36m(pid=624)\u001b[0m   \"`Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for train_LensResnet_tune_checkpoint_83150_00001:\n",
            "  auroc: 0.5439504981040955\n",
            "  date: 2021-07-08_05-15-56\n",
            "  done: false\n",
            "  experiment_id: df7e1d89950f49d7aa8323e28e9e0044\n",
            "  hostname: 0b93bd12a956\n",
            "  iterations_since_restore: 1\n",
            "  loss: 1.1251014471054077\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 624\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 533.1734511852264\n",
            "  time_this_iter_s: 533.1734511852264\n",
            "  time_total_s: 533.1734511852264\n",
            "  timestamp: 1625721356\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 1\n",
            "  trial_id: '83150_00001'\n",
            "  \n",
            "== Status ==\n",
            "Memory usage on this node: 4.2/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=1\n",
            "Bracket: Iter 2.000: 0.547535240650177 | Iter 1.000: 0.5239685475826263\n",
            "Resources requested: 2.0/2 CPUs, 1.0/1 GPUs, 0.0/7.48 GiB heap, 0.0/3.74 GiB objects (0.0/2.0 CPU_group_7647334c6aa32c88cd055440d9a148be, 0.0/1.0 accelerator_type:T4, 0.0/1.0 GPU_group_0_7647334c6aa32c88cd055440d9a148be, 0.0/1.0 GPU_group_7647334c6aa32c88cd055440d9a148be, 0.0/2.0 CPU_group_0_7647334c6aa32c88cd055440d9a148be)\n",
            "Current best trial: 83150_00000 with auroc=0.6731033325195312 and parameters={'learning_rate': 0.01, 'batch_size': 64}\n",
            "Result logdir: /content/drive/MyDrive/Logs/LensResNet_F\n",
            "Number of trials: 12/12 (10 PENDING, 1 RUNNING, 1 TERMINATED)\n",
            "+----------------------------------------------+------------+----------------+-----------------+--------------+---------+----------+----------------------+\n",
            "| Trial name                                   | status     | loc            |   learning_rate |   batch_size |    loss |    auroc |   training_iteration |\n",
            "|----------------------------------------------+------------+----------------+-----------------+--------------+---------+----------+----------------------|\n",
            "| train_LensResnet_tune_checkpoint_83150_00001 | RUNNING    | 172.28.0.2:624 |          0.001  |           64 | 1.1251  | 0.54395  |                    1 |\n",
            "| train_LensResnet_tune_checkpoint_83150_00002 | PENDING    |                |          0.001  |          128 |         |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00003 | PENDING    |                |          0.01   |           32 |         |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00004 | PENDING    |                |          1e-05  |          128 |         |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00005 | PENDING    |                |          0.0001 |           64 |         |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00006 | PENDING    |                |          0.0001 |           32 |         |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00007 | PENDING    |                |          1e-05  |           32 |         |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00008 | PENDING    |                |          1e-05  |           32 |         |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00009 | PENDING    |                |          0.0001 |          128 |         |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00010 | PENDING    |                |          0.0001 |           32 |         |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00011 | PENDING    |                |          0.01   |           32 |         |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00000 | TERMINATED |                |          0.01   |           64 | 9.62847 | 0.673103 |                    3 |\n",
            "+----------------------------------------------+------------+----------------+-----------------+--------------+---------+----------+----------------------+\n",
            "\n",
            "\n",
            "Epoch 0: 100%|██████████| 1173/1173 [08:43<00:00,  2.24it/s, loss=1.1, v_num=.] \n",
            "                                                             \u001b[A\n",
            "Epoch 1:   0%|          | 0/1173 [00:00<?, ?it/s, loss=1.1, v_num=.]\n",
            "Epoch 1:   2%|▏         | 20/1173 [00:09<09:16,  2.07it/s, loss=1.09, v_num=.]\n",
            "Epoch 1:   3%|▎         | 40/1173 [00:18<08:57,  2.11it/s, loss=1.1, v_num=.] \n",
            "Epoch 1:   5%|▌         | 60/1173 [00:28<08:44,  2.12it/s, loss=1.08, v_num=.]\n",
            "Epoch 1:   7%|▋         | 80/1173 [00:37<08:34,  2.13it/s, loss=1.1, v_num=.] \n",
            "Epoch 1:   9%|▊         | 100/1173 [00:46<08:23,  2.13it/s, loss=1.07, v_num=.]\n",
            "Epoch 1:  10%|█         | 120/1173 [00:56<08:12,  2.14it/s, loss=1.06, v_num=.]\n",
            "Epoch 1:  12%|█▏        | 140/1173 [01:05<08:03,  2.14it/s, loss=1.07, v_num=.]\n",
            "Epoch 1:  14%|█▎        | 160/1173 [01:14<07:52,  2.14it/s, loss=1.08, v_num=.]\n",
            "Epoch 1:  15%|█▌        | 180/1173 [01:23<07:42,  2.15it/s, loss=1.08, v_num=.]\n",
            "Epoch 1:  17%|█▋        | 200/1173 [01:33<07:33,  2.15it/s, loss=1.07, v_num=.]\n",
            "Epoch 1:  19%|█▉        | 220/1173 [01:42<07:23,  2.15it/s, loss=1.06, v_num=.]\n",
            "Epoch 1:  20%|██        | 240/1173 [01:51<07:13,  2.15it/s, loss=1.04, v_num=.]\n",
            "Epoch 1:  22%|██▏       | 260/1173 [02:00<07:04,  2.15it/s, loss=1.04, v_num=.]\n",
            "Epoch 1:  24%|██▍       | 280/1173 [02:10<06:54,  2.15it/s, loss=1.01, v_num=.]\n",
            "Epoch 1:  26%|██▌       | 300/1173 [02:19<06:45,  2.15it/s, loss=0.971, v_num=.]\n",
            "Epoch 1:  27%|██▋       | 320/1173 [02:28<06:35,  2.15it/s, loss=0.969, v_num=.]\n",
            "Epoch 1:  29%|██▉       | 340/1173 [02:37<06:26,  2.16it/s, loss=0.983, v_num=.]\n",
            "Epoch 1:  31%|███       | 360/1173 [02:46<06:17,  2.16it/s, loss=0.95, v_num=.] \n",
            "Epoch 1:  32%|███▏      | 380/1173 [02:56<06:07,  2.16it/s, loss=0.963, v_num=.]\n",
            "Epoch 1:  34%|███▍      | 400/1173 [03:05<05:58,  2.16it/s, loss=0.951, v_num=.]\n",
            "Epoch 1:  36%|███▌      | 420/1173 [03:14<05:48,  2.16it/s, loss=0.922, v_num=.]\n",
            "Epoch 1:  38%|███▊      | 440/1173 [03:23<05:39,  2.16it/s, loss=0.993, v_num=.]\n",
            "Epoch 1:  39%|███▉      | 460/1173 [03:33<05:30,  2.16it/s, loss=0.901, v_num=.]\n",
            "Epoch 1:  41%|████      | 480/1173 [03:42<05:21,  2.16it/s, loss=0.926, v_num=.]\n",
            "Epoch 1:  43%|████▎     | 500/1173 [03:51<05:11,  2.16it/s, loss=0.895, v_num=.]\n",
            "Epoch 1:  44%|████▍     | 520/1173 [04:00<05:02,  2.16it/s, loss=0.946, v_num=.]\n",
            "Epoch 1:  46%|████▌     | 540/1173 [04:09<04:52,  2.16it/s, loss=0.908, v_num=.]\n",
            "Epoch 1:  48%|████▊     | 560/1173 [04:18<04:43,  2.16it/s, loss=0.921, v_num=.]\n",
            "Epoch 1:  49%|████▉     | 580/1173 [04:28<04:34,  2.16it/s, loss=0.892, v_num=.]\n",
            "Epoch 1:  51%|█████     | 600/1173 [04:37<04:24,  2.16it/s, loss=0.886, v_num=.]\n",
            "Epoch 1:  53%|█████▎    | 620/1173 [04:46<04:15,  2.16it/s, loss=0.886, v_num=.]\n",
            "Epoch 1:  53%|█████▎    | 620/1173 [04:46<04:15,  2.16it/s, loss=0.876, v_num=.]\n",
            "Epoch 1:  55%|█████▍    | 640/1173 [04:55<04:06,  2.16it/s, loss=0.941, v_num=.]\n",
            "Epoch 1:  56%|█████▋    | 660/1173 [05:04<03:56,  2.16it/s, loss=0.889, v_num=.]\n",
            "Epoch 1:  58%|█████▊    | 680/1173 [05:14<03:47,  2.17it/s, loss=0.884, v_num=.]\n",
            "Epoch 1:  60%|█████▉    | 700/1173 [05:23<03:38,  2.17it/s, loss=0.885, v_num=.]\n",
            "Epoch 1:  61%|██████▏   | 720/1173 [05:32<03:29,  2.17it/s, loss=0.896, v_num=.]\n",
            "Epoch 1:  63%|██████▎   | 740/1173 [05:41<03:19,  2.17it/s, loss=0.878, v_num=.]\n",
            "Epoch 1:  65%|██████▍   | 760/1173 [05:50<03:10,  2.17it/s, loss=0.893, v_num=.]\n",
            "Epoch 1:  66%|██████▋   | 780/1173 [05:59<03:01,  2.17it/s, loss=0.889, v_num=.]\n",
            "Epoch 1:  68%|██████▊   | 800/1173 [06:09<02:52,  2.17it/s, loss=0.847, v_num=.]\n",
            "Epoch 1:  70%|██████▉   | 820/1173 [06:18<02:42,  2.17it/s, loss=0.861, v_num=.]\n",
            "Epoch 1:  72%|███████▏  | 840/1173 [06:27<02:33,  2.17it/s, loss=0.865, v_num=.]\n",
            "Epoch 1:  73%|███████▎  | 860/1173 [06:36<02:24,  2.17it/s, loss=0.919, v_num=.]\n",
            "Epoch 1:  75%|███████▌  | 880/1173 [06:45<02:15,  2.17it/s, loss=0.851, v_num=.]\n",
            "Epoch 1:  77%|███████▋  | 900/1173 [06:55<02:05,  2.17it/s, loss=0.831, v_num=.]\n",
            "Epoch 1:  78%|███████▊  | 920/1173 [07:04<01:56,  2.17it/s, loss=0.831, v_num=.]\n",
            "Epoch 1:  80%|████████  | 940/1173 [07:12<01:47,  2.18it/s, loss=0.831, v_num=.]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/235 [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[2m\u001b[36m(pid=624)\u001b[0m \n",
            "Epoch 1:  82%|████████▏ | 960/1173 [07:18<01:37,  2.19it/s, loss=0.831, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=624)\u001b[0m \n",
            "Epoch 1:  84%|████████▎ | 980/1173 [07:24<01:27,  2.20it/s, loss=0.831, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=624)\u001b[0m \n",
            "Epoch 1:  85%|████████▌ | 1000/1173 [07:31<01:18,  2.22it/s, loss=0.831, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=624)\u001b[0m \n",
            "Epoch 1:  87%|████████▋ | 1020/1173 [07:37<01:08,  2.23it/s, loss=0.831, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=624)\u001b[0m \n",
            "Epoch 1:  89%|████████▊ | 1040/1173 [07:45<00:59,  2.23it/s, loss=0.831, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=624)\u001b[0m \n",
            "Epoch 1:  90%|█████████ | 1060/1173 [07:54<00:50,  2.23it/s, loss=0.831, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=624)\u001b[0m \n",
            "Epoch 1:  92%|█████████▏| 1080/1173 [08:03<00:41,  2.23it/s, loss=0.831, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=624)\u001b[0m \n",
            "Epoch 1:  94%|█████████▍| 1100/1173 [08:12<00:32,  2.23it/s, loss=0.831, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=624)\u001b[0m \n",
            "Epoch 1:  95%|█████████▌| 1120/1173 [08:21<00:23,  2.23it/s, loss=0.831, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=624)\u001b[0m \n",
            "Epoch 1:  97%|█████████▋| 1140/1173 [08:30<00:14,  2.23it/s, loss=0.831, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=624)\u001b[0m \n",
            "Epoch 1:  99%|█████████▉| 1160/1173 [08:39<00:05,  2.23it/s, loss=0.831, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=624)\u001b[0m \n",
            "Validating: 100%|██████████| 235/235 [01:33<00:00,  2.32it/s]\u001b[A\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=624)\u001b[0m /usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/deprecated_api.py:152: LightningDeprecationWarning: `Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\n",
            "\u001b[2m\u001b[36m(pid=624)\u001b[0m   \"`Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for train_LensResnet_tune_checkpoint_83150_00001:\n",
            "  auroc: 0.7151361107826233\n",
            "  date: 2021-07-08_05-24-45\n",
            "  done: false\n",
            "  experiment_id: df7e1d89950f49d7aa8323e28e9e0044\n",
            "  hostname: 0b93bd12a956\n",
            "  iterations_since_restore: 2\n",
            "  loss: 1.335142970085144\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 624\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 1062.2367935180664\n",
            "  time_this_iter_s: 529.06334233284\n",
            "  time_total_s: 1062.2367935180664\n",
            "  timestamp: 1625721885\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 2\n",
            "  trial_id: '83150_00001'\n",
            "  \n",
            "== Status ==\n",
            "Memory usage on this node: 4.2/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=1\n",
            "Bracket: Iter 2.000: 0.6313356757164001 | Iter 1.000: 0.5239685475826263\n",
            "Resources requested: 2.0/2 CPUs, 1.0/1 GPUs, 0.0/7.48 GiB heap, 0.0/3.74 GiB objects (0.0/1.0 GPU_group_0_7647334c6aa32c88cd055440d9a148be, 0.0/1.0 accelerator_type:T4, 0.0/2.0 CPU_group_7647334c6aa32c88cd055440d9a148be, 0.0/2.0 CPU_group_0_7647334c6aa32c88cd055440d9a148be, 0.0/1.0 GPU_group_7647334c6aa32c88cd055440d9a148be)\n",
            "Current best trial: 83150_00001 with auroc=0.7151361107826233 and parameters={'learning_rate': 0.001, 'batch_size': 64}\n",
            "Result logdir: /content/drive/MyDrive/Logs/LensResNet_F\n",
            "Number of trials: 12/12 (10 PENDING, 1 RUNNING, 1 TERMINATED)\n",
            "+----------------------------------------------+------------+----------------+-----------------+--------------+---------+----------+----------------------+\n",
            "| Trial name                                   | status     | loc            |   learning_rate |   batch_size |    loss |    auroc |   training_iteration |\n",
            "|----------------------------------------------+------------+----------------+-----------------+--------------+---------+----------+----------------------|\n",
            "| train_LensResnet_tune_checkpoint_83150_00001 | RUNNING    | 172.28.0.2:624 |          0.001  |           64 | 1.33514 | 0.715136 |                    2 |\n",
            "| train_LensResnet_tune_checkpoint_83150_00002 | PENDING    |                |          0.001  |          128 |         |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00003 | PENDING    |                |          0.01   |           32 |         |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00004 | PENDING    |                |          1e-05  |          128 |         |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00005 | PENDING    |                |          0.0001 |           64 |         |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00006 | PENDING    |                |          0.0001 |           32 |         |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00007 | PENDING    |                |          1e-05  |           32 |         |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00008 | PENDING    |                |          1e-05  |           32 |         |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00009 | PENDING    |                |          0.0001 |          128 |         |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00010 | PENDING    |                |          0.0001 |           32 |         |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00011 | PENDING    |                |          0.01   |           32 |         |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00000 | TERMINATED |                |          0.01   |           64 | 9.62847 | 0.673103 |                    3 |\n",
            "+----------------------------------------------+------------+----------------+-----------------+--------------+---------+----------+----------------------+\n",
            "\n",
            "\n",
            "Epoch 1: 100%|██████████| 1173/1173 [08:48<00:00,  2.22it/s, loss=0.851, v_num=.]\n",
            "                                                             \u001b[A\n",
            "Epoch 2:   0%|          | 0/1173 [00:00<?, ?it/s, loss=0.851, v_num=.]\n",
            "Epoch 2:   2%|▏         | 20/1173 [00:09<09:14,  2.08it/s, loss=0.835, v_num=.]\n",
            "Epoch 2:   3%|▎         | 40/1173 [00:18<08:57,  2.11it/s, loss=0.83, v_num=.] \n",
            "Epoch 2:   5%|▌         | 60/1173 [00:28<08:45,  2.12it/s, loss=0.818, v_num=.]\n",
            "Epoch 2:   7%|▋         | 80/1173 [00:37<08:33,  2.13it/s, loss=0.81, v_num=.] \n",
            "Epoch 2:   9%|▊         | 100/1173 [00:46<08:23,  2.13it/s, loss=0.804, v_num=.]\n",
            "Epoch 2:  10%|█         | 120/1173 [00:56<08:12,  2.14it/s, loss=0.836, v_num=.]\n",
            "Epoch 2:  12%|█▏        | 140/1173 [01:05<08:01,  2.14it/s, loss=0.802, v_num=.]\n",
            "Epoch 2:  14%|█▎        | 160/1173 [01:14<07:51,  2.15it/s, loss=0.785, v_num=.]\n",
            "Epoch 2:  15%|█▌        | 180/1173 [01:23<07:41,  2.15it/s, loss=0.775, v_num=.]\n",
            "Epoch 2:  17%|█▋        | 200/1173 [01:32<07:31,  2.15it/s, loss=0.777, v_num=.]\n",
            "Epoch 2:  19%|█▉        | 220/1173 [01:42<07:21,  2.16it/s, loss=0.738, v_num=.]\n",
            "Epoch 2:  20%|██        | 240/1173 [01:51<07:11,  2.16it/s, loss=0.777, v_num=.]\n",
            "Epoch 2:  22%|██▏       | 260/1173 [02:00<07:02,  2.16it/s, loss=0.771, v_num=.]\n",
            "Epoch 2:  24%|██▍       | 280/1173 [02:09<06:53,  2.16it/s, loss=0.809, v_num=.]\n",
            "Epoch 2:  26%|██▌       | 300/1173 [02:18<06:43,  2.16it/s, loss=0.796, v_num=.]\n",
            "Epoch 2:  27%|██▋       | 320/1173 [02:27<06:34,  2.16it/s, loss=0.789, v_num=.]\n",
            "Epoch 2:  29%|██▉       | 340/1173 [02:37<06:24,  2.17it/s, loss=0.716, v_num=.]\n",
            "Epoch 2:  31%|███       | 360/1173 [02:46<06:15,  2.17it/s, loss=0.684, v_num=.]\n",
            "Epoch 2:  32%|███▏      | 380/1173 [02:55<06:06,  2.17it/s, loss=0.738, v_num=.]\n",
            "Epoch 2:  34%|███▍      | 400/1173 [03:04<05:56,  2.17it/s, loss=0.724, v_num=.]\n",
            "Epoch 2:  36%|███▌      | 420/1173 [03:13<05:47,  2.17it/s, loss=0.725, v_num=.]\n",
            "Epoch 2:  38%|███▊      | 440/1173 [03:22<05:38,  2.17it/s, loss=0.736, v_num=.]\n",
            "Epoch 2:  39%|███▉      | 460/1173 [03:32<05:28,  2.17it/s, loss=0.783, v_num=.]\n",
            "Epoch 2:  41%|████      | 480/1173 [03:41<05:19,  2.17it/s, loss=0.708, v_num=.]\n",
            "Epoch 2:  43%|████▎     | 500/1173 [03:50<05:10,  2.17it/s, loss=0.7, v_num=.]  \n",
            "Epoch 2:  44%|████▍     | 520/1173 [03:59<05:00,  2.17it/s, loss=0.67, v_num=.]\n",
            "Epoch 2:  46%|████▌     | 540/1173 [04:08<04:51,  2.17it/s, loss=0.682, v_num=.]\n",
            "Epoch 2:  48%|████▊     | 560/1173 [04:17<04:42,  2.17it/s, loss=0.631, v_num=.]\n",
            "Epoch 2:  49%|████▉     | 580/1173 [04:27<04:33,  2.17it/s, loss=0.628, v_num=.]\n",
            "Epoch 2:  51%|█████     | 600/1173 [04:36<04:24,  2.17it/s, loss=0.678, v_num=.]\n",
            "Epoch 2:  53%|█████▎    | 620/1173 [04:45<04:14,  2.17it/s, loss=0.636, v_num=.]\n",
            "Epoch 2:  55%|█████▍    | 640/1173 [04:54<04:05,  2.17it/s, loss=0.63, v_num=.] \n",
            "Epoch 2:  56%|█████▋    | 660/1173 [05:04<03:56,  2.17it/s, loss=0.638, v_num=.]\n",
            "Epoch 2:  58%|█████▊    | 680/1173 [05:13<03:47,  2.17it/s, loss=0.637, v_num=.]\n",
            "Epoch 2:  60%|█████▉    | 700/1173 [05:22<03:37,  2.17it/s, loss=0.608, v_num=.]\n",
            "Epoch 2:  61%|██████▏   | 720/1173 [05:31<03:28,  2.17it/s, loss=0.592, v_num=.]\n",
            "Epoch 2:  63%|██████▎   | 740/1173 [05:41<03:19,  2.17it/s, loss=0.585, v_num=.]\n",
            "Epoch 2:  65%|██████▍   | 760/1173 [05:50<03:10,  2.17it/s, loss=0.565, v_num=.]\n",
            "Epoch 2:  66%|██████▋   | 780/1173 [05:59<03:01,  2.17it/s, loss=0.586, v_num=.]\n",
            "Epoch 2:  68%|██████▊   | 800/1173 [06:08<02:51,  2.17it/s, loss=0.561, v_num=.]\n",
            "Epoch 2:  70%|██████▉   | 820/1173 [06:17<02:42,  2.17it/s, loss=0.571, v_num=.]\n",
            "Epoch 2:  72%|███████▏  | 840/1173 [06:26<02:33,  2.17it/s, loss=0.554, v_num=.]\n",
            "Epoch 2:  73%|███████▎  | 860/1173 [06:35<02:24,  2.17it/s, loss=0.558, v_num=.]\n",
            "Epoch 2:  75%|███████▌  | 880/1173 [06:45<02:14,  2.17it/s, loss=0.562, v_num=.]\n",
            "Epoch 2:  77%|███████▋  | 900/1173 [06:54<02:05,  2.17it/s, loss=0.546, v_num=.]\n",
            "Epoch 2:  78%|███████▊  | 920/1173 [07:03<01:56,  2.17it/s, loss=0.53, v_num=.] \n",
            "Epoch 2:  80%|████████  | 940/1173 [07:11<01:46,  2.18it/s, loss=0.53, v_num=.]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/235 [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[2m\u001b[36m(pid=624)\u001b[0m \n",
            "Epoch 2:  82%|████████▏ | 960/1173 [07:17<01:37,  2.19it/s, loss=0.53, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=624)\u001b[0m \n",
            "Epoch 2:  84%|████████▎ | 980/1173 [07:24<01:27,  2.21it/s, loss=0.53, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=624)\u001b[0m \n",
            "Epoch 2:  85%|████████▌ | 1000/1173 [07:30<01:17,  2.22it/s, loss=0.53, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=624)\u001b[0m \n",
            "Epoch 2:  87%|████████▋ | 1020/1173 [07:36<01:08,  2.23it/s, loss=0.53, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=624)\u001b[0m \n",
            "Epoch 2:  89%|████████▊ | 1040/1173 [07:44<00:59,  2.24it/s, loss=0.53, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=624)\u001b[0m \n",
            "Epoch 2:  90%|█████████ | 1060/1173 [07:54<00:50,  2.24it/s, loss=0.53, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=624)\u001b[0m \n",
            "Epoch 2:  92%|█████████▏| 1080/1173 [08:02<00:41,  2.24it/s, loss=0.53, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=624)\u001b[0m \n",
            "Epoch 2:  94%|█████████▍| 1100/1173 [08:11<00:32,  2.24it/s, loss=0.53, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=624)\u001b[0m \n",
            "Epoch 2:  95%|█████████▌| 1120/1173 [08:20<00:23,  2.24it/s, loss=0.53, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=624)\u001b[0m \n",
            "Epoch 2:  97%|█████████▋| 1140/1173 [08:29<00:14,  2.24it/s, loss=0.53, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=624)\u001b[0m \n",
            "Epoch 2:  99%|█████████▉| 1160/1173 [08:38<00:05,  2.24it/s, loss=0.53, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=624)\u001b[0m \n",
            "Validating: 100%|██████████| 235/235 [01:33<00:00,  2.33it/s]\u001b[A\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=624)\u001b[0m /usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/deprecated_api.py:152: LightningDeprecationWarning: `Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\n",
            "\u001b[2m\u001b[36m(pid=624)\u001b[0m   \"`Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for train_LensResnet_tune_checkpoint_83150_00001:\n",
            "  auroc: 0.8491126894950867\n",
            "  date: 2021-07-08_05-33-34\n",
            "  done: true\n",
            "  experiment_id: df7e1d89950f49d7aa8323e28e9e0044\n",
            "  hostname: 0b93bd12a956\n",
            "  iterations_since_restore: 3\n",
            "  loss: 2.214468240737915\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 624\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 1590.5424454212189\n",
            "  time_this_iter_s: 528.3056519031525\n",
            "  time_total_s: 1590.5424454212189\n",
            "  timestamp: 1625722414\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 3\n",
            "  trial_id: '83150_00001'\n",
            "  \n",
            "== Status ==\n",
            "Memory usage on this node: 4.2/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=2\n",
            "Bracket: Iter 2.000: 0.6313356757164001 | Iter 1.000: 0.5239685475826263\n",
            "Resources requested: 2.0/2 CPUs, 1.0/1 GPUs, 0.0/7.48 GiB heap, 0.0/3.74 GiB objects (0.0/1.0 GPU_group_0_7647334c6aa32c88cd055440d9a148be, 0.0/1.0 GPU_group_7647334c6aa32c88cd055440d9a148be, 0.0/1.0 accelerator_type:T4, 0.0/2.0 CPU_group_7647334c6aa32c88cd055440d9a148be, 0.0/2.0 CPU_group_0_7647334c6aa32c88cd055440d9a148be)\n",
            "Current best trial: 83150_00001 with auroc=0.8491126894950867 and parameters={'learning_rate': 0.001, 'batch_size': 64}\n",
            "Result logdir: /content/drive/MyDrive/Logs/LensResNet_F\n",
            "Number of trials: 12/12 (10 PENDING, 1 RUNNING, 1 TERMINATED)\n",
            "+----------------------------------------------+------------+----------------+-----------------+--------------+---------+----------+----------------------+\n",
            "| Trial name                                   | status     | loc            |   learning_rate |   batch_size |    loss |    auroc |   training_iteration |\n",
            "|----------------------------------------------+------------+----------------+-----------------+--------------+---------+----------+----------------------|\n",
            "| train_LensResnet_tune_checkpoint_83150_00001 | RUNNING    | 172.28.0.2:624 |          0.001  |           64 | 2.21447 | 0.849113 |                    3 |\n",
            "| train_LensResnet_tune_checkpoint_83150_00002 | PENDING    |                |          0.001  |          128 |         |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00003 | PENDING    |                |          0.01   |           32 |         |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00004 | PENDING    |                |          1e-05  |          128 |         |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00005 | PENDING    |                |          0.0001 |           64 |         |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00006 | PENDING    |                |          0.0001 |           32 |         |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00007 | PENDING    |                |          1e-05  |           32 |         |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00008 | PENDING    |                |          1e-05  |           32 |         |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00009 | PENDING    |                |          0.0001 |          128 |         |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00010 | PENDING    |                |          0.0001 |           32 |         |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00011 | PENDING    |                |          0.01   |           32 |         |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00000 | TERMINATED |                |          0.01   |           64 | 9.62847 | 0.673103 |                    3 |\n",
            "+----------------------------------------------+------------+----------------+-----------------+--------------+---------+----------+----------------------+\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[33m(raylet)\u001b[0m /usr/local/lib/python3.7/dist-packages/ray/autoscaler/_private/cli_logger.py:61: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
            "\u001b[2m\u001b[33m(raylet)\u001b[0m   \"update your install command.\", FutureWarning)\n",
            "\u001b[2m\u001b[36m(pid=1088)\u001b[0m GPU available: True, used: True\n",
            "\u001b[2m\u001b[36m(pid=1088)\u001b[0m TPU available: False, using: 0 TPU cores\n",
            "\u001b[2m\u001b[36m(pid=1088)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\u001b[2m\u001b[36m(pid=1088)\u001b[0m 2021-07-08 05:33:52.631326: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "\u001b[2m\u001b[36m(pid=1088)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1088)\u001b[0m   | Name     | Type   | Params\n",
            "\u001b[2m\u001b[36m(pid=1088)\u001b[0m ------------------------------------\n",
            "\u001b[2m\u001b[36m(pid=1088)\u001b[0m 0 | backbone | ResNet | 11.2 M\n",
            "\u001b[2m\u001b[36m(pid=1088)\u001b[0m ------------------------------------\n",
            "\u001b[2m\u001b[36m(pid=1088)\u001b[0m 11.2 M    Trainable params\n",
            "\u001b[2m\u001b[36m(pid=1088)\u001b[0m 0         Non-trainable params\n",
            "\u001b[2m\u001b[36m(pid=1088)\u001b[0m 11.2 M    Total params\n",
            "\u001b[2m\u001b[36m(pid=1088)\u001b[0m 44.687    Total estimated model params size (MB)\n",
            "\u001b[2m\u001b[36m(pid=1088)\u001b[0m /usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/data_loading.py:349: UserWarning: Your val_dataloader has `shuffle=True`, it is best practice to turn this off for val/test/predict dataloaders.\n",
            "\u001b[2m\u001b[36m(pid=1088)\u001b[0m   f'Your {mode}_dataloader has `shuffle=True`, it is best practice to turn'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1088)\u001b[0m \rTraining: 0it [00:00, ?it/s]\rTraining:   0%|          | 0/587 [00:00<?, ?it/s]\rEpoch 0:   0%|          | 0/587 [00:00<?, ?it/s] \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1088)\u001b[0m /usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "\u001b[2m\u001b[36m(pid=1088)\u001b[0m   return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0:   3%|▎         | 20/587 [00:16<07:56,  1.19it/s, loss=1.23, v_num=.]\n",
            "Epoch 0:   7%|▋         | 40/587 [00:33<07:38,  1.19it/s, loss=1.18, v_num=.]\n",
            "Epoch 0:  10%|█         | 60/587 [00:50<07:25,  1.18it/s, loss=1.16, v_num=.]\n",
            "Epoch 0:  14%|█▎        | 80/587 [01:08<07:13,  1.17it/s, loss=1.14, v_num=.]\n",
            "Epoch 0:  17%|█▋        | 100/587 [01:26<07:01,  1.16it/s, loss=1.13, v_num=.]\n",
            "Epoch 0:  20%|██        | 120/587 [01:44<06:48,  1.14it/s, loss=1.12, v_num=.]\n",
            "Epoch 0:  24%|██▍       | 140/587 [02:03<06:33,  1.14it/s, loss=1.12, v_num=.]\n",
            "Epoch 0:  27%|██▋       | 160/587 [02:21<06:18,  1.13it/s, loss=1.12, v_num=.]\n",
            "Epoch 0:  31%|███       | 180/587 [02:40<06:02,  1.12it/s, loss=1.12, v_num=.]\n",
            "Epoch 0:  34%|███▍      | 200/587 [02:58<05:45,  1.12it/s, loss=1.11, v_num=.]\n",
            "Epoch 0:  37%|███▋      | 220/587 [03:17<05:28,  1.12it/s, loss=1.12, v_num=.]\n",
            "Epoch 0:  41%|████      | 240/587 [03:35<05:11,  1.11it/s, loss=1.12, v_num=.]\n",
            "Epoch 0:  44%|████▍     | 260/587 [03:53<04:53,  1.11it/s, loss=1.11, v_num=.]\n",
            "Epoch 0:  48%|████▊     | 280/587 [04:12<04:36,  1.11it/s, loss=1.1, v_num=.] \n",
            "Epoch 0:  51%|█████     | 300/587 [04:30<04:18,  1.11it/s, loss=1.09, v_num=.]\n",
            "Epoch 0:  55%|█████▍    | 320/587 [04:48<04:00,  1.11it/s, loss=1.1, v_num=.] \n",
            "Epoch 0:  58%|█████▊    | 340/587 [05:07<03:43,  1.11it/s, loss=1.1, v_num=.]\n",
            "Epoch 0:  61%|██████▏   | 360/587 [05:25<03:25,  1.10it/s, loss=1.1, v_num=.]\n",
            "Epoch 0:  65%|██████▍   | 380/587 [05:44<03:07,  1.10it/s, loss=1.08, v_num=.]\n",
            "Epoch 0:  68%|██████▊   | 400/587 [06:02<02:49,  1.10it/s, loss=1.08, v_num=.]\n",
            "Epoch 0:  72%|███████▏  | 420/587 [06:20<02:31,  1.10it/s, loss=1.05, v_num=.]\n",
            "Epoch 0:  75%|███████▍  | 440/587 [06:39<02:13,  1.10it/s, loss=1.02, v_num=.]\n",
            "Epoch 0:  78%|███████▊  | 460/587 [06:57<01:55,  1.10it/s, loss=1.02, v_num=.]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1088)\u001b[0m /usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/callback_hook.py:101: LightningDeprecationWarning: The signature of `Callback.on_train_epoch_end` has changed in v1.3. `outputs` parameter has been removed. Support for the old signature will be removed in v1.5\n",
            "\u001b[2m\u001b[36m(pid=1088)\u001b[0m   \"The signature of `Callback.on_train_epoch_end` has changed in v1.3.\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1088)\u001b[0m \rEpoch 0:  82%|████████▏ | 480/587 [07:05<01:34,  1.13it/s, loss=1.02, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1088)\u001b[0m \rValidating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/118 [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[2m\u001b[36m(pid=1088)\u001b[0m \n",
            "Epoch 0:  85%|████████▌ | 500/587 [07:18<01:16,  1.14it/s, loss=1.02, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1088)\u001b[0m \n",
            "Epoch 0:  89%|████████▊ | 520/587 [07:36<00:58,  1.14it/s, loss=1.02, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1088)\u001b[0m \n",
            "Epoch 0:  92%|█████████▏| 540/587 [07:54<00:41,  1.14it/s, loss=1.02, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1088)\u001b[0m \n",
            "Epoch 0:  95%|█████████▌| 560/587 [08:12<00:23,  1.14it/s, loss=1.02, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1088)\u001b[0m \n",
            "Epoch 0:  99%|█████████▉| 580/587 [08:30<00:06,  1.14it/s, loss=1.02, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1088)\u001b[0m \n",
            "Epoch 0: 100%|██████████| 587/587 [08:46<00:00,  1.12it/s, loss=1.02, v_num=.]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1088)\u001b[0m /usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/deprecated_api.py:152: LightningDeprecationWarning: `Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\n",
            "\u001b[2m\u001b[36m(pid=1088)\u001b[0m   \"`Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for train_LensResnet_tune_checkpoint_83150_00002:\n",
            "  auroc: 0.6530711054801941\n",
            "  date: 2021-07-08_05-42-45\n",
            "  done: false\n",
            "  experiment_id: caca53553b4b411d896dfef803623f9a\n",
            "  hostname: 0b93bd12a956\n",
            "  iterations_since_restore: 1\n",
            "  loss: 3.1327438354492188\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 1088\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 543.5132093429565\n",
            "  time_this_iter_s: 543.5132093429565\n",
            "  time_total_s: 543.5132093429565\n",
            "  timestamp: 1625722965\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 1\n",
            "  trial_id: '83150_00002'\n",
            "  \n",
            "== Status ==\n",
            "Memory usage on this node: 4.2/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=2\n",
            "Bracket: Iter 2.000: 0.6313356757164001 | Iter 1.000: 0.5439504981040955\n",
            "Resources requested: 2.0/2 CPUs, 1.0/1 GPUs, 0.0/7.48 GiB heap, 0.0/3.74 GiB objects (0.0/1.0 GPU_group_0_7647334c6aa32c88cd055440d9a148be, 0.0/2.0 CPU_group_7647334c6aa32c88cd055440d9a148be, 0.0/2.0 CPU_group_0_7647334c6aa32c88cd055440d9a148be, 0.0/1.0 GPU_group_7647334c6aa32c88cd055440d9a148be, 0.0/1.0 accelerator_type:T4)\n",
            "Current best trial: 83150_00001 with auroc=0.8491126894950867 and parameters={'learning_rate': 0.001, 'batch_size': 64}\n",
            "Result logdir: /content/drive/MyDrive/Logs/LensResNet_F\n",
            "Number of trials: 12/12 (9 PENDING, 1 RUNNING, 2 TERMINATED)\n",
            "+----------------------------------------------+------------+-----------------+-----------------+--------------+---------+----------+----------------------+\n",
            "| Trial name                                   | status     | loc             |   learning_rate |   batch_size |    loss |    auroc |   training_iteration |\n",
            "|----------------------------------------------+------------+-----------------+-----------------+--------------+---------+----------+----------------------|\n",
            "| train_LensResnet_tune_checkpoint_83150_00002 | RUNNING    | 172.28.0.2:1088 |          0.001  |          128 | 3.13274 | 0.653071 |                    1 |\n",
            "| train_LensResnet_tune_checkpoint_83150_00003 | PENDING    |                 |          0.01   |           32 |         |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00004 | PENDING    |                 |          1e-05  |          128 |         |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00005 | PENDING    |                 |          0.0001 |           64 |         |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00006 | PENDING    |                 |          0.0001 |           32 |         |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00007 | PENDING    |                 |          1e-05  |           32 |         |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00008 | PENDING    |                 |          1e-05  |           32 |         |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00009 | PENDING    |                 |          0.0001 |          128 |         |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00010 | PENDING    |                 |          0.0001 |           32 |         |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00011 | PENDING    |                 |          0.01   |           32 |         |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00000 | TERMINATED |                 |          0.01   |           64 | 9.62847 | 0.673103 |                    3 |\n",
            "| train_LensResnet_tune_checkpoint_83150_00001 | TERMINATED |                 |          0.001  |           64 | 2.21447 | 0.849113 |                    3 |\n",
            "+----------------------------------------------+------------+-----------------+-----------------+--------------+---------+----------+----------------------+\n",
            "\n",
            "\n",
            "Epoch 0: 100%|██████████| 587/587 [08:48<00:00,  1.11it/s, loss=1.01, v_num=.]\n",
            "                                                             \u001b[A\n",
            "Epoch 1:   0%|          | 0/587 [00:00<?, ?it/s, loss=1.01, v_num=.]\n",
            "Epoch 1:   3%|▎         | 20/587 [00:19<09:06,  1.04it/s, loss=0.998, v_num=.]\n",
            "Epoch 1:   7%|▋         | 40/587 [00:38<08:41,  1.05it/s, loss=1, v_num=.]    \n",
            "Epoch 1:  10%|█         | 60/587 [00:57<08:20,  1.05it/s, loss=0.951, v_num=.]\n",
            "Epoch 1:  14%|█▎        | 80/587 [01:15<08:01,  1.05it/s, loss=0.964, v_num=.]\n",
            "Epoch 1:  17%|█▋        | 100/587 [01:34<07:41,  1.06it/s, loss=0.945, v_num=.]\n",
            "Epoch 1:  20%|██        | 120/587 [01:53<07:22,  1.06it/s, loss=0.898, v_num=.]\n",
            "Epoch 1:  24%|██▍       | 140/587 [02:11<07:01,  1.06it/s, loss=0.898, v_num=.]\n",
            "Epoch 1:  27%|██▋       | 160/587 [02:30<06:41,  1.06it/s, loss=0.907, v_num=.]\n",
            "Epoch 1:  31%|███       | 180/587 [02:48<06:21,  1.07it/s, loss=0.875, v_num=.]\n",
            "Epoch 1:  34%|███▍      | 200/587 [03:06<06:01,  1.07it/s, loss=0.903, v_num=.]\n",
            "Epoch 1:  37%|███▋      | 220/587 [03:25<05:42,  1.07it/s, loss=0.868, v_num=.]\n",
            "Epoch 1:  41%|████      | 240/587 [03:43<05:23,  1.07it/s, loss=0.843, v_num=.]\n",
            "Epoch 1:  44%|████▍     | 260/587 [04:01<05:04,  1.07it/s, loss=0.819, v_num=.]\n",
            "Epoch 1:  48%|████▊     | 280/587 [04:20<04:45,  1.08it/s, loss=0.848, v_num=.]\n",
            "Epoch 1:  51%|█████     | 300/587 [04:38<04:26,  1.08it/s, loss=0.822, v_num=.]\n",
            "Epoch 1:  55%|█████▍    | 320/587 [04:57<04:07,  1.08it/s, loss=0.777, v_num=.]\n",
            "Epoch 1:  58%|█████▊    | 340/587 [05:15<03:49,  1.08it/s, loss=0.771, v_num=.]\n",
            "Epoch 1:  61%|██████▏   | 360/587 [05:33<03:30,  1.08it/s, loss=0.768, v_num=.]\n",
            "Epoch 1:  65%|██████▍   | 380/587 [05:52<03:11,  1.08it/s, loss=0.758, v_num=.]\n",
            "Epoch 1:  68%|██████▊   | 400/587 [06:10<02:53,  1.08it/s, loss=0.815, v_num=.]\n",
            "Epoch 1:  72%|███████▏  | 420/587 [06:28<02:34,  1.08it/s, loss=0.742, v_num=.]\n",
            "Epoch 1:  75%|███████▍  | 440/587 [06:47<02:16,  1.08it/s, loss=0.71, v_num=.] \n",
            "Epoch 1:  78%|███████▊  | 460/587 [07:05<01:57,  1.08it/s, loss=0.736, v_num=.]\n",
            "Epoch 1:  82%|████████▏ | 480/587 [07:13<01:36,  1.11it/s, loss=0.736, v_num=.]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/118 [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[2m\u001b[36m(pid=1088)\u001b[0m \n",
            "Epoch 1:  85%|████████▌ | 500/587 [07:26<01:17,  1.12it/s, loss=0.736, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1088)\u001b[0m \n",
            "Epoch 1:  89%|████████▊ | 520/587 [07:40<00:59,  1.13it/s, loss=0.736, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1088)\u001b[0m \n",
            "Epoch 1:  92%|█████████▏| 540/587 [07:58<00:41,  1.13it/s, loss=0.736, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1088)\u001b[0m \n",
            "Epoch 1:  95%|█████████▌| 560/587 [08:16<00:23,  1.13it/s, loss=0.736, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1088)\u001b[0m \n",
            "Epoch 1:  99%|█████████▉| 580/587 [08:34<00:06,  1.13it/s, loss=0.736, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1088)\u001b[0m \n",
            "Epoch 1: 100%|██████████| 587/587 [08:49<00:00,  1.11it/s, loss=0.736, v_num=.]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1088)\u001b[0m /usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/deprecated_api.py:152: LightningDeprecationWarning: `Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\n",
            "\u001b[2m\u001b[36m(pid=1088)\u001b[0m   \"`Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for train_LensResnet_tune_checkpoint_83150_00002:\n",
            "  auroc: 0.8292022347450256\n",
            "  date: 2021-07-08_05-51-38\n",
            "  done: false\n",
            "  experiment_id: caca53553b4b411d896dfef803623f9a\n",
            "  hostname: 0b93bd12a956\n",
            "  iterations_since_restore: 2\n",
            "  loss: 0.904776930809021\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 1088\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 1076.3702998161316\n",
            "  time_this_iter_s: 532.857090473175\n",
            "  time_total_s: 1076.3702998161316\n",
            "  timestamp: 1625723498\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 2\n",
            "  trial_id: '83150_00002'\n",
            "  \n",
            "== Status ==\n",
            "Memory usage on this node: 4.2/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=2\n",
            "Bracket: Iter 2.000: 0.7151361107826233 | Iter 1.000: 0.5439504981040955\n",
            "Resources requested: 2.0/2 CPUs, 1.0/1 GPUs, 0.0/7.48 GiB heap, 0.0/3.74 GiB objects (0.0/2.0 CPU_group_7647334c6aa32c88cd055440d9a148be, 0.0/1.0 GPU_group_0_7647334c6aa32c88cd055440d9a148be, 0.0/1.0 accelerator_type:T4, 0.0/1.0 GPU_group_7647334c6aa32c88cd055440d9a148be, 0.0/2.0 CPU_group_0_7647334c6aa32c88cd055440d9a148be)\n",
            "Current best trial: 83150_00001 with auroc=0.8491126894950867 and parameters={'learning_rate': 0.001, 'batch_size': 64}\n",
            "Result logdir: /content/drive/MyDrive/Logs/LensResNet_F\n",
            "Number of trials: 12/12 (9 PENDING, 1 RUNNING, 2 TERMINATED)\n",
            "+----------------------------------------------+------------+-----------------+-----------------+--------------+----------+----------+----------------------+\n",
            "| Trial name                                   | status     | loc             |   learning_rate |   batch_size |     loss |    auroc |   training_iteration |\n",
            "|----------------------------------------------+------------+-----------------+-----------------+--------------+----------+----------+----------------------|\n",
            "| train_LensResnet_tune_checkpoint_83150_00002 | RUNNING    | 172.28.0.2:1088 |          0.001  |          128 | 0.904777 | 0.829202 |                    2 |\n",
            "| train_LensResnet_tune_checkpoint_83150_00003 | PENDING    |                 |          0.01   |           32 |          |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00004 | PENDING    |                 |          1e-05  |          128 |          |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00005 | PENDING    |                 |          0.0001 |           64 |          |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00006 | PENDING    |                 |          0.0001 |           32 |          |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00007 | PENDING    |                 |          1e-05  |           32 |          |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00008 | PENDING    |                 |          1e-05  |           32 |          |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00009 | PENDING    |                 |          0.0001 |          128 |          |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00010 | PENDING    |                 |          0.0001 |           32 |          |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00011 | PENDING    |                 |          0.01   |           32 |          |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00000 | TERMINATED |                 |          0.01   |           64 | 9.62847  | 0.673103 |                    3 |\n",
            "| train_LensResnet_tune_checkpoint_83150_00001 | TERMINATED |                 |          0.001  |           64 | 2.21447  | 0.849113 |                    3 |\n",
            "+----------------------------------------------+------------+-----------------+-----------------+--------------+----------+----------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(pid=1088)\u001b[0m \rEpoch 1: 100%|██████████| 587/587 [08:51<00:00,  1.10it/s, loss=0.741, v_num=.]\n",
            "                                                             \u001b[A\n",
            "Epoch 2:   0%|          | 0/587 [00:00<?, ?it/s, loss=0.741, v_num=.]\n",
            "Epoch 2:   3%|▎         | 20/587 [00:19<08:59,  1.05it/s, loss=0.707, v_num=.]\n",
            "Epoch 2:   7%|▋         | 40/587 [00:37<08:33,  1.07it/s, loss=0.751, v_num=.]\n",
            "Epoch 2:  10%|█         | 60/587 [00:56<08:12,  1.07it/s, loss=0.723, v_num=.]\n",
            "Epoch 2:  14%|█▎        | 80/587 [01:14<07:54,  1.07it/s, loss=0.652, v_num=.]\n",
            "Epoch 2:  17%|█▋        | 100/587 [01:33<07:34,  1.07it/s, loss=0.709, v_num=.]\n",
            "Epoch 2:  20%|██        | 120/587 [01:51<07:15,  1.07it/s, loss=0.7, v_num=.]  \n",
            "Epoch 2:  24%|██▍       | 140/587 [02:10<06:56,  1.07it/s, loss=0.687, v_num=.]\n",
            "Epoch 2:  27%|██▋       | 160/587 [02:28<06:36,  1.08it/s, loss=0.641, v_num=.]\n",
            "Epoch 2:  31%|███       | 180/587 [02:47<06:17,  1.08it/s, loss=0.63, v_num=.] \n",
            "Epoch 2:  34%|███▍      | 200/587 [03:05<05:58,  1.08it/s, loss=0.624, v_num=.]\n",
            "Epoch 2:  37%|███▋      | 220/587 [03:23<05:39,  1.08it/s, loss=0.679, v_num=.]\n",
            "Epoch 2:  41%|████      | 240/587 [03:42<05:21,  1.08it/s, loss=0.617, v_num=.]\n",
            "Epoch 2:  44%|████▍     | 260/587 [04:00<05:02,  1.08it/s, loss=0.626, v_num=.]\n",
            "Epoch 2:  48%|████▊     | 280/587 [04:18<04:43,  1.08it/s, loss=0.582, v_num=.]\n",
            "Epoch 2:  51%|█████     | 300/587 [04:37<04:25,  1.08it/s, loss=0.635, v_num=.]\n",
            "Epoch 2:  55%|█████▍    | 320/587 [04:55<04:06,  1.08it/s, loss=0.623, v_num=.]\n",
            "Epoch 2:  58%|█████▊    | 340/587 [05:13<03:47,  1.08it/s, loss=0.571, v_num=.]\n",
            "Epoch 2:  61%|██████▏   | 360/587 [05:32<03:29,  1.08it/s, loss=0.623, v_num=.]\n",
            "Epoch 2:  65%|██████▍   | 380/587 [05:50<03:10,  1.08it/s, loss=0.597, v_num=.]\n",
            "Epoch 2:  68%|██████▊   | 400/587 [06:08<02:52,  1.08it/s, loss=0.6, v_num=.]  \n",
            "Epoch 2:  72%|███████▏  | 420/587 [06:26<02:33,  1.09it/s, loss=0.539, v_num=.]\n",
            "Epoch 2:  75%|███████▍  | 440/587 [06:45<02:15,  1.09it/s, loss=0.548, v_num=.]\n",
            "Epoch 2:  78%|███████▊  | 460/587 [07:03<01:56,  1.09it/s, loss=0.608, v_num=.]\n",
            "Epoch 2:  82%|████████▏ | 480/587 [07:11<01:36,  1.11it/s, loss=0.608, v_num=.]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/118 [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[2m\u001b[36m(pid=1088)\u001b[0m \n",
            "Epoch 2:  85%|████████▌ | 500/587 [07:23<01:17,  1.13it/s, loss=0.608, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1088)\u001b[0m \n",
            "Epoch 2:  89%|████████▊ | 520/587 [07:38<00:59,  1.13it/s, loss=0.608, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1088)\u001b[0m \n",
            "Epoch 2:  92%|█████████▏| 540/587 [07:55<00:41,  1.13it/s, loss=0.608, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1088)\u001b[0m \n",
            "Epoch 2:  95%|█████████▌| 560/587 [08:13<00:23,  1.13it/s, loss=0.608, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1088)\u001b[0m \n",
            "Epoch 2:  99%|█████████▉| 580/587 [08:31<00:06,  1.13it/s, loss=0.608, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1088)\u001b[0m \n",
            "Epoch 2: 100%|██████████| 587/587 [08:46<00:00,  1.11it/s, loss=0.608, v_num=.]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1088)\u001b[0m /usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/deprecated_api.py:152: LightningDeprecationWarning: `Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\n",
            "\u001b[2m\u001b[36m(pid=1088)\u001b[0m   \"`Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for train_LensResnet_tune_checkpoint_83150_00002:\n",
            "  auroc: 0.8051936030387878\n",
            "  date: 2021-07-08_06-00-27\n",
            "  done: true\n",
            "  experiment_id: caca53553b4b411d896dfef803623f9a\n",
            "  hostname: 0b93bd12a956\n",
            "  iterations_since_restore: 3\n",
            "  loss: 2.065347671508789\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 1088\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 1605.9440941810608\n",
            "  time_this_iter_s: 529.5737943649292\n",
            "  time_total_s: 1605.9440941810608\n",
            "  timestamp: 1625724027\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 3\n",
            "  trial_id: '83150_00002'\n",
            "  \n",
            "== Status ==\n",
            "Memory usage on this node: 4.2/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=3\n",
            "Bracket: Iter 2.000: 0.7151361107826233 | Iter 1.000: 0.5439504981040955\n",
            "Resources requested: 2.0/2 CPUs, 1.0/1 GPUs, 0.0/7.48 GiB heap, 0.0/3.74 GiB objects (0.0/1.0 GPU_group_0_7647334c6aa32c88cd055440d9a148be, 0.0/1.0 GPU_group_7647334c6aa32c88cd055440d9a148be, 0.0/1.0 accelerator_type:T4, 0.0/2.0 CPU_group_0_7647334c6aa32c88cd055440d9a148be, 0.0/2.0 CPU_group_7647334c6aa32c88cd055440d9a148be)\n",
            "Current best trial: 83150_00001 with auroc=0.8491126894950867 and parameters={'learning_rate': 0.001, 'batch_size': 64}\n",
            "Result logdir: /content/drive/MyDrive/Logs/LensResNet_F\n",
            "Number of trials: 12/12 (9 PENDING, 1 RUNNING, 2 TERMINATED)\n",
            "+----------------------------------------------+------------+-----------------+-----------------+--------------+---------+----------+----------------------+\n",
            "| Trial name                                   | status     | loc             |   learning_rate |   batch_size |    loss |    auroc |   training_iteration |\n",
            "|----------------------------------------------+------------+-----------------+-----------------+--------------+---------+----------+----------------------|\n",
            "| train_LensResnet_tune_checkpoint_83150_00002 | RUNNING    | 172.28.0.2:1088 |          0.001  |          128 | 2.06535 | 0.805194 |                    3 |\n",
            "| train_LensResnet_tune_checkpoint_83150_00003 | PENDING    |                 |          0.01   |           32 |         |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00004 | PENDING    |                 |          1e-05  |          128 |         |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00005 | PENDING    |                 |          0.0001 |           64 |         |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00006 | PENDING    |                 |          0.0001 |           32 |         |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00007 | PENDING    |                 |          1e-05  |           32 |         |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00008 | PENDING    |                 |          1e-05  |           32 |         |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00009 | PENDING    |                 |          0.0001 |          128 |         |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00010 | PENDING    |                 |          0.0001 |           32 |         |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00011 | PENDING    |                 |          0.01   |           32 |         |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00000 | TERMINATED |                 |          0.01   |           64 | 9.62847 | 0.673103 |                    3 |\n",
            "| train_LensResnet_tune_checkpoint_83150_00001 | TERMINATED |                 |          0.001  |           64 | 2.21447 | 0.849113 |                    3 |\n",
            "+----------------------------------------------+------------+-----------------+-----------------+--------------+---------+----------+----------------------+\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[33m(raylet)\u001b[0m /usr/local/lib/python3.7/dist-packages/ray/autoscaler/_private/cli_logger.py:61: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
            "\u001b[2m\u001b[33m(raylet)\u001b[0m   \"update your install command.\", FutureWarning)\n",
            "\u001b[2m\u001b[33m(raylet)\u001b[0m /usr/local/lib/python3.7/dist-packages/ray/autoscaler/_private/cli_logger.py:61: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
            "\u001b[2m\u001b[33m(raylet)\u001b[0m   \"update your install command.\", FutureWarning)\n",
            "\u001b[2m\u001b[36m(pid=1340)\u001b[0m GPU available: True, used: True\n",
            "\u001b[2m\u001b[36m(pid=1340)\u001b[0m TPU available: False, using: 0 TPU cores\n",
            "\u001b[2m\u001b[36m(pid=1340)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\u001b[2m\u001b[36m(pid=1340)\u001b[0m 2021-07-08 06:00:48.020660: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "\u001b[2m\u001b[36m(pid=1340)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1340)\u001b[0m   | Name     | Type   | Params\n",
            "\u001b[2m\u001b[36m(pid=1340)\u001b[0m ------------------------------------\n",
            "\u001b[2m\u001b[36m(pid=1340)\u001b[0m 0 | backbone | ResNet | 11.2 M\n",
            "\u001b[2m\u001b[36m(pid=1340)\u001b[0m ------------------------------------\n",
            "\u001b[2m\u001b[36m(pid=1340)\u001b[0m 11.2 M    Trainable params\n",
            "\u001b[2m\u001b[36m(pid=1340)\u001b[0m 0         Non-trainable params\n",
            "\u001b[2m\u001b[36m(pid=1340)\u001b[0m 11.2 M    Total params\n",
            "\u001b[2m\u001b[36m(pid=1340)\u001b[0m 44.687    Total estimated model params size (MB)\n",
            "\u001b[2m\u001b[36m(pid=1340)\u001b[0m /usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/data_loading.py:349: UserWarning: Your val_dataloader has `shuffle=True`, it is best practice to turn this off for val/test/predict dataloaders.\n",
            "\u001b[2m\u001b[36m(pid=1340)\u001b[0m   f'Your {mode}_dataloader has `shuffle=True`, it is best practice to turn'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1340)\u001b[0m \rTraining: 0it [00:00, ?it/s]\rTraining:   0%|          | 0/2344 [00:00<?, ?it/s]\rEpoch 0:   0%|          | 0/2344 [00:00<?, ?it/s] \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1340)\u001b[0m /usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "\u001b[2m\u001b[36m(pid=1340)\u001b[0m   return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0:   1%|          | 20/2344 [00:04<08:02,  4.81it/s, loss=1.7, v_num=.]\n",
            "Epoch 0:   2%|▏         | 40/2344 [00:07<07:37,  5.04it/s, loss=1.27, v_num=.]\n",
            "Epoch 0:   3%|▎         | 60/2344 [00:11<07:26,  5.12it/s, loss=1.25, v_num=.]\n",
            "Epoch 0:   3%|▎         | 80/2344 [00:15<07:22,  5.12it/s, loss=1.2, v_num=.] \n",
            "Epoch 0:   4%|▍         | 100/2344 [00:19<07:18,  5.12it/s, loss=1.18, v_num=.]\n",
            "Epoch 0:   5%|▌         | 120/2344 [00:23<07:16,  5.09it/s, loss=1.18, v_num=.]\n",
            "Epoch 0:   5%|▌         | 120/2344 [00:23<07:16,  5.09it/s, loss=1.22, v_num=.]\n",
            "Epoch 0:   6%|▌         | 140/2344 [00:27<07:13,  5.08it/s, loss=1.19, v_num=.]\n",
            "Epoch 0:   7%|▋         | 160/2344 [00:31<07:10,  5.07it/s, loss=1.15, v_num=.]\n",
            "Epoch 0:   8%|▊         | 180/2344 [00:35<07:08,  5.05it/s, loss=1.13, v_num=.]\n",
            "Epoch 0:   9%|▊         | 200/2344 [00:39<07:05,  5.04it/s, loss=1.14, v_num=.]\n",
            "Epoch 0:   9%|▉         | 220/2344 [00:43<07:01,  5.04it/s, loss=1.17, v_num=.]\n",
            "Epoch 0:  10%|█         | 240/2344 [00:47<06:57,  5.04it/s, loss=1.18, v_num=.]\n",
            "Epoch 0:  11%|█         | 260/2344 [00:52<06:57,  4.99it/s, loss=1.15, v_num=.]\n",
            "Epoch 0:  12%|█▏        | 280/2344 [00:56<06:57,  4.94it/s, loss=1.16, v_num=.]\n",
            "Epoch 0:  13%|█▎        | 300/2344 [01:01<06:56,  4.91it/s, loss=1.15, v_num=.]\n",
            "Epoch 0:  14%|█▎        | 320/2344 [01:05<06:55,  4.88it/s, loss=1.15, v_num=.]\n",
            "Epoch 0:  15%|█▍        | 340/2344 [01:10<06:53,  4.85it/s, loss=1.15, v_num=.]\n",
            "Epoch 0:  15%|█▌        | 360/2344 [01:14<06:52,  4.80it/s, loss=1.13, v_num=.]\n",
            "Epoch 0:  16%|█▌        | 380/2344 [01:19<06:51,  4.77it/s, loss=1.14, v_num=.]\n",
            "Epoch 0:  17%|█▋        | 400/2344 [01:24<06:49,  4.75it/s, loss=1.12, v_num=.]\n",
            "Epoch 0:  18%|█▊        | 420/2344 [01:28<06:47,  4.72it/s, loss=1.12, v_num=.]\n",
            "Epoch 0:  19%|█▉        | 440/2344 [01:33<06:45,  4.70it/s, loss=1.15, v_num=.]\n",
            "Epoch 0:  20%|█▉        | 460/2344 [01:38<06:43,  4.66it/s, loss=1.13, v_num=.]\n",
            "Epoch 0:  20%|██        | 480/2344 [01:43<06:41,  4.64it/s, loss=1.13, v_num=.]\n",
            "Epoch 0:  21%|██▏       | 500/2344 [01:48<06:39,  4.61it/s, loss=1.13, v_num=.]\n",
            "Epoch 0:  22%|██▏       | 520/2344 [01:53<06:37,  4.59it/s, loss=1.13, v_num=.]\n",
            "Epoch 0:  23%|██▎       | 540/2344 [01:57<06:34,  4.58it/s, loss=1.12, v_num=.]\n",
            "Epoch 0:  24%|██▍       | 560/2344 [02:02<06:30,  4.57it/s, loss=1.13, v_num=.]\n",
            "Epoch 0:  25%|██▍       | 580/2344 [02:07<06:27,  4.55it/s, loss=1.12, v_num=.]\n",
            "Epoch 0:  26%|██▌       | 600/2344 [02:12<06:23,  4.54it/s, loss=1.12, v_num=.]\n",
            "Epoch 0:  26%|██▋       | 620/2344 [02:16<06:20,  4.53it/s, loss=1.11, v_num=.]\n",
            "Epoch 0:  27%|██▋       | 640/2344 [02:21<06:16,  4.52it/s, loss=1.12, v_num=.]\n",
            "Epoch 0:  28%|██▊       | 660/2344 [02:26<06:12,  4.52it/s, loss=1.12, v_num=.]\n",
            "Epoch 0:  29%|██▉       | 680/2344 [02:30<06:09,  4.51it/s, loss=1.1, v_num=.] \n",
            "Epoch 0:  30%|██▉       | 700/2344 [02:35<06:04,  4.51it/s, loss=1.11, v_num=.]\n",
            "Epoch 0:  31%|███       | 720/2344 [02:39<06:00,  4.51it/s, loss=1.1, v_num=.] \n",
            "Epoch 0:  32%|███▏      | 740/2344 [02:44<05:56,  4.51it/s, loss=1.11, v_num=.]\n",
            "Epoch 0:  32%|███▏      | 760/2344 [02:48<05:51,  4.50it/s, loss=1.1, v_num=.] \n",
            "Epoch 0:  33%|███▎      | 780/2344 [02:53<05:47,  4.50it/s, loss=1.1, v_num=.]\n",
            "Epoch 0:  34%|███▍      | 800/2344 [02:57<05:43,  4.50it/s, loss=1.11, v_num=.]\n",
            "Epoch 0:  35%|███▍      | 820/2344 [03:02<05:38,  4.50it/s, loss=1.11, v_num=.]\n",
            "Epoch 0:  36%|███▌      | 840/2344 [03:07<05:34,  4.49it/s, loss=1.1, v_num=.] \n",
            "Epoch 0:  37%|███▋      | 860/2344 [03:11<05:30,  4.49it/s, loss=1.11, v_num=.]\n",
            "Epoch 0:  38%|███▊      | 880/2344 [03:15<05:26,  4.49it/s, loss=1.11, v_num=.]\n",
            "Epoch 0:  38%|███▊      | 900/2344 [03:20<05:21,  4.49it/s, loss=1.11, v_num=.]\n",
            "Epoch 0:  39%|███▉      | 920/2344 [03:25<05:17,  4.49it/s, loss=1.1, v_num=.] \n",
            "Epoch 0:  40%|████      | 940/2344 [03:29<05:12,  4.49it/s, loss=1.1, v_num=.]\n",
            "Epoch 0:  41%|████      | 960/2344 [03:33<05:08,  4.49it/s, loss=1.11, v_num=.]\n",
            "Epoch 0:  42%|████▏     | 980/2344 [03:38<05:04,  4.49it/s, loss=1.11, v_num=.]\n",
            "Epoch 0:  43%|████▎     | 1000/2344 [03:43<04:59,  4.48it/s, loss=1.11, v_num=.]\n",
            "Epoch 0:  44%|████▎     | 1020/2344 [03:47<04:55,  4.48it/s, loss=1.1, v_num=.] \n",
            "Epoch 0:  44%|████▍     | 1040/2344 [03:52<04:50,  4.48it/s, loss=1.11, v_num=.]\n",
            "Epoch 0:  45%|████▌     | 1060/2344 [03:56<04:46,  4.48it/s, loss=1.1, v_num=.] \n",
            "Epoch 0:  46%|████▌     | 1080/2344 [04:00<04:42,  4.48it/s, loss=1.1, v_num=.]\n",
            "Epoch 0:  47%|████▋     | 1100/2344 [04:05<04:37,  4.48it/s, loss=1.1, v_num=.]\n",
            "Epoch 0:  48%|████▊     | 1120/2344 [04:10<04:33,  4.48it/s, loss=1.11, v_num=.]\n",
            "Epoch 0:  49%|████▊     | 1140/2344 [04:14<04:29,  4.47it/s, loss=1.1, v_num=.] \n",
            "Epoch 0:  49%|████▉     | 1160/2344 [04:19<04:24,  4.47it/s, loss=1.1, v_num=.]\n",
            "Epoch 0:  50%|█████     | 1180/2344 [04:23<04:20,  4.47it/s, loss=1.1, v_num=.]\n",
            "Epoch 0:  51%|█████     | 1200/2344 [04:28<04:16,  4.47it/s, loss=1.1, v_num=.]\n",
            "Epoch 0:  52%|█████▏    | 1220/2344 [04:33<04:11,  4.47it/s, loss=1.1, v_num=.]\n",
            "Epoch 0:  53%|█████▎    | 1240/2344 [04:37<04:07,  4.47it/s, loss=1.1, v_num=.]\n",
            "Epoch 0:  54%|█████▍    | 1260/2344 [04:42<04:02,  4.47it/s, loss=1.09, v_num=.]\n",
            "Epoch 0:  55%|█████▍    | 1280/2344 [04:46<03:58,  4.47it/s, loss=1.11, v_num=.]\n",
            "Epoch 0:  55%|█████▌    | 1300/2344 [04:51<03:53,  4.47it/s, loss=1.1, v_num=.] \n",
            "Epoch 0:  56%|█████▋    | 1320/2344 [04:55<03:49,  4.47it/s, loss=1.1, v_num=.]\n",
            "Epoch 0:  57%|█████▋    | 1340/2344 [05:00<03:44,  4.46it/s, loss=1.1, v_num=.]\n",
            "Epoch 0:  58%|█████▊    | 1360/2344 [05:04<03:40,  4.46it/s, loss=1.1, v_num=.]\n",
            "Epoch 0:  59%|█████▉    | 1380/2344 [05:09<03:36,  4.46it/s, loss=1.1, v_num=.]\n",
            "Epoch 0:  60%|█████▉    | 1400/2344 [05:13<03:31,  4.46it/s, loss=1.1, v_num=.]\n",
            "Epoch 0:  61%|██████    | 1420/2344 [05:18<03:27,  4.46it/s, loss=1.1, v_num=.]\n",
            "Epoch 0:  61%|██████▏   | 1440/2344 [05:22<03:22,  4.46it/s, loss=1.1, v_num=.]\n",
            "Epoch 0:  62%|██████▏   | 1460/2344 [05:27<03:18,  4.46it/s, loss=1.1, v_num=.]\n",
            "Epoch 0:  63%|██████▎   | 1480/2344 [05:31<03:13,  4.46it/s, loss=1.1, v_num=.]\n",
            "Epoch 0:  64%|██████▍   | 1500/2344 [05:36<03:09,  4.46it/s, loss=1.11, v_num=.]\n",
            "Epoch 0:  65%|██████▍   | 1520/2344 [05:41<03:04,  4.46it/s, loss=1.1, v_num=.] \n",
            "Epoch 0:  66%|██████▌   | 1540/2344 [05:45<03:00,  4.46it/s, loss=1.11, v_num=.]\n",
            "Epoch 0:  67%|██████▋   | 1560/2344 [05:49<02:55,  4.46it/s, loss=1.1, v_num=.] \n",
            "Epoch 0:  67%|██████▋   | 1580/2344 [05:54<02:51,  4.46it/s, loss=1.1, v_num=.]\n",
            "Epoch 0:  68%|██████▊   | 1600/2344 [05:59<02:46,  4.46it/s, loss=1.1, v_num=.]\n",
            "Epoch 0:  69%|██████▉   | 1620/2344 [06:03<02:42,  4.46it/s, loss=1.1, v_num=.]\n",
            "Epoch 0:  70%|██████▉   | 1640/2344 [06:07<02:37,  4.46it/s, loss=1.11, v_num=.]\n",
            "Epoch 0:  71%|███████   | 1660/2344 [06:12<02:33,  4.46it/s, loss=1.1, v_num=.] \n",
            "Epoch 0:  72%|███████▏  | 1680/2344 [06:16<02:28,  4.46it/s, loss=1.1, v_num=.]\n",
            "Epoch 0:  73%|███████▎  | 1700/2344 [06:21<02:24,  4.46it/s, loss=1.11, v_num=.]\n",
            "Epoch 0:  73%|███████▎  | 1720/2344 [06:25<02:19,  4.46it/s, loss=1.1, v_num=.] \n",
            "Epoch 0:  74%|███████▍  | 1740/2344 [06:30<02:15,  4.46it/s, loss=1.1, v_num=.]\n",
            "Epoch 0:  75%|███████▌  | 1760/2344 [06:34<02:10,  4.46it/s, loss=1.11, v_num=.]\n",
            "Epoch 0:  76%|███████▌  | 1780/2344 [06:38<02:06,  4.46it/s, loss=1.1, v_num=.] \n",
            "Epoch 0:  77%|███████▋  | 1800/2344 [06:43<02:01,  4.46it/s, loss=1.1, v_num=.]\n",
            "Epoch 0:  78%|███████▊  | 1820/2344 [06:48<01:57,  4.46it/s, loss=1.1, v_num=.]\n",
            "Epoch 0:  78%|███████▊  | 1840/2344 [06:52<01:52,  4.46it/s, loss=1.1, v_num=.]\n",
            "Epoch 0:  79%|███████▉  | 1860/2344 [06:57<01:48,  4.46it/s, loss=1.1, v_num=.]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1340)\u001b[0m /usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/callback_hook.py:101: LightningDeprecationWarning: The signature of `Callback.on_train_epoch_end` has changed in v1.3. `outputs` parameter has been removed. Support for the old signature will be removed in v1.5\n",
            "\u001b[2m\u001b[36m(pid=1340)\u001b[0m   \"The signature of `Callback.on_train_epoch_end` has changed in v1.3.\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1340)\u001b[0m \rEpoch 0:  80%|████████  | 1880/2344 [07:00<01:43,  4.47it/s, loss=1.1, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1340)\u001b[0m \rValidating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/469 [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[2m\u001b[36m(pid=1340)\u001b[0m \n",
            "Epoch 0:  81%|████████  | 1900/2344 [07:04<01:39,  4.47it/s, loss=1.1, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1340)\u001b[0m \n",
            "Epoch 0:  82%|████████▏ | 1920/2344 [07:09<01:34,  4.47it/s, loss=1.1, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1340)\u001b[0m \n",
            "Epoch 0:  83%|████████▎ | 1940/2344 [07:13<01:30,  4.47it/s, loss=1.1, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1340)\u001b[0m \n",
            "Epoch 0:  84%|████████▎ | 1960/2344 [07:18<01:25,  4.47it/s, loss=1.1, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1340)\u001b[0m \n",
            "Epoch 0:  84%|████████▍ | 1980/2344 [07:22<01:21,  4.47it/s, loss=1.1, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1340)\u001b[0m \n",
            "Epoch 0:  85%|████████▌ | 2000/2344 [07:27<01:16,  4.47it/s, loss=1.1, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1340)\u001b[0m \n",
            "Epoch 0:  86%|████████▌ | 2020/2344 [07:31<01:12,  4.47it/s, loss=1.1, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1340)\u001b[0m \n",
            "Epoch 0:  87%|████████▋ | 2040/2344 [07:35<01:07,  4.47it/s, loss=1.1, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1340)\u001b[0m \n",
            "Epoch 0:  88%|████████▊ | 2060/2344 [07:40<01:03,  4.47it/s, loss=1.1, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1340)\u001b[0m \n",
            "Epoch 0:  89%|████████▊ | 2080/2344 [07:44<00:59,  4.47it/s, loss=1.1, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1340)\u001b[0m \n",
            "Epoch 0:  90%|████████▉ | 2100/2344 [07:49<00:54,  4.47it/s, loss=1.1, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1340)\u001b[0m \n",
            "Epoch 0:  90%|█████████ | 2120/2344 [07:53<00:50,  4.47it/s, loss=1.1, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1340)\u001b[0m \n",
            "Epoch 0:  91%|█████████▏| 2140/2344 [07:58<00:45,  4.47it/s, loss=1.1, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1340)\u001b[0m \n",
            "Epoch 0:  92%|█████████▏| 2160/2344 [08:02<00:41,  4.47it/s, loss=1.1, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1340)\u001b[0m \n",
            "Epoch 0:  93%|█████████▎| 2180/2344 [08:07<00:36,  4.47it/s, loss=1.1, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1340)\u001b[0m \n",
            "Epoch 0:  94%|█████████▍| 2200/2344 [08:11<00:32,  4.47it/s, loss=1.1, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1340)\u001b[0m \n",
            "Epoch 0:  95%|█████████▍| 2220/2344 [08:16<00:27,  4.47it/s, loss=1.1, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1340)\u001b[0m \n",
            "Epoch 0:  96%|█████████▌| 2240/2344 [08:20<00:23,  4.47it/s, loss=1.1, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1340)\u001b[0m \n",
            "Epoch 0:  96%|█████████▋| 2260/2344 [08:25<00:18,  4.47it/s, loss=1.1, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1340)\u001b[0m \n",
            "Epoch 0:  97%|█████████▋| 2280/2344 [08:29<00:14,  4.47it/s, loss=1.1, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1340)\u001b[0m \n",
            "Epoch 0:  98%|█████████▊| 2300/2344 [08:34<00:09,  4.47it/s, loss=1.1, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1340)\u001b[0m \n",
            "Epoch 0:  99%|█████████▉| 2320/2344 [08:38<00:05,  4.47it/s, loss=1.1, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1340)\u001b[0m \n",
            "Epoch 0: 100%|█████████▉| 2340/2344 [08:42<00:00,  4.48it/s, loss=1.1, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1340)\u001b[0m \n",
            "Validating: 100%|██████████| 469/469 [01:44<00:00,  4.50it/s]\u001b[A\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1340)\u001b[0m /usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/deprecated_api.py:152: LightningDeprecationWarning: `Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\n",
            "\u001b[2m\u001b[36m(pid=1340)\u001b[0m   \"`Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for train_LensResnet_tune_checkpoint_83150_00003:\n",
            "  auroc: 0.4955345690250397\n",
            "  date: 2021-07-08_06-09-38\n",
            "  done: true\n",
            "  experiment_id: e7850442bd644aaeb5aa062274aa8010\n",
            "  hostname: 0b93bd12a956\n",
            "  iterations_since_restore: 1\n",
            "  loss: 1.1013730764389038\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 1340\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 539.6207184791565\n",
            "  time_this_iter_s: 539.6207184791565\n",
            "  time_total_s: 539.6207184791565\n",
            "  timestamp: 1625724578\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 1\n",
            "  trial_id: '83150_00003'\n",
            "  \n",
            "== Status ==\n",
            "Memory usage on this node: 4.3/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=4\n",
            "Bracket: Iter 2.000: 0.7151361107826233 | Iter 1.000: 0.5239685475826263\n",
            "Resources requested: 2.0/2 CPUs, 1.0/1 GPUs, 0.0/7.48 GiB heap, 0.0/3.74 GiB objects (0.0/2.0 CPU_group_7647334c6aa32c88cd055440d9a148be, 0.0/2.0 CPU_group_0_7647334c6aa32c88cd055440d9a148be, 0.0/1.0 accelerator_type:T4, 0.0/1.0 GPU_group_7647334c6aa32c88cd055440d9a148be, 0.0/1.0 GPU_group_0_7647334c6aa32c88cd055440d9a148be)\n",
            "Current best trial: 83150_00001 with auroc=0.8491126894950867 and parameters={'learning_rate': 0.001, 'batch_size': 64}\n",
            "Result logdir: /content/drive/MyDrive/Logs/LensResNet_F\n",
            "Number of trials: 12/12 (8 PENDING, 1 RUNNING, 3 TERMINATED)\n",
            "+----------------------------------------------+------------+-----------------+-----------------+--------------+---------+----------+----------------------+\n",
            "| Trial name                                   | status     | loc             |   learning_rate |   batch_size |    loss |    auroc |   training_iteration |\n",
            "|----------------------------------------------+------------+-----------------+-----------------+--------------+---------+----------+----------------------|\n",
            "| train_LensResnet_tune_checkpoint_83150_00003 | RUNNING    | 172.28.0.2:1340 |          0.01   |           32 | 1.10137 | 0.495535 |                    1 |\n",
            "| train_LensResnet_tune_checkpoint_83150_00004 | PENDING    |                 |          1e-05  |          128 |         |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00005 | PENDING    |                 |          0.0001 |           64 |         |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00006 | PENDING    |                 |          0.0001 |           32 |         |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00007 | PENDING    |                 |          1e-05  |           32 |         |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00008 | PENDING    |                 |          1e-05  |           32 |         |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00009 | PENDING    |                 |          0.0001 |          128 |         |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00010 | PENDING    |                 |          0.0001 |           32 |         |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00011 | PENDING    |                 |          0.01   |           32 |         |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00000 | TERMINATED |                 |          0.01   |           64 | 9.62847 | 0.673103 |                    3 |\n",
            "| train_LensResnet_tune_checkpoint_83150_00001 | TERMINATED |                 |          0.001  |           64 | 2.21447 | 0.849113 |                    3 |\n",
            "| train_LensResnet_tune_checkpoint_83150_00002 | TERMINATED |                 |          0.001  |          128 | 2.06535 | 0.805194 |                    3 |\n",
            "+----------------------------------------------+------------+-----------------+-----------------+--------------+---------+----------+----------------------+\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1357)\u001b[0m GPU available: True, used: True\n",
            "\u001b[2m\u001b[36m(pid=1357)\u001b[0m TPU available: False, using: 0 TPU cores\n",
            "\u001b[2m\u001b[36m(pid=1357)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\u001b[2m\u001b[36m(pid=1357)\u001b[0m 2021-07-08 06:09:53.000776: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "\u001b[2m\u001b[36m(pid=1357)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1357)\u001b[0m   | Name     | Type   | Params\n",
            "\u001b[2m\u001b[36m(pid=1357)\u001b[0m ------------------------------------\n",
            "\u001b[2m\u001b[36m(pid=1357)\u001b[0m 0 | backbone | ResNet | 11.2 M\n",
            "\u001b[2m\u001b[36m(pid=1357)\u001b[0m ------------------------------------\n",
            "\u001b[2m\u001b[36m(pid=1357)\u001b[0m 11.2 M    Trainable params\n",
            "\u001b[2m\u001b[36m(pid=1357)\u001b[0m 0         Non-trainable params\n",
            "\u001b[2m\u001b[36m(pid=1357)\u001b[0m 11.2 M    Total params\n",
            "\u001b[2m\u001b[36m(pid=1357)\u001b[0m 44.687    Total estimated model params size (MB)\n",
            "\u001b[2m\u001b[36m(pid=1357)\u001b[0m /usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/data_loading.py:349: UserWarning: Your val_dataloader has `shuffle=True`, it is best practice to turn this off for val/test/predict dataloaders.\n",
            "\u001b[2m\u001b[36m(pid=1357)\u001b[0m   f'Your {mode}_dataloader has `shuffle=True`, it is best practice to turn'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1357)\u001b[0m \rTraining: 0it [00:00, ?it/s]\rTraining:   0%|          | 0/587 [00:00<?, ?it/s]\rEpoch 0:   0%|          | 0/587 [00:00<?, ?it/s] \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1357)\u001b[0m /usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "\u001b[2m\u001b[36m(pid=1357)\u001b[0m   return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0:   3%|▎         | 20/587 [00:16<07:39,  1.23it/s, loss=1.21, v_num=.]\n",
            "Epoch 0:   7%|▋         | 40/587 [00:32<07:24,  1.23it/s, loss=1.18, v_num=.]\n",
            "Epoch 0:  10%|█         | 60/587 [00:49<07:11,  1.22it/s, loss=1.16, v_num=.]\n",
            "Epoch 0:  14%|█▎        | 80/587 [01:06<06:59,  1.21it/s, loss=1.17, v_num=.]\n",
            "Epoch 0:  17%|█▋        | 100/587 [01:24<06:49,  1.19it/s, loss=1.16, v_num=.]\n",
            "Epoch 0:  20%|██        | 120/587 [01:42<06:37,  1.18it/s, loss=1.16, v_num=.]\n",
            "Epoch 0:  24%|██▍       | 140/587 [02:00<06:23,  1.16it/s, loss=1.16, v_num=.]\n",
            "Epoch 0:  27%|██▋       | 160/587 [02:18<06:09,  1.16it/s, loss=1.16, v_num=.]\n",
            "Epoch 0:  31%|███       | 180/587 [02:36<05:54,  1.15it/s, loss=1.16, v_num=.]\n",
            "Epoch 0:  31%|███       | 180/587 [02:36<05:54,  1.15it/s, loss=1.15, v_num=.]\n",
            "Epoch 0:  34%|███▍      | 200/587 [02:55<05:39,  1.14it/s, loss=1.16, v_num=.]\n",
            "Epoch 0:  37%|███▋      | 220/587 [03:13<05:22,  1.14it/s, loss=1.14, v_num=.]\n",
            "Epoch 0:  41%|████      | 240/587 [03:31<05:06,  1.13it/s, loss=1.15, v_num=.]\n",
            "Epoch 0:  44%|████▍     | 260/587 [03:49<04:48,  1.13it/s, loss=1.14, v_num=.]\n",
            "Epoch 0:  48%|████▊     | 280/587 [04:07<04:31,  1.13it/s, loss=1.14, v_num=.]\n",
            "Epoch 0:  51%|█████     | 300/587 [04:25<04:14,  1.13it/s, loss=1.14, v_num=.]\n",
            "Epoch 0:  55%|█████▍    | 320/587 [04:43<03:56,  1.13it/s, loss=1.14, v_num=.]\n",
            "Epoch 0:  58%|█████▊    | 340/587 [05:02<03:39,  1.13it/s, loss=1.13, v_num=.]\n",
            "Epoch 0:  61%|██████▏   | 360/587 [05:20<03:21,  1.12it/s, loss=1.15, v_num=.]\n",
            "Epoch 0:  65%|██████▍   | 380/587 [05:38<03:04,  1.12it/s, loss=1.14, v_num=.]\n",
            "Epoch 0:  68%|██████▊   | 400/587 [05:56<02:46,  1.12it/s, loss=1.16, v_num=.]\n",
            "Epoch 0:  72%|███████▏  | 420/587 [06:14<02:28,  1.12it/s, loss=1.15, v_num=.]\n",
            "Epoch 0:  75%|███████▍  | 440/587 [06:32<02:11,  1.12it/s, loss=1.14, v_num=.]\n",
            "Epoch 0:  78%|███████▊  | 460/587 [06:50<01:53,  1.12it/s, loss=1.13, v_num=.]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1357)\u001b[0m /usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/callback_hook.py:101: LightningDeprecationWarning: The signature of `Callback.on_train_epoch_end` has changed in v1.3. `outputs` parameter has been removed. Support for the old signature will be removed in v1.5\n",
            "\u001b[2m\u001b[36m(pid=1357)\u001b[0m   \"The signature of `Callback.on_train_epoch_end` has changed in v1.3.\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1357)\u001b[0m \rEpoch 0:  82%|████████▏ | 480/587 [06:57<01:33,  1.15it/s, loss=1.13, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1357)\u001b[0m \rValidating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/118 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 0:  82%|████████▏ | 480/587 [07:10<01:35,  1.12it/s, loss=1.13, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1357)\u001b[0m \n",
            "Epoch 0:  85%|████████▌ | 500/587 [07:14<01:15,  1.15it/s, loss=1.13, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1357)\u001b[0m \n",
            "Epoch 0:  89%|████████▊ | 520/587 [07:31<00:58,  1.15it/s, loss=1.13, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1357)\u001b[0m \n",
            "Epoch 0:  92%|█████████▏| 540/587 [07:49<00:40,  1.15it/s, loss=1.13, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1357)\u001b[0m \n",
            "Epoch 0:  95%|█████████▌| 560/587 [08:07<00:23,  1.15it/s, loss=1.13, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1357)\u001b[0m \n",
            "Epoch 0:  99%|█████████▉| 580/587 [08:25<00:06,  1.15it/s, loss=1.13, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1357)\u001b[0m \n",
            "Epoch 0: 100%|██████████| 587/587 [08:40<00:00,  1.13it/s, loss=1.13, v_num=.]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1357)\u001b[0m /usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/deprecated_api.py:152: LightningDeprecationWarning: `Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\n",
            "\u001b[2m\u001b[36m(pid=1357)\u001b[0m   \"`Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for train_LensResnet_tune_checkpoint_83150_00004:\n",
            "  auroc: 0.5865452885627747\n",
            "  date: 2021-07-08_06-18-39\n",
            "  done: false\n",
            "  experiment_id: 478ced1d31b442d68df9fefb9ffdca01\n",
            "  hostname: 0b93bd12a956\n",
            "  iterations_since_restore: 1\n",
            "  loss: 1.079414963722229\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 1357\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 535.4767372608185\n",
            "  time_this_iter_s: 535.4767372608185\n",
            "  time_total_s: 535.4767372608185\n",
            "  timestamp: 1625725119\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 1\n",
            "  trial_id: '83150_00004'\n",
            "  \n",
            "== Status ==\n",
            "Memory usage on this node: 4.2/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=4\n",
            "Bracket: Iter 2.000: 0.7151361107826233 | Iter 1.000: 0.5439504981040955\n",
            "Resources requested: 2.0/2 CPUs, 1.0/1 GPUs, 0.0/7.48 GiB heap, 0.0/3.74 GiB objects (0.0/1.0 accelerator_type:T4, 0.0/2.0 CPU_group_7647334c6aa32c88cd055440d9a148be, 0.0/1.0 GPU_group_0_7647334c6aa32c88cd055440d9a148be, 0.0/1.0 GPU_group_7647334c6aa32c88cd055440d9a148be, 0.0/2.0 CPU_group_0_7647334c6aa32c88cd055440d9a148be)\n",
            "Current best trial: 83150_00001 with auroc=0.8491126894950867 and parameters={'learning_rate': 0.001, 'batch_size': 64}\n",
            "Result logdir: /content/drive/MyDrive/Logs/LensResNet_F\n",
            "Number of trials: 12/12 (7 PENDING, 1 RUNNING, 4 TERMINATED)\n",
            "+----------------------------------------------+------------+-----------------+-----------------+--------------+---------+----------+----------------------+\n",
            "| Trial name                                   | status     | loc             |   learning_rate |   batch_size |    loss |    auroc |   training_iteration |\n",
            "|----------------------------------------------+------------+-----------------+-----------------+--------------+---------+----------+----------------------|\n",
            "| train_LensResnet_tune_checkpoint_83150_00004 | RUNNING    | 172.28.0.2:1357 |          1e-05  |          128 | 1.07941 | 0.586545 |                    1 |\n",
            "| train_LensResnet_tune_checkpoint_83150_00005 | PENDING    |                 |          0.0001 |           64 |         |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00006 | PENDING    |                 |          0.0001 |           32 |         |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00007 | PENDING    |                 |          1e-05  |           32 |         |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00008 | PENDING    |                 |          1e-05  |           32 |         |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00009 | PENDING    |                 |          0.0001 |          128 |         |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00010 | PENDING    |                 |          0.0001 |           32 |         |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00011 | PENDING    |                 |          0.01   |           32 |         |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00000 | TERMINATED |                 |          0.01   |           64 | 9.62847 | 0.673103 |                    3 |\n",
            "| train_LensResnet_tune_checkpoint_83150_00001 | TERMINATED |                 |          0.001  |           64 | 2.21447 | 0.849113 |                    3 |\n",
            "| train_LensResnet_tune_checkpoint_83150_00002 | TERMINATED |                 |          0.001  |          128 | 2.06535 | 0.805194 |                    3 |\n",
            "| train_LensResnet_tune_checkpoint_83150_00003 | TERMINATED |                 |          0.01   |           32 | 1.10137 | 0.495535 |                    1 |\n",
            "+----------------------------------------------+------------+-----------------+-----------------+--------------+---------+----------+----------------------+\n",
            "\n",
            "\n",
            "Epoch 0: 100%|██████████| 587/587 [08:44<00:00,  1.12it/s, loss=1.13, v_num=.]\n",
            "                                                             \u001b[A\n",
            "Epoch 1:   0%|          | 0/587 [00:00<?, ?it/s, loss=1.13, v_num=.]\n",
            "Epoch 1:   3%|▎         | 20/587 [00:19<09:04,  1.04it/s, loss=1.13, v_num=.]\n",
            "Epoch 1:   7%|▋         | 40/587 [00:38<08:43,  1.05it/s, loss=1.12, v_num=.]\n",
            "Epoch 1:  10%|█         | 60/587 [00:57<08:24,  1.04it/s, loss=1.13, v_num=.]\n",
            "Epoch 1:  14%|█▎        | 80/587 [01:16<08:04,  1.05it/s, loss=1.12, v_num=.]\n",
            "Epoch 1:  17%|█▋        | 100/587 [01:35<07:43,  1.05it/s, loss=1.11, v_num=.]\n",
            "Epoch 1:  20%|██        | 120/587 [01:53<07:23,  1.05it/s, loss=1.11, v_num=.]\n",
            "Epoch 1:  24%|██▍       | 140/587 [02:12<07:01,  1.06it/s, loss=1.11, v_num=.]\n",
            "Epoch 1:  27%|██▋       | 160/587 [02:30<06:41,  1.06it/s, loss=1.11, v_num=.]\n",
            "Epoch 1:  31%|███       | 180/587 [02:48<06:21,  1.07it/s, loss=1.1, v_num=.] \n",
            "Epoch 1:  34%|███▍      | 200/587 [03:06<06:01,  1.07it/s, loss=1.11, v_num=.]\n",
            "Epoch 1:  37%|███▋      | 220/587 [03:24<05:41,  1.07it/s, loss=1.11, v_num=.]\n",
            "Epoch 1:  41%|████      | 240/587 [03:42<05:22,  1.08it/s, loss=1.11, v_num=.]\n",
            "Epoch 1:  44%|████▍     | 260/587 [04:00<05:03,  1.08it/s, loss=1.11, v_num=.]\n",
            "Epoch 1:  48%|████▊     | 280/587 [04:19<04:44,  1.08it/s, loss=1.09, v_num=.]\n",
            "Epoch 1:  51%|█████     | 300/587 [04:37<04:25,  1.08it/s, loss=1.1, v_num=.] \n",
            "Epoch 1:  55%|█████▍    | 320/587 [04:55<04:06,  1.08it/s, loss=1.1, v_num=.]\n",
            "Epoch 1:  58%|█████▊    | 340/587 [05:13<03:47,  1.09it/s, loss=1.1, v_num=.]\n",
            "Epoch 1:  61%|██████▏   | 360/587 [05:31<03:28,  1.09it/s, loss=1.08, v_num=.]\n",
            "Epoch 1:  65%|██████▍   | 380/587 [05:49<03:10,  1.09it/s, loss=1.09, v_num=.]\n",
            "Epoch 1:  68%|██████▊   | 400/587 [06:07<02:51,  1.09it/s, loss=1.08, v_num=.]\n",
            "Epoch 1:  72%|███████▏  | 420/587 [06:25<02:33,  1.09it/s, loss=1.08, v_num=.]\n",
            "Epoch 1:  75%|███████▍  | 440/587 [06:43<02:14,  1.09it/s, loss=1.07, v_num=.]\n",
            "Epoch 1:  78%|███████▊  | 460/587 [07:01<01:56,  1.09it/s, loss=1.06, v_num=.]\n",
            "Epoch 1:  82%|████████▏ | 480/587 [07:09<01:35,  1.12it/s, loss=1.06, v_num=.]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/118 [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[2m\u001b[36m(pid=1357)\u001b[0m \n",
            "Epoch 1:  85%|████████▌ | 500/587 [07:22<01:17,  1.13it/s, loss=1.06, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1357)\u001b[0m \n",
            "Epoch 1:  89%|████████▊ | 520/587 [07:40<00:59,  1.13it/s, loss=1.06, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1357)\u001b[0m \n",
            "Epoch 1:  92%|█████████▏| 540/587 [07:58<00:41,  1.13it/s, loss=1.06, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1357)\u001b[0m \n",
            "Epoch 1:  95%|█████████▌| 560/587 [08:16<00:23,  1.13it/s, loss=1.06, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1357)\u001b[0m \n",
            "Epoch 1:  99%|█████████▉| 580/587 [08:34<00:06,  1.13it/s, loss=1.06, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1357)\u001b[0m \n",
            "Epoch 1: 100%|██████████| 587/587 [08:49<00:00,  1.11it/s, loss=1.06, v_num=.]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1357)\u001b[0m /usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/deprecated_api.py:152: LightningDeprecationWarning: `Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\n",
            "\u001b[2m\u001b[36m(pid=1357)\u001b[0m   \"`Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for train_LensResnet_tune_checkpoint_83150_00004:\n",
            "  auroc: 0.6422694325447083\n",
            "  date: 2021-07-08_06-27-32\n",
            "  done: true\n",
            "  experiment_id: 478ced1d31b442d68df9fefb9ffdca01\n",
            "  hostname: 0b93bd12a956\n",
            "  iterations_since_restore: 2\n",
            "  loss: 1.0285533666610718\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 1357\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 1068.1362552642822\n",
            "  time_this_iter_s: 532.6595180034637\n",
            "  time_total_s: 1068.1362552642822\n",
            "  timestamp: 1625725652\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 2\n",
            "  trial_id: '83150_00004'\n",
            "  \n",
            "== Status ==\n",
            "Memory usage on this node: 4.2/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=5\n",
            "Bracket: Iter 2.000: 0.6787027716636658 | Iter 1.000: 0.5439504981040955\n",
            "Resources requested: 2.0/2 CPUs, 1.0/1 GPUs, 0.0/7.48 GiB heap, 0.0/3.74 GiB objects (0.0/2.0 CPU_group_7647334c6aa32c88cd055440d9a148be, 0.0/1.0 GPU_group_7647334c6aa32c88cd055440d9a148be, 0.0/1.0 GPU_group_0_7647334c6aa32c88cd055440d9a148be, 0.0/2.0 CPU_group_0_7647334c6aa32c88cd055440d9a148be, 0.0/1.0 accelerator_type:T4)\n",
            "Current best trial: 83150_00001 with auroc=0.8491126894950867 and parameters={'learning_rate': 0.001, 'batch_size': 64}\n",
            "Result logdir: /content/drive/MyDrive/Logs/LensResNet_F\n",
            "Number of trials: 12/12 (7 PENDING, 1 RUNNING, 4 TERMINATED)\n",
            "+----------------------------------------------+------------+-----------------+-----------------+--------------+---------+----------+----------------------+\n",
            "| Trial name                                   | status     | loc             |   learning_rate |   batch_size |    loss |    auroc |   training_iteration |\n",
            "|----------------------------------------------+------------+-----------------+-----------------+--------------+---------+----------+----------------------|\n",
            "| train_LensResnet_tune_checkpoint_83150_00004 | RUNNING    | 172.28.0.2:1357 |          1e-05  |          128 | 1.02855 | 0.642269 |                    2 |\n",
            "| train_LensResnet_tune_checkpoint_83150_00005 | PENDING    |                 |          0.0001 |           64 |         |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00006 | PENDING    |                 |          0.0001 |           32 |         |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00007 | PENDING    |                 |          1e-05  |           32 |         |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00008 | PENDING    |                 |          1e-05  |           32 |         |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00009 | PENDING    |                 |          0.0001 |          128 |         |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00010 | PENDING    |                 |          0.0001 |           32 |         |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00011 | PENDING    |                 |          0.01   |           32 |         |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00000 | TERMINATED |                 |          0.01   |           64 | 9.62847 | 0.673103 |                    3 |\n",
            "| train_LensResnet_tune_checkpoint_83150_00001 | TERMINATED |                 |          0.001  |           64 | 2.21447 | 0.849113 |                    3 |\n",
            "| train_LensResnet_tune_checkpoint_83150_00002 | TERMINATED |                 |          0.001  |          128 | 2.06535 | 0.805194 |                    3 |\n",
            "| train_LensResnet_tune_checkpoint_83150_00003 | TERMINATED |                 |          0.01   |           32 | 1.10137 | 0.495535 |                    1 |\n",
            "+----------------------------------------------+------------+-----------------+-----------------+--------------+---------+----------+----------------------+\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[33m(raylet)\u001b[0m /usr/local/lib/python3.7/dist-packages/ray/autoscaler/_private/cli_logger.py:61: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
            "\u001b[2m\u001b[33m(raylet)\u001b[0m   \"update your install command.\", FutureWarning)\n",
            "\u001b[2m\u001b[36m(pid=1549)\u001b[0m GPU available: True, used: True\n",
            "\u001b[2m\u001b[36m(pid=1549)\u001b[0m TPU available: False, using: 0 TPU cores\n",
            "\u001b[2m\u001b[36m(pid=1549)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\u001b[2m\u001b[36m(pid=1549)\u001b[0m 2021-07-08 06:27:50.999165: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "\u001b[2m\u001b[36m(pid=1549)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1549)\u001b[0m   | Name     | Type   | Params\n",
            "\u001b[2m\u001b[36m(pid=1549)\u001b[0m ------------------------------------\n",
            "\u001b[2m\u001b[36m(pid=1549)\u001b[0m 0 | backbone | ResNet | 11.2 M\n",
            "\u001b[2m\u001b[36m(pid=1549)\u001b[0m ------------------------------------\n",
            "\u001b[2m\u001b[36m(pid=1549)\u001b[0m 11.2 M    Trainable params\n",
            "\u001b[2m\u001b[36m(pid=1549)\u001b[0m 0         Non-trainable params\n",
            "\u001b[2m\u001b[36m(pid=1549)\u001b[0m 11.2 M    Total params\n",
            "\u001b[2m\u001b[36m(pid=1549)\u001b[0m 44.687    Total estimated model params size (MB)\n",
            "\u001b[2m\u001b[36m(pid=1549)\u001b[0m /usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/data_loading.py:349: UserWarning: Your val_dataloader has `shuffle=True`, it is best practice to turn this off for val/test/predict dataloaders.\n",
            "\u001b[2m\u001b[36m(pid=1549)\u001b[0m   f'Your {mode}_dataloader has `shuffle=True`, it is best practice to turn'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1549)\u001b[0m \rTraining: 0it [00:00, ?it/s]\rTraining:   0%|          | 0/1173 [00:00<?, ?it/s]\rEpoch 0:   0%|          | 0/1173 [00:00<?, ?it/s] \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1549)\u001b[0m /usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "\u001b[2m\u001b[36m(pid=1549)\u001b[0m   return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0:   2%|▏         | 20/1173 [00:08<07:49,  2.46it/s, loss=1.17, v_num=.]\n",
            "Epoch 0:   3%|▎         | 40/1173 [00:16<07:37,  2.47it/s, loss=1.18, v_num=.]\n",
            "Epoch 0:   5%|▌         | 60/1173 [00:24<07:28,  2.48it/s, loss=1.19, v_num=.]\n",
            "Epoch 0:   7%|▋         | 80/1173 [00:32<07:19,  2.49it/s, loss=1.16, v_num=.]\n",
            "Epoch 0:   9%|▊         | 100/1173 [00:40<07:13,  2.48it/s, loss=1.15, v_num=.]\n",
            "Epoch 0:  10%|█         | 120/1173 [00:48<07:09,  2.45it/s, loss=1.15, v_num=.]\n",
            "Epoch 0:  12%|█▏        | 140/1173 [00:57<07:04,  2.43it/s, loss=1.14, v_num=.]\n",
            "Epoch 0:  14%|█▎        | 160/1173 [01:06<07:01,  2.40it/s, loss=1.14, v_num=.]\n",
            "Epoch 0:  15%|█▌        | 180/1173 [01:15<06:58,  2.38it/s, loss=1.14, v_num=.]\n",
            "Epoch 0:  17%|█▋        | 200/1173 [01:25<06:54,  2.35it/s, loss=1.16, v_num=.]\n",
            "Epoch 0:  19%|█▉        | 220/1173 [01:34<06:50,  2.32it/s, loss=1.13, v_num=.]\n",
            "Epoch 0:  20%|██        | 240/1173 [01:44<06:45,  2.30it/s, loss=1.15, v_num=.]\n",
            "Epoch 0:  22%|██▏       | 260/1173 [01:53<06:39,  2.29it/s, loss=1.13, v_num=.]\n",
            "Epoch 0:  24%|██▍       | 280/1173 [02:03<06:33,  2.27it/s, loss=1.12, v_num=.]\n",
            "Epoch 0:  26%|██▌       | 300/1173 [02:12<06:26,  2.26it/s, loss=1.14, v_num=.]\n",
            "Epoch 0:  27%|██▋       | 320/1173 [02:22<06:19,  2.25it/s, loss=1.12, v_num=.]\n",
            "Epoch 0:  29%|██▉       | 340/1173 [02:31<06:11,  2.25it/s, loss=1.12, v_num=.]\n",
            "Epoch 0:  31%|███       | 360/1173 [02:40<06:02,  2.24it/s, loss=1.12, v_num=.]\n",
            "Epoch 0:  32%|███▏      | 380/1173 [02:49<05:54,  2.24it/s, loss=1.1, v_num=.] \n",
            "Epoch 0:  34%|███▍      | 400/1173 [02:59<05:46,  2.23it/s, loss=1.11, v_num=.]\n",
            "Epoch 0:  36%|███▌      | 420/1173 [03:08<05:37,  2.23it/s, loss=1.13, v_num=.]\n",
            "Epoch 0:  38%|███▊      | 440/1173 [03:17<05:29,  2.23it/s, loss=1.1, v_num=.] \n",
            "Epoch 0:  39%|███▉      | 460/1173 [03:26<05:20,  2.22it/s, loss=1.1, v_num=.]\n",
            "Epoch 0:  41%|████      | 480/1173 [03:36<05:11,  2.22it/s, loss=1.11, v_num=.]\n",
            "Epoch 0:  43%|████▎     | 500/1173 [03:45<05:03,  2.22it/s, loss=1.09, v_num=.]\n",
            "Epoch 0:  44%|████▍     | 520/1173 [03:54<04:54,  2.22it/s, loss=1.09, v_num=.]\n",
            "Epoch 0:  46%|████▌     | 540/1173 [04:03<04:45,  2.22it/s, loss=1.1, v_num=.] \n",
            "Epoch 0:  48%|████▊     | 560/1173 [04:12<04:36,  2.21it/s, loss=1.08, v_num=.]\n",
            "Epoch 0:  49%|████▉     | 580/1173 [04:22<04:27,  2.21it/s, loss=1.08, v_num=.]\n",
            "Epoch 0:  51%|█████     | 600/1173 [04:31<04:18,  2.21it/s, loss=1.06, v_num=.]\n",
            "Epoch 0:  53%|█████▎    | 620/1173 [04:40<04:10,  2.21it/s, loss=1.05, v_num=.]\n",
            "Epoch 0:  55%|█████▍    | 640/1173 [04:49<04:01,  2.21it/s, loss=1.08, v_num=.]\n",
            "Epoch 0:  56%|█████▋    | 660/1173 [04:58<03:52,  2.21it/s, loss=1.08, v_num=.]\n",
            "Epoch 0:  58%|█████▊    | 680/1173 [05:07<03:43,  2.21it/s, loss=1.06, v_num=.]\n",
            "Epoch 0:  60%|█████▉    | 700/1173 [05:16<03:34,  2.21it/s, loss=1.04, v_num=.]\n",
            "Epoch 0:  61%|██████▏   | 720/1173 [05:26<03:25,  2.21it/s, loss=1.05, v_num=.]\n",
            "Epoch 0:  63%|██████▎   | 740/1173 [05:35<03:16,  2.21it/s, loss=1.04, v_num=.]\n",
            "Epoch 0:  65%|██████▍   | 760/1173 [05:44<03:07,  2.21it/s, loss=1.04, v_num=.]\n",
            "Epoch 0:  66%|██████▋   | 780/1173 [05:53<02:58,  2.21it/s, loss=1.05, v_num=.]\n",
            "Epoch 0:  68%|██████▊   | 800/1173 [06:02<02:49,  2.21it/s, loss=1.01, v_num=.]\n",
            "Epoch 0:  70%|██████▉   | 820/1173 [06:11<02:40,  2.21it/s, loss=1.03, v_num=.]\n",
            "Epoch 0:  72%|███████▏  | 840/1173 [06:20<02:31,  2.21it/s, loss=1.02, v_num=.]\n",
            "Epoch 0:  73%|███████▎  | 860/1173 [06:30<02:21,  2.20it/s, loss=1.03, v_num=.]\n",
            "Epoch 0:  75%|███████▌  | 880/1173 [06:39<02:12,  2.20it/s, loss=1.03, v_num=.]\n",
            "Epoch 0:  77%|███████▋  | 900/1173 [06:48<02:03,  2.20it/s, loss=1.02, v_num=.]\n",
            "Epoch 0:  78%|███████▊  | 920/1173 [06:57<01:54,  2.20it/s, loss=0.999, v_num=.]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1549)\u001b[0m /usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/callback_hook.py:101: LightningDeprecationWarning: The signature of `Callback.on_train_epoch_end` has changed in v1.3. `outputs` parameter has been removed. Support for the old signature will be removed in v1.5\n",
            "\u001b[2m\u001b[36m(pid=1549)\u001b[0m   \"The signature of `Callback.on_train_epoch_end` has changed in v1.3.\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1549)\u001b[0m \rEpoch 0:  80%|████████  | 940/1173 [07:05<01:45,  2.21it/s, loss=0.999, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1549)\u001b[0m \rValidating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/235 [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[2m\u001b[36m(pid=1549)\u001b[0m \n",
            "Epoch 0:  82%|████████▏ | 960/1173 [07:11<01:35,  2.22it/s, loss=0.999, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1549)\u001b[0m \n",
            "Epoch 0:  84%|████████▎ | 980/1173 [07:18<01:26,  2.24it/s, loss=0.999, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1549)\u001b[0m \n",
            "Epoch 0:  85%|████████▌ | 1000/1173 [07:26<01:17,  2.24it/s, loss=0.999, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1549)\u001b[0m \n",
            "Epoch 0:  87%|████████▋ | 1020/1173 [07:35<01:08,  2.24it/s, loss=0.999, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1549)\u001b[0m \n",
            "Epoch 0:  89%|████████▊ | 1040/1173 [07:44<00:59,  2.24it/s, loss=0.999, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1549)\u001b[0m \n",
            "Epoch 0:  90%|█████████ | 1060/1173 [07:53<00:50,  2.24it/s, loss=0.999, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1549)\u001b[0m \n",
            "Epoch 0:  92%|█████████▏| 1080/1173 [08:01<00:41,  2.24it/s, loss=0.999, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1549)\u001b[0m \n",
            "Epoch 0:  94%|█████████▍| 1100/1173 [08:11<00:32,  2.24it/s, loss=0.999, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1549)\u001b[0m \n",
            "Epoch 0:  95%|█████████▌| 1120/1173 [08:19<00:23,  2.24it/s, loss=0.999, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1549)\u001b[0m \n",
            "Epoch 0:  97%|█████████▋| 1140/1173 [08:28<00:14,  2.24it/s, loss=0.999, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1549)\u001b[0m \n",
            "Epoch 0:  99%|█████████▉| 1160/1173 [08:37<00:05,  2.24it/s, loss=0.999, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1549)\u001b[0m \n",
            "Validating: 100%|██████████| 235/235 [01:38<00:00,  2.29it/s]\u001b[A\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1549)\u001b[0m /usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/deprecated_api.py:152: LightningDeprecationWarning: `Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\n",
            "\u001b[2m\u001b[36m(pid=1549)\u001b[0m   \"`Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for train_LensResnet_tune_checkpoint_83150_00005:\n",
            "  auroc: 0.685188353061676\n",
            "  date: 2021-07-08_06-36-41\n",
            "  done: false\n",
            "  experiment_id: 6b7b8f2f646b4f9eac9c418edeba319d\n",
            "  hostname: 0b93bd12a956\n",
            "  iterations_since_restore: 1\n",
            "  loss: 0.9885507822036743\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 1549\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 538.3944439888\n",
            "  time_this_iter_s: 538.3944439888\n",
            "  time_total_s: 538.3944439888\n",
            "  timestamp: 1625726201\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 1\n",
            "  trial_id: '83150_00005'\n",
            "  \n",
            "== Status ==\n",
            "Memory usage on this node: 4.2/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=5\n",
            "Bracket: Iter 2.000: 0.6787027716636658 | Iter 1.000: 0.5652478933334351\n",
            "Resources requested: 2.0/2 CPUs, 1.0/1 GPUs, 0.0/7.48 GiB heap, 0.0/3.74 GiB objects (0.0/2.0 CPU_group_0_7647334c6aa32c88cd055440d9a148be, 0.0/1.0 GPU_group_7647334c6aa32c88cd055440d9a148be, 0.0/1.0 GPU_group_0_7647334c6aa32c88cd055440d9a148be, 0.0/2.0 CPU_group_7647334c6aa32c88cd055440d9a148be, 0.0/1.0 accelerator_type:T4)\n",
            "Current best trial: 83150_00001 with auroc=0.8491126894950867 and parameters={'learning_rate': 0.001, 'batch_size': 64}\n",
            "Result logdir: /content/drive/MyDrive/Logs/LensResNet_F\n",
            "Number of trials: 12/12 (6 PENDING, 1 RUNNING, 5 TERMINATED)\n",
            "+----------------------------------------------+------------+-----------------+-----------------+--------------+----------+----------+----------------------+\n",
            "| Trial name                                   | status     | loc             |   learning_rate |   batch_size |     loss |    auroc |   training_iteration |\n",
            "|----------------------------------------------+------------+-----------------+-----------------+--------------+----------+----------+----------------------|\n",
            "| train_LensResnet_tune_checkpoint_83150_00005 | RUNNING    | 172.28.0.2:1549 |          0.0001 |           64 | 0.988551 | 0.685188 |                    1 |\n",
            "| train_LensResnet_tune_checkpoint_83150_00006 | PENDING    |                 |          0.0001 |           32 |          |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00007 | PENDING    |                 |          1e-05  |           32 |          |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00008 | PENDING    |                 |          1e-05  |           32 |          |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00009 | PENDING    |                 |          0.0001 |          128 |          |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00010 | PENDING    |                 |          0.0001 |           32 |          |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00011 | PENDING    |                 |          0.01   |           32 |          |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00000 | TERMINATED |                 |          0.01   |           64 | 9.62847  | 0.673103 |                    3 |\n",
            "| train_LensResnet_tune_checkpoint_83150_00001 | TERMINATED |                 |          0.001  |           64 | 2.21447  | 0.849113 |                    3 |\n",
            "| train_LensResnet_tune_checkpoint_83150_00002 | TERMINATED |                 |          0.001  |          128 | 2.06535  | 0.805194 |                    3 |\n",
            "| train_LensResnet_tune_checkpoint_83150_00003 | TERMINATED |                 |          0.01   |           32 | 1.10137  | 0.495535 |                    1 |\n",
            "| train_LensResnet_tune_checkpoint_83150_00004 | TERMINATED |                 |          1e-05  |          128 | 1.02855  | 0.642269 |                    2 |\n",
            "+----------------------------------------------+------------+-----------------+-----------------+--------------+----------+----------+----------------------+\n",
            "\n",
            "\n",
            "Epoch 0: 100%|██████████| 1173/1173 [08:46<00:00,  2.23it/s, loss=1.02, v_num=.] \n",
            "                                                             \u001b[A\n",
            "Epoch 1:   0%|          | 0/1173 [00:00<?, ?it/s, loss=1.02, v_num=.]\n",
            "Epoch 1:   2%|▏         | 20/1173 [00:09<09:21,  2.05it/s, loss=1, v_num=.]   \n",
            "Epoch 1:   3%|▎         | 40/1173 [00:19<09:02,  2.09it/s, loss=0.982, v_num=.]\n",
            "Epoch 1:   5%|▌         | 60/1173 [00:28<08:49,  2.10it/s, loss=0.985, v_num=.]\n",
            "Epoch 1:   7%|▋         | 80/1173 [00:37<08:35,  2.12it/s, loss=1, v_num=.]    \n",
            "Epoch 1:   9%|▊         | 100/1173 [00:47<08:24,  2.13it/s, loss=1.02, v_num=.]\n",
            "Epoch 1:  10%|█         | 120/1173 [00:56<08:14,  2.13it/s, loss=0.971, v_num=.]\n",
            "Epoch 1:  12%|█▏        | 140/1173 [01:05<08:03,  2.14it/s, loss=0.962, v_num=.]\n",
            "Epoch 1:  14%|█▎        | 160/1173 [01:14<07:53,  2.14it/s, loss=0.985, v_num=.]\n",
            "Epoch 1:  15%|█▌        | 180/1173 [01:24<07:43,  2.14it/s, loss=0.958, v_num=.]\n",
            "Epoch 1:  17%|█▋        | 200/1173 [01:33<07:34,  2.14it/s, loss=0.971, v_num=.]\n",
            "Epoch 1:  19%|█▉        | 220/1173 [01:42<07:24,  2.14it/s, loss=0.943, v_num=.]\n",
            "Epoch 1:  20%|██        | 240/1173 [01:51<07:15,  2.14it/s, loss=0.934, v_num=.]\n",
            "Epoch 1:  22%|██▏       | 260/1173 [02:01<07:05,  2.15it/s, loss=0.978, v_num=.]\n",
            "Epoch 1:  24%|██▍       | 280/1173 [02:10<06:55,  2.15it/s, loss=0.977, v_num=.]\n",
            "Epoch 1:  26%|██▌       | 300/1173 [02:19<06:46,  2.15it/s, loss=0.957, v_num=.]\n",
            "Epoch 1:  27%|██▋       | 320/1173 [02:28<06:36,  2.15it/s, loss=0.948, v_num=.]\n",
            "Epoch 1:  29%|██▉       | 340/1173 [02:38<06:27,  2.15it/s, loss=0.953, v_num=.]\n",
            "Epoch 1:  31%|███       | 360/1173 [02:47<06:18,  2.15it/s, loss=0.972, v_num=.]\n",
            "Epoch 1:  32%|███▏      | 380/1173 [02:56<06:09,  2.15it/s, loss=0.929, v_num=.]\n",
            "Epoch 1:  34%|███▍      | 400/1173 [03:06<05:59,  2.15it/s, loss=0.991, v_num=.]\n",
            "Epoch 1:  36%|███▌      | 420/1173 [03:15<05:50,  2.15it/s, loss=0.956, v_num=.]\n",
            "Epoch 1:  38%|███▊      | 440/1173 [03:24<05:40,  2.15it/s, loss=0.951, v_num=.]\n",
            "Epoch 1:  39%|███▉      | 460/1173 [03:33<05:31,  2.15it/s, loss=0.939, v_num=.]\n",
            "Epoch 1:  41%|████      | 480/1173 [03:42<05:21,  2.15it/s, loss=0.937, v_num=.]\n",
            "Epoch 1:  43%|████▎     | 500/1173 [03:52<05:12,  2.15it/s, loss=0.942, v_num=.]\n",
            "Epoch 1:  44%|████▍     | 520/1173 [04:01<05:03,  2.15it/s, loss=0.911, v_num=.]\n",
            "Epoch 1:  46%|████▌     | 540/1173 [04:10<04:54,  2.15it/s, loss=0.918, v_num=.]\n",
            "Epoch 1:  48%|████▊     | 560/1173 [04:20<04:44,  2.15it/s, loss=0.95, v_num=.] \n",
            "Epoch 1:  49%|████▉     | 580/1173 [04:29<04:35,  2.15it/s, loss=0.91, v_num=.]\n",
            "Epoch 1:  51%|█████     | 600/1173 [04:38<04:26,  2.15it/s, loss=0.902, v_num=.]\n",
            "Epoch 1:  53%|█████▎    | 620/1173 [04:48<04:16,  2.15it/s, loss=0.918, v_num=.]\n",
            "Epoch 1:  55%|█████▍    | 640/1173 [04:57<04:07,  2.15it/s, loss=0.931, v_num=.]\n",
            "Epoch 1:  56%|█████▋    | 660/1173 [05:06<03:58,  2.15it/s, loss=0.891, v_num=.]\n",
            "Epoch 1:  58%|█████▊    | 680/1173 [05:15<03:48,  2.15it/s, loss=0.933, v_num=.]\n",
            "Epoch 1:  60%|█████▉    | 700/1173 [05:25<03:39,  2.15it/s, loss=0.931, v_num=.]\n",
            "Epoch 1:  61%|██████▏   | 720/1173 [05:34<03:30,  2.15it/s, loss=0.919, v_num=.]\n",
            "Epoch 1:  63%|██████▎   | 740/1173 [05:43<03:21,  2.15it/s, loss=0.891, v_num=.]\n",
            "Epoch 1:  65%|██████▍   | 760/1173 [05:52<03:11,  2.15it/s, loss=0.865, v_num=.]\n",
            "Epoch 1:  66%|██████▋   | 780/1173 [06:02<03:02,  2.15it/s, loss=0.89, v_num=.] \n",
            "Epoch 1:  68%|██████▊   | 800/1173 [06:11<02:53,  2.15it/s, loss=0.903, v_num=.]\n",
            "Epoch 1:  70%|██████▉   | 820/1173 [06:20<02:43,  2.15it/s, loss=0.869, v_num=.]\n",
            "Epoch 1:  72%|███████▏  | 840/1173 [06:29<02:34,  2.15it/s, loss=0.865, v_num=.]\n",
            "Epoch 1:  73%|███████▎  | 860/1173 [06:39<02:25,  2.16it/s, loss=0.914, v_num=.]\n",
            "Epoch 1:  75%|███████▌  | 880/1173 [06:48<02:15,  2.16it/s, loss=0.869, v_num=.]\n",
            "Epoch 1:  77%|███████▋  | 900/1173 [06:57<02:06,  2.16it/s, loss=0.865, v_num=.]\n",
            "Epoch 1:  78%|███████▊  | 920/1173 [07:06<01:57,  2.16it/s, loss=0.897, v_num=.]\n",
            "Epoch 1:  80%|████████  | 940/1173 [07:14<01:47,  2.16it/s, loss=0.897, v_num=.]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/235 [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[2m\u001b[36m(pid=1549)\u001b[0m \n",
            "Epoch 1:  82%|████████▏ | 960/1173 [07:20<01:37,  2.18it/s, loss=0.897, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1549)\u001b[0m \n",
            "Epoch 1:  84%|████████▎ | 980/1173 [07:27<01:28,  2.19it/s, loss=0.897, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1549)\u001b[0m \n",
            "Epoch 1:  85%|████████▌ | 1000/1173 [07:33<01:18,  2.20it/s, loss=0.897, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1549)\u001b[0m \n",
            "Epoch 1:  87%|████████▋ | 1020/1173 [07:40<01:09,  2.22it/s, loss=0.897, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1549)\u001b[0m \n",
            "Epoch 1:  89%|████████▊ | 1040/1173 [07:46<00:59,  2.23it/s, loss=0.897, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1549)\u001b[0m \n",
            "Epoch 1:  90%|█████████ | 1060/1173 [07:55<00:50,  2.23it/s, loss=0.897, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1549)\u001b[0m \n",
            "Epoch 1:  92%|█████████▏| 1080/1173 [08:03<00:41,  2.23it/s, loss=0.897, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1549)\u001b[0m \n",
            "Epoch 1:  94%|█████████▍| 1100/1173 [08:12<00:32,  2.23it/s, loss=0.897, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1549)\u001b[0m \n",
            "Epoch 1:  95%|█████████▌| 1120/1173 [08:21<00:23,  2.23it/s, loss=0.897, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1549)\u001b[0m \n",
            "Epoch 1:  97%|█████████▋| 1140/1173 [08:30<00:14,  2.23it/s, loss=0.897, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1549)\u001b[0m \n",
            "Epoch 1:  99%|█████████▉| 1160/1173 [08:39<00:05,  2.23it/s, loss=0.897, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1549)\u001b[0m \n",
            "Validating: 100%|██████████| 235/235 [01:31<00:00,  2.33it/s]\u001b[A\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1549)\u001b[0m /usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/deprecated_api.py:152: LightningDeprecationWarning: `Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\n",
            "\u001b[2m\u001b[36m(pid=1549)\u001b[0m   \"`Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for train_LensResnet_tune_checkpoint_83150_00005:\n",
            "  auroc: 0.6825814247131348\n",
            "  date: 2021-07-08_06-45-30\n",
            "  done: false\n",
            "  experiment_id: 6b7b8f2f646b4f9eac9c418edeba319d\n",
            "  hostname: 0b93bd12a956\n",
            "  iterations_since_restore: 2\n",
            "  loss: 1.0902965068817139\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 1549\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 1067.6672236919403\n",
            "  time_this_iter_s: 529.2727797031403\n",
            "  time_total_s: 1067.6672236919403\n",
            "  timestamp: 1625726730\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 2\n",
            "  trial_id: '83150_00005'\n",
            "  \n",
            "== Status ==\n",
            "Memory usage on this node: 4.2/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=5\n",
            "Bracket: Iter 2.000: 0.6825814247131348 | Iter 1.000: 0.5652478933334351\n",
            "Resources requested: 2.0/2 CPUs, 1.0/1 GPUs, 0.0/7.48 GiB heap, 0.0/3.74 GiB objects (0.0/1.0 GPU_group_7647334c6aa32c88cd055440d9a148be, 0.0/1.0 accelerator_type:T4, 0.0/2.0 CPU_group_7647334c6aa32c88cd055440d9a148be, 0.0/2.0 CPU_group_0_7647334c6aa32c88cd055440d9a148be, 0.0/1.0 GPU_group_0_7647334c6aa32c88cd055440d9a148be)\n",
            "Current best trial: 83150_00001 with auroc=0.8491126894950867 and parameters={'learning_rate': 0.001, 'batch_size': 64}\n",
            "Result logdir: /content/drive/MyDrive/Logs/LensResNet_F\n",
            "Number of trials: 12/12 (6 PENDING, 1 RUNNING, 5 TERMINATED)\n",
            "+----------------------------------------------+------------+-----------------+-----------------+--------------+---------+----------+----------------------+\n",
            "| Trial name                                   | status     | loc             |   learning_rate |   batch_size |    loss |    auroc |   training_iteration |\n",
            "|----------------------------------------------+------------+-----------------+-----------------+--------------+---------+----------+----------------------|\n",
            "| train_LensResnet_tune_checkpoint_83150_00005 | RUNNING    | 172.28.0.2:1549 |          0.0001 |           64 | 1.0903  | 0.682581 |                    2 |\n",
            "| train_LensResnet_tune_checkpoint_83150_00006 | PENDING    |                 |          0.0001 |           32 |         |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00007 | PENDING    |                 |          1e-05  |           32 |         |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00008 | PENDING    |                 |          1e-05  |           32 |         |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00009 | PENDING    |                 |          0.0001 |          128 |         |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00010 | PENDING    |                 |          0.0001 |           32 |         |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00011 | PENDING    |                 |          0.01   |           32 |         |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00000 | TERMINATED |                 |          0.01   |           64 | 9.62847 | 0.673103 |                    3 |\n",
            "| train_LensResnet_tune_checkpoint_83150_00001 | TERMINATED |                 |          0.001  |           64 | 2.21447 | 0.849113 |                    3 |\n",
            "| train_LensResnet_tune_checkpoint_83150_00002 | TERMINATED |                 |          0.001  |          128 | 2.06535 | 0.805194 |                    3 |\n",
            "| train_LensResnet_tune_checkpoint_83150_00003 | TERMINATED |                 |          0.01   |           32 | 1.10137 | 0.495535 |                    1 |\n",
            "| train_LensResnet_tune_checkpoint_83150_00004 | TERMINATED |                 |          1e-05  |          128 | 1.02855 | 0.642269 |                    2 |\n",
            "+----------------------------------------------+------------+-----------------+-----------------+--------------+---------+----------+----------------------+\n",
            "\n",
            "\n",
            "Epoch 1: 100%|██████████| 1173/1173 [08:48<00:00,  2.22it/s, loss=0.913, v_num=.]\n",
            "                                                             \u001b[A\n",
            "Epoch 2:   0%|          | 0/1173 [00:00<?, ?it/s, loss=0.913, v_num=.]\n",
            "Epoch 2:   2%|▏         | 20/1173 [00:09<09:24,  2.04it/s, loss=0.88, v_num=.] \n",
            "Epoch 2:   3%|▎         | 40/1173 [00:19<09:03,  2.09it/s, loss=0.833, v_num=.]\n",
            "Epoch 2:   5%|▌         | 60/1173 [00:28<08:50,  2.10it/s, loss=0.893, v_num=.]\n",
            "Epoch 2:   7%|▋         | 80/1173 [00:37<08:38,  2.11it/s, loss=0.851, v_num=.]\n",
            "Epoch 2:   9%|▊         | 100/1173 [00:47<08:26,  2.12it/s, loss=0.841, v_num=.]\n",
            "Epoch 2:  10%|█         | 120/1173 [00:56<08:14,  2.13it/s, loss=0.824, v_num=.]\n",
            "Epoch 2:  12%|█▏        | 140/1173 [01:05<08:03,  2.13it/s, loss=0.853, v_num=.]\n",
            "Epoch 2:  14%|█▎        | 160/1173 [01:14<07:52,  2.14it/s, loss=0.801, v_num=.]\n",
            "Epoch 2:  15%|█▌        | 180/1173 [01:23<07:42,  2.15it/s, loss=0.848, v_num=.]\n",
            "Epoch 2:  17%|█▋        | 200/1173 [01:33<07:32,  2.15it/s, loss=0.896, v_num=.]\n",
            "Epoch 2:  19%|█▉        | 220/1173 [01:42<07:23,  2.15it/s, loss=0.859, v_num=.]\n",
            "Epoch 2:  20%|██        | 240/1173 [01:51<07:13,  2.15it/s, loss=0.838, v_num=.]\n",
            "Epoch 2:  22%|██▏       | 260/1173 [02:00<07:03,  2.16it/s, loss=0.835, v_num=.]\n",
            "Epoch 2:  24%|██▍       | 280/1173 [02:09<06:53,  2.16it/s, loss=0.833, v_num=.]\n",
            "Epoch 2:  26%|██▌       | 300/1173 [02:18<06:43,  2.16it/s, loss=0.859, v_num=.]\n",
            "Epoch 2:  27%|██▋       | 320/1173 [02:27<06:34,  2.16it/s, loss=0.838, v_num=.]\n",
            "Epoch 2:  29%|██▉       | 340/1173 [02:36<06:24,  2.17it/s, loss=0.82, v_num=.] \n",
            "Epoch 2:  31%|███       | 360/1173 [02:46<06:15,  2.17it/s, loss=0.804, v_num=.]\n",
            "Epoch 2:  32%|███▏      | 380/1173 [02:55<06:05,  2.17it/s, loss=0.793, v_num=.]\n",
            "Epoch 2:  34%|███▍      | 400/1173 [03:04<05:56,  2.17it/s, loss=0.833, v_num=.]\n",
            "Epoch 2:  36%|███▌      | 420/1173 [03:13<05:47,  2.17it/s, loss=0.805, v_num=.]\n",
            "Epoch 2:  38%|███▊      | 440/1173 [03:22<05:37,  2.17it/s, loss=0.824, v_num=.]\n",
            "Epoch 2:  39%|███▉      | 460/1173 [03:31<05:28,  2.17it/s, loss=0.771, v_num=.]\n",
            "Epoch 2:  41%|████      | 480/1173 [03:41<05:19,  2.17it/s, loss=0.791, v_num=.]\n",
            "Epoch 2:  43%|████▎     | 500/1173 [03:50<05:10,  2.17it/s, loss=0.766, v_num=.]\n",
            "Epoch 2:  44%|████▍     | 520/1173 [03:59<05:00,  2.17it/s, loss=0.805, v_num=.]\n",
            "Epoch 2:  46%|████▌     | 540/1173 [04:08<04:51,  2.17it/s, loss=0.76, v_num=.] \n",
            "Epoch 2:  48%|████▊     | 560/1173 [04:17<04:42,  2.17it/s, loss=0.766, v_num=.]\n",
            "Epoch 2:  49%|████▉     | 580/1173 [04:27<04:33,  2.17it/s, loss=0.795, v_num=.]\n",
            "Epoch 2:  51%|█████     | 600/1173 [04:36<04:23,  2.17it/s, loss=0.77, v_num=.] \n",
            "Epoch 2:  53%|█████▎    | 620/1173 [04:45<04:14,  2.17it/s, loss=0.775, v_num=.]\n",
            "Epoch 2:  55%|█████▍    | 640/1173 [04:54<04:05,  2.17it/s, loss=0.726, v_num=.]\n",
            "Epoch 2:  56%|█████▋    | 660/1173 [05:03<03:56,  2.17it/s, loss=0.754, v_num=.]\n",
            "Epoch 2:  58%|█████▊    | 680/1173 [05:13<03:46,  2.17it/s, loss=0.733, v_num=.]\n",
            "Epoch 2:  60%|█████▉    | 700/1173 [05:22<03:37,  2.17it/s, loss=0.757, v_num=.]\n",
            "Epoch 2:  61%|██████▏   | 720/1173 [05:31<03:28,  2.17it/s, loss=0.741, v_num=.]\n",
            "Epoch 2:  63%|██████▎   | 740/1173 [05:40<03:19,  2.17it/s, loss=0.702, v_num=.]\n",
            "Epoch 2:  65%|██████▍   | 760/1173 [05:49<03:10,  2.17it/s, loss=0.751, v_num=.]\n",
            "Epoch 2:  66%|██████▋   | 780/1173 [05:58<03:00,  2.17it/s, loss=0.735, v_num=.]\n",
            "Epoch 2:  68%|██████▊   | 800/1173 [06:08<02:51,  2.17it/s, loss=0.682, v_num=.]\n",
            "Epoch 2:  70%|██████▉   | 820/1173 [06:17<02:42,  2.17it/s, loss=0.701, v_num=.]\n",
            "Epoch 2:  72%|███████▏  | 840/1173 [06:26<02:33,  2.17it/s, loss=0.687, v_num=.]\n",
            "Epoch 2:  73%|███████▎  | 860/1173 [06:35<02:24,  2.17it/s, loss=0.689, v_num=.]\n",
            "Epoch 2:  75%|███████▌  | 880/1173 [06:44<02:14,  2.17it/s, loss=0.725, v_num=.]\n",
            "Epoch 2:  77%|███████▋  | 900/1173 [06:54<02:05,  2.17it/s, loss=0.713, v_num=.]\n",
            "Epoch 2:  78%|███████▊  | 920/1173 [07:03<01:56,  2.17it/s, loss=0.708, v_num=.]\n",
            "Epoch 2:  80%|████████  | 940/1173 [07:11<01:46,  2.18it/s, loss=0.708, v_num=.]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/235 [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[2m\u001b[36m(pid=1549)\u001b[0m \n",
            "Epoch 2:  82%|████████▏ | 960/1173 [07:17<01:37,  2.19it/s, loss=0.708, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1549)\u001b[0m \n",
            "Epoch 2:  84%|████████▎ | 980/1173 [07:23<01:27,  2.21it/s, loss=0.708, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1549)\u001b[0m \n",
            "Epoch 2:  85%|████████▌ | 1000/1173 [07:30<01:17,  2.22it/s, loss=0.708, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1549)\u001b[0m \n",
            "Epoch 2:  87%|████████▋ | 1020/1173 [07:36<01:08,  2.23it/s, loss=0.708, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1549)\u001b[0m \n",
            "Epoch 2:  89%|████████▊ | 1040/1173 [07:45<00:59,  2.23it/s, loss=0.708, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1549)\u001b[0m \n",
            "Epoch 2:  90%|█████████ | 1060/1173 [07:54<00:50,  2.23it/s, loss=0.708, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1549)\u001b[0m \n",
            "Epoch 2:  92%|█████████▏| 1080/1173 [08:03<00:41,  2.23it/s, loss=0.708, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1549)\u001b[0m \n",
            "Epoch 2:  94%|█████████▍| 1100/1173 [08:12<00:32,  2.24it/s, loss=0.708, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1549)\u001b[0m \n",
            "Epoch 2:  95%|█████████▌| 1120/1173 [08:21<00:23,  2.24it/s, loss=0.708, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1549)\u001b[0m \n",
            "Epoch 2:  97%|█████████▋| 1140/1173 [08:29<00:14,  2.24it/s, loss=0.708, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1549)\u001b[0m \n",
            "Epoch 2:  99%|█████████▉| 1160/1173 [08:38<00:05,  2.24it/s, loss=0.708, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1549)\u001b[0m \n",
            "Validating: 100%|██████████| 235/235 [01:34<00:00,  2.32it/s]\u001b[A\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1549)\u001b[0m /usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/deprecated_api.py:152: LightningDeprecationWarning: `Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\n",
            "\u001b[2m\u001b[36m(pid=1549)\u001b[0m   \"`Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for train_LensResnet_tune_checkpoint_83150_00005:\n",
            "  auroc: 0.8451821208000183\n",
            "  date: 2021-07-08_06-54-18\n",
            "  done: true\n",
            "  experiment_id: 6b7b8f2f646b4f9eac9c418edeba319d\n",
            "  hostname: 0b93bd12a956\n",
            "  iterations_since_restore: 3\n",
            "  loss: 0.8032200336456299\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 1549\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 1596.2367482185364\n",
            "  time_this_iter_s: 528.5695245265961\n",
            "  time_total_s: 1596.2367482185364\n",
            "  timestamp: 1625727258\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 3\n",
            "  trial_id: '83150_00005'\n",
            "  \n",
            "== Status ==\n",
            "Memory usage on this node: 4.3/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=6\n",
            "Bracket: Iter 2.000: 0.6825814247131348 | Iter 1.000: 0.5652478933334351\n",
            "Resources requested: 2.0/2 CPUs, 1.0/1 GPUs, 0.0/7.48 GiB heap, 0.0/3.74 GiB objects (0.0/1.0 GPU_group_7647334c6aa32c88cd055440d9a148be, 0.0/2.0 CPU_group_7647334c6aa32c88cd055440d9a148be, 0.0/1.0 accelerator_type:T4, 0.0/1.0 GPU_group_0_7647334c6aa32c88cd055440d9a148be, 0.0/2.0 CPU_group_0_7647334c6aa32c88cd055440d9a148be)\n",
            "Current best trial: 83150_00001 with auroc=0.8491126894950867 and parameters={'learning_rate': 0.001, 'batch_size': 64}\n",
            "Result logdir: /content/drive/MyDrive/Logs/LensResNet_F\n",
            "Number of trials: 12/12 (6 PENDING, 1 RUNNING, 5 TERMINATED)\n",
            "+----------------------------------------------+------------+-----------------+-----------------+--------------+---------+----------+----------------------+\n",
            "| Trial name                                   | status     | loc             |   learning_rate |   batch_size |    loss |    auroc |   training_iteration |\n",
            "|----------------------------------------------+------------+-----------------+-----------------+--------------+---------+----------+----------------------|\n",
            "| train_LensResnet_tune_checkpoint_83150_00005 | RUNNING    | 172.28.0.2:1549 |          0.0001 |           64 | 0.80322 | 0.845182 |                    3 |\n",
            "| train_LensResnet_tune_checkpoint_83150_00006 | PENDING    |                 |          0.0001 |           32 |         |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00007 | PENDING    |                 |          1e-05  |           32 |         |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00008 | PENDING    |                 |          1e-05  |           32 |         |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00009 | PENDING    |                 |          0.0001 |          128 |         |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00010 | PENDING    |                 |          0.0001 |           32 |         |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00011 | PENDING    |                 |          0.01   |           32 |         |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00000 | TERMINATED |                 |          0.01   |           64 | 9.62847 | 0.673103 |                    3 |\n",
            "| train_LensResnet_tune_checkpoint_83150_00001 | TERMINATED |                 |          0.001  |           64 | 2.21447 | 0.849113 |                    3 |\n",
            "| train_LensResnet_tune_checkpoint_83150_00002 | TERMINATED |                 |          0.001  |          128 | 2.06535 | 0.805194 |                    3 |\n",
            "| train_LensResnet_tune_checkpoint_83150_00003 | TERMINATED |                 |          0.01   |           32 | 1.10137 | 0.495535 |                    1 |\n",
            "| train_LensResnet_tune_checkpoint_83150_00004 | TERMINATED |                 |          1e-05  |          128 | 1.02855 | 0.642269 |                    2 |\n",
            "+----------------------------------------------+------------+-----------------+-----------------+--------------+---------+----------+----------------------+\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2021-07-08 06:54:19,984\tWARNING util.py:162 -- The `start_trial` operation took 0.516 s, which may be a performance bottleneck.\n",
            "\u001b[2m\u001b[33m(raylet)\u001b[0m /usr/local/lib/python3.7/dist-packages/ray/autoscaler/_private/cli_logger.py:61: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
            "\u001b[2m\u001b[33m(raylet)\u001b[0m   \"update your install command.\", FutureWarning)\n",
            "\u001b[2m\u001b[33m(raylet)\u001b[0m /usr/local/lib/python3.7/dist-packages/ray/autoscaler/_private/cli_logger.py:61: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
            "\u001b[2m\u001b[33m(raylet)\u001b[0m   \"update your install command.\", FutureWarning)\n",
            "\u001b[2m\u001b[36m(pid=1811)\u001b[0m GPU available: True, used: True\n",
            "\u001b[2m\u001b[36m(pid=1811)\u001b[0m TPU available: False, using: 0 TPU cores\n",
            "\u001b[2m\u001b[36m(pid=1811)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\u001b[2m\u001b[36m(pid=1811)\u001b[0m 2021-07-08 06:54:39.209082: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "\u001b[2m\u001b[36m(pid=1811)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=1811)\u001b[0m   | Name     | Type   | Params\n",
            "\u001b[2m\u001b[36m(pid=1811)\u001b[0m ------------------------------------\n",
            "\u001b[2m\u001b[36m(pid=1811)\u001b[0m 0 | backbone | ResNet | 11.2 M\n",
            "\u001b[2m\u001b[36m(pid=1811)\u001b[0m ------------------------------------\n",
            "\u001b[2m\u001b[36m(pid=1811)\u001b[0m 11.2 M    Trainable params\n",
            "\u001b[2m\u001b[36m(pid=1811)\u001b[0m 0         Non-trainable params\n",
            "\u001b[2m\u001b[36m(pid=1811)\u001b[0m 11.2 M    Total params\n",
            "\u001b[2m\u001b[36m(pid=1811)\u001b[0m 44.687    Total estimated model params size (MB)\n",
            "\u001b[2m\u001b[36m(pid=1811)\u001b[0m /usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/data_loading.py:349: UserWarning: Your val_dataloader has `shuffle=True`, it is best practice to turn this off for val/test/predict dataloaders.\n",
            "\u001b[2m\u001b[36m(pid=1811)\u001b[0m   f'Your {mode}_dataloader has `shuffle=True`, it is best practice to turn'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1811)\u001b[0m \rTraining: 0it [00:00, ?it/s]\rTraining:   0%|          | 0/2344 [00:00<?, ?it/s]\rEpoch 0:   0%|          | 0/2344 [00:00<?, ?it/s] \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1811)\u001b[0m /usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "\u001b[2m\u001b[36m(pid=1811)\u001b[0m   return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0:   1%|          | 20/2344 [00:04<08:04,  4.79it/s, loss=1.21, v_num=.]\n",
            "Epoch 0:   2%|▏         | 40/2344 [00:08<07:51,  4.89it/s, loss=1.2, v_num=.] \n",
            "Epoch 0:   3%|▎         | 60/2344 [00:11<07:36,  5.01it/s, loss=1.18, v_num=.]\n",
            "Epoch 0:   3%|▎         | 80/2344 [00:15<07:26,  5.07it/s, loss=1.19, v_num=.]\n",
            "Epoch 0:   4%|▍         | 100/2344 [00:19<07:23,  5.06it/s, loss=1.21, v_num=.]\n",
            "Epoch 0:   5%|▌         | 120/2344 [00:23<07:20,  5.05it/s, loss=1.19, v_num=.]\n",
            "Epoch 0:   6%|▌         | 140/2344 [00:27<07:17,  5.04it/s, loss=1.17, v_num=.]\n",
            "Epoch 0:   7%|▋         | 160/2344 [00:31<07:13,  5.04it/s, loss=1.17, v_num=.]\n",
            "Epoch 0:   8%|▊         | 180/2344 [00:35<07:12,  5.01it/s, loss=1.15, v_num=.]\n",
            "Epoch 0:   9%|▊         | 200/2344 [00:39<07:08,  5.01it/s, loss=1.16, v_num=.]\n",
            "Epoch 0:   9%|▉         | 220/2344 [00:43<07:04,  5.01it/s, loss=1.15, v_num=.]\n",
            "Epoch 0:  10%|█         | 240/2344 [00:47<07:00,  5.01it/s, loss=1.16, v_num=.]\n",
            "Epoch 0:  11%|█         | 260/2344 [00:51<06:56,  5.00it/s, loss=1.14, v_num=.]\n",
            "Epoch 0:  12%|█▏        | 280/2344 [00:56<06:54,  4.98it/s, loss=1.16, v_num=.]\n",
            "Epoch 0:  13%|█▎        | 300/2344 [01:00<06:53,  4.95it/s, loss=1.16, v_num=.]\n",
            "Epoch 0:  14%|█▎        | 320/2344 [01:05<06:52,  4.91it/s, loss=1.15, v_num=.]\n",
            "Epoch 0:  15%|█▍        | 340/2344 [01:10<06:54,  4.84it/s, loss=1.15, v_num=.]\n",
            "Epoch 0:  15%|█▌        | 360/2344 [01:15<06:53,  4.80it/s, loss=1.13, v_num=.]\n",
            "Epoch 0:  16%|█▌        | 380/2344 [01:19<06:52,  4.76it/s, loss=1.18, v_num=.]\n",
            "Epoch 0:  17%|█▋        | 400/2344 [01:24<06:50,  4.73it/s, loss=1.15, v_num=.]\n",
            "Epoch 0:  18%|█▊        | 420/2344 [01:29<06:49,  4.70it/s, loss=1.13, v_num=.]\n",
            "Epoch 0:  19%|█▉        | 440/2344 [01:34<06:47,  4.67it/s, loss=1.13, v_num=.]\n",
            "Epoch 0:  20%|█▉        | 460/2344 [01:39<06:45,  4.65it/s, loss=1.15, v_num=.]\n",
            "Epoch 0:  20%|██        | 480/2344 [01:43<06:43,  4.62it/s, loss=1.1, v_num=.] \n",
            "Epoch 0:  21%|██▏       | 500/2344 [01:48<06:41,  4.59it/s, loss=1.14, v_num=.]\n",
            "Epoch 0:  22%|██▏       | 520/2344 [01:53<06:38,  4.58it/s, loss=1.15, v_num=.]\n",
            "Epoch 0:  23%|██▎       | 540/2344 [01:58<06:35,  4.56it/s, loss=1.13, v_num=.]\n",
            "Epoch 0:  24%|██▍       | 560/2344 [02:02<06:31,  4.55it/s, loss=1.11, v_num=.]\n",
            "Epoch 0:  25%|██▍       | 580/2344 [02:07<06:28,  4.54it/s, loss=1.14, v_num=.]\n",
            "Epoch 0:  26%|██▌       | 600/2344 [02:12<06:24,  4.53it/s, loss=1.11, v_num=.]\n",
            "Epoch 0:  26%|██▋       | 620/2344 [02:17<06:20,  4.53it/s, loss=1.13, v_num=.]\n",
            "Epoch 0:  27%|██▋       | 640/2344 [02:21<06:17,  4.52it/s, loss=1.1, v_num=.] \n",
            "Epoch 0:  28%|██▊       | 660/2344 [02:26<06:13,  4.51it/s, loss=1.13, v_num=.]\n",
            "Epoch 0:  29%|██▉       | 680/2344 [02:30<06:08,  4.51it/s, loss=1.11, v_num=.]\n",
            "Epoch 0:  30%|██▉       | 700/2344 [02:35<06:04,  4.51it/s, loss=1.12, v_num=.]\n",
            "Epoch 0:  31%|███       | 720/2344 [02:39<06:00,  4.50it/s, loss=1.1, v_num=.] \n",
            "Epoch 0:  32%|███▏      | 740/2344 [02:44<05:56,  4.49it/s, loss=1.11, v_num=.]\n",
            "Epoch 0:  32%|███▏      | 760/2344 [02:49<05:52,  4.49it/s, loss=1.11, v_num=.]\n",
            "Epoch 0:  33%|███▎      | 780/2344 [02:53<05:48,  4.49it/s, loss=1.12, v_num=.]\n",
            "Epoch 0:  34%|███▍      | 800/2344 [02:58<05:44,  4.48it/s, loss=1.1, v_num=.] \n",
            "Epoch 0:  35%|███▍      | 820/2344 [03:03<05:40,  4.48it/s, loss=1.14, v_num=.]\n",
            "Epoch 0:  36%|███▌      | 840/2344 [03:07<05:35,  4.48it/s, loss=1.11, v_num=.]\n",
            "Epoch 0:  37%|███▋      | 860/2344 [03:12<05:31,  4.48it/s, loss=1.11, v_num=.]\n",
            "Epoch 0:  38%|███▊      | 880/2344 [03:16<05:27,  4.48it/s, loss=1.12, v_num=.]\n",
            "Epoch 0:  38%|███▊      | 900/2344 [03:21<05:22,  4.47it/s, loss=1.12, v_num=.]\n",
            "Epoch 0:  39%|███▉      | 920/2344 [03:25<05:18,  4.47it/s, loss=1.1, v_num=.] \n",
            "Epoch 0:  40%|████      | 940/2344 [03:30<05:14,  4.47it/s, loss=1.08, v_num=.]\n",
            "Epoch 0:  41%|████      | 960/2344 [03:34<05:09,  4.47it/s, loss=1.13, v_num=.]\n",
            "Epoch 0:  42%|████▏     | 980/2344 [03:39<05:05,  4.47it/s, loss=1.11, v_num=.]\n",
            "Epoch 0:  43%|████▎     | 1000/2344 [03:43<05:00,  4.47it/s, loss=1.08, v_num=.]\n",
            "Epoch 0:  44%|████▎     | 1020/2344 [03:48<04:56,  4.46it/s, loss=1.11, v_num=.]\n",
            "Epoch 0:  44%|████▍     | 1040/2344 [03:53<04:52,  4.46it/s, loss=1.08, v_num=.]\n",
            "Epoch 0:  45%|████▌     | 1060/2344 [03:57<04:47,  4.46it/s, loss=1.1, v_num=.] \n",
            "Epoch 0:  46%|████▌     | 1080/2344 [04:02<04:43,  4.46it/s, loss=1.08, v_num=.]\n",
            "Epoch 0:  47%|████▋     | 1100/2344 [04:06<04:39,  4.46it/s, loss=1.12, v_num=.]\n",
            "Epoch 0:  48%|████▊     | 1120/2344 [04:11<04:34,  4.46it/s, loss=1.11, v_num=.]\n",
            "Epoch 0:  49%|████▊     | 1140/2344 [04:15<04:30,  4.45it/s, loss=1.09, v_num=.]\n",
            "Epoch 0:  49%|████▉     | 1160/2344 [04:20<04:25,  4.45it/s, loss=1.08, v_num=.]\n",
            "Epoch 0:  50%|█████     | 1180/2344 [04:25<04:21,  4.45it/s, loss=1.12, v_num=.]\n",
            "Epoch 0:  51%|█████     | 1200/2344 [04:29<04:16,  4.45it/s, loss=1.09, v_num=.]\n",
            "Epoch 0:  52%|█████▏    | 1220/2344 [04:34<04:12,  4.45it/s, loss=1.09, v_num=.]\n",
            "Epoch 0:  52%|█████▏    | 1220/2344 [04:34<04:12,  4.45it/s, loss=1.09, v_num=.]\n",
            "Epoch 0:  53%|█████▎    | 1240/2344 [04:38<04:08,  4.45it/s, loss=1.06, v_num=.]\n",
            "Epoch 0:  54%|█████▍    | 1260/2344 [04:43<04:03,  4.45it/s, loss=1.08, v_num=.]\n",
            "Epoch 0:  55%|█████▍    | 1280/2344 [04:47<03:59,  4.45it/s, loss=1.07, v_num=.]\n",
            "Epoch 0:  55%|█████▌    | 1300/2344 [04:52<03:54,  4.45it/s, loss=1.09, v_num=.]\n",
            "Epoch 0:  56%|█████▋    | 1320/2344 [04:56<03:50,  4.45it/s, loss=1.07, v_num=.]\n",
            "Epoch 0:  57%|█████▋    | 1340/2344 [05:01<03:45,  4.45it/s, loss=1.06, v_num=.]\n",
            "Epoch 0:  58%|█████▊    | 1360/2344 [05:05<03:41,  4.45it/s, loss=1.1, v_num=.] \n",
            "Epoch 0:  59%|█████▉    | 1380/2344 [05:10<03:36,  4.45it/s, loss=1.09, v_num=.]\n",
            "Epoch 0:  60%|█████▉    | 1400/2344 [05:14<03:32,  4.45it/s, loss=1.09, v_num=.]\n",
            "Epoch 0:  61%|██████    | 1420/2344 [05:19<03:27,  4.45it/s, loss=1.04, v_num=.]\n",
            "Epoch 0:  61%|██████▏   | 1440/2344 [05:23<03:23,  4.45it/s, loss=1.03, v_num=.]\n",
            "Epoch 0:  62%|██████▏   | 1460/2344 [05:28<03:18,  4.45it/s, loss=1.04, v_num=.]\n",
            "Epoch 0:  63%|██████▎   | 1480/2344 [05:32<03:14,  4.45it/s, loss=1.05, v_num=.]\n",
            "Epoch 0:  64%|██████▍   | 1500/2344 [05:37<03:09,  4.45it/s, loss=1.06, v_num=.]\n",
            "Epoch 0:  65%|██████▍   | 1520/2344 [05:41<03:05,  4.45it/s, loss=1.08, v_num=.]\n",
            "Epoch 0:  66%|██████▌   | 1540/2344 [05:46<03:00,  4.45it/s, loss=1.02, v_num=.]\n",
            "Epoch 0:  67%|██████▋   | 1560/2344 [05:50<02:56,  4.45it/s, loss=1.04, v_num=.]\n",
            "Epoch 0:  67%|██████▋   | 1580/2344 [05:55<02:51,  4.45it/s, loss=1.08, v_num=.]\n",
            "Epoch 0:  68%|██████▊   | 1600/2344 [05:59<02:47,  4.45it/s, loss=1.04, v_num=.]\n",
            "Epoch 0:  69%|██████▉   | 1620/2344 [06:04<02:42,  4.45it/s, loss=1, v_num=.]   \n",
            "Epoch 0:  70%|██████▉   | 1640/2344 [06:08<02:38,  4.45it/s, loss=1.07, v_num=.]\n",
            "Epoch 0:  71%|███████   | 1660/2344 [06:13<02:33,  4.45it/s, loss=1.03, v_num=.]\n",
            "Epoch 0:  72%|███████▏  | 1680/2344 [06:17<02:29,  4.45it/s, loss=1.04, v_num=.]\n",
            "Epoch 0:  73%|███████▎  | 1700/2344 [06:22<02:24,  4.45it/s, loss=1.01, v_num=.]\n",
            "Epoch 0:  73%|███████▎  | 1720/2344 [06:26<02:20,  4.45it/s, loss=1.05, v_num=.]\n",
            "Epoch 0:  74%|███████▍  | 1740/2344 [06:31<02:15,  4.45it/s, loss=1.03, v_num=.]\n",
            "Epoch 0:  75%|███████▌  | 1760/2344 [06:35<02:11,  4.45it/s, loss=1.02, v_num=.]\n",
            "Epoch 0:  76%|███████▌  | 1780/2344 [06:40<02:06,  4.45it/s, loss=0.998, v_num=.]\n",
            "Epoch 0:  77%|███████▋  | 1800/2344 [06:44<02:02,  4.45it/s, loss=1.03, v_num=.] \n",
            "Epoch 0:  78%|███████▊  | 1820/2344 [06:49<01:57,  4.45it/s, loss=1.03, v_num=.]\n",
            "Epoch 0:  78%|███████▊  | 1840/2344 [06:53<01:53,  4.45it/s, loss=1.05, v_num=.]\n",
            "Epoch 0:  79%|███████▉  | 1860/2344 [06:58<01:48,  4.45it/s, loss=1.01, v_num=.]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1811)\u001b[0m /usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/callback_hook.py:101: LightningDeprecationWarning: The signature of `Callback.on_train_epoch_end` has changed in v1.3. `outputs` parameter has been removed. Support for the old signature will be removed in v1.5\n",
            "\u001b[2m\u001b[36m(pid=1811)\u001b[0m   \"The signature of `Callback.on_train_epoch_end` has changed in v1.3.\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1811)\u001b[0m \rEpoch 0:  80%|████████  | 1880/2344 [07:01<01:44,  4.46it/s, loss=1.01, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1811)\u001b[0m \rValidating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/469 [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[2m\u001b[36m(pid=1811)\u001b[0m \n",
            "Epoch 0:  81%|████████  | 1900/2344 [07:05<01:39,  4.47it/s, loss=1.01, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1811)\u001b[0m \n",
            "Epoch 0:  82%|████████▏ | 1920/2344 [07:10<01:34,  4.46it/s, loss=1.01, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1811)\u001b[0m \n",
            "Epoch 0:  83%|████████▎ | 1940/2344 [07:14<01:30,  4.47it/s, loss=1.01, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1811)\u001b[0m \n",
            "Epoch 0:  84%|████████▎ | 1960/2344 [07:18<01:26,  4.47it/s, loss=1.01, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1811)\u001b[0m \n",
            "Epoch 0:  84%|████████▍ | 1980/2344 [07:23<01:21,  4.47it/s, loss=1.01, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1811)\u001b[0m \n",
            "Epoch 0:  85%|████████▌ | 2000/2344 [07:27<01:17,  4.47it/s, loss=1.01, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1811)\u001b[0m \n",
            "Epoch 0:  86%|████████▌ | 2020/2344 [07:32<01:12,  4.47it/s, loss=1.01, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1811)\u001b[0m \n",
            "Epoch 0:  87%|████████▋ | 2040/2344 [07:36<01:08,  4.47it/s, loss=1.01, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1811)\u001b[0m \n",
            "Epoch 0:  88%|████████▊ | 2060/2344 [07:41<01:03,  4.47it/s, loss=1.01, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1811)\u001b[0m \n",
            "Epoch 0:  89%|████████▊ | 2080/2344 [07:45<00:59,  4.47it/s, loss=1.01, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1811)\u001b[0m \n",
            "Epoch 0:  90%|████████▉ | 2100/2344 [07:50<00:54,  4.47it/s, loss=1.01, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1811)\u001b[0m \n",
            "Epoch 0:  90%|█████████ | 2120/2344 [07:54<00:50,  4.47it/s, loss=1.01, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1811)\u001b[0m \n",
            "Epoch 0:  91%|█████████▏| 2140/2344 [07:59<00:45,  4.47it/s, loss=1.01, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1811)\u001b[0m \n",
            "Epoch 0:  92%|█████████▏| 2160/2344 [08:03<00:41,  4.47it/s, loss=1.01, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1811)\u001b[0m \n",
            "Epoch 0:  93%|█████████▎| 2180/2344 [08:08<00:36,  4.47it/s, loss=1.01, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1811)\u001b[0m \n",
            "Epoch 0:  94%|█████████▍| 2200/2344 [08:12<00:32,  4.47it/s, loss=1.01, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1811)\u001b[0m \n",
            "Epoch 0:  95%|█████████▍| 2220/2344 [08:16<00:27,  4.47it/s, loss=1.01, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1811)\u001b[0m \n",
            "Epoch 0:  96%|█████████▌| 2240/2344 [08:21<00:23,  4.47it/s, loss=1.01, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1811)\u001b[0m \n",
            "Epoch 0:  96%|█████████▋| 2260/2344 [08:25<00:18,  4.47it/s, loss=1.01, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1811)\u001b[0m \n",
            "Epoch 0:  97%|█████████▋| 2280/2344 [08:30<00:14,  4.47it/s, loss=1.01, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1811)\u001b[0m \n",
            "Epoch 0:  98%|█████████▊| 2300/2344 [08:34<00:09,  4.47it/s, loss=1.01, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1811)\u001b[0m \n",
            "Epoch 0:  99%|█████████▉| 2320/2344 [08:39<00:05,  4.47it/s, loss=1.01, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1811)\u001b[0m \n",
            "Epoch 0: 100%|█████████▉| 2340/2344 [08:43<00:00,  4.47it/s, loss=1.01, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1811)\u001b[0m \n",
            "Validating: 100%|██████████| 469/469 [01:44<00:00,  4.47it/s]\u001b[A\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1811)\u001b[0m /usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/deprecated_api.py:152: LightningDeprecationWarning: `Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\n",
            "\u001b[2m\u001b[36m(pid=1811)\u001b[0m   \"`Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for train_LensResnet_tune_checkpoint_83150_00006:\n",
            "  auroc: 0.6666943430900574\n",
            "  date: 2021-07-08_07-03-31\n",
            "  done: false\n",
            "  experiment_id: a314a9233c754abab55f490af4e6b86d\n",
            "  hostname: 0b93bd12a956\n",
            "  iterations_since_restore: 1\n",
            "  loss: 1.012158989906311\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 1811\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 542.5436894893646\n",
            "  time_this_iter_s: 542.5436894893646\n",
            "  time_total_s: 542.5436894893646\n",
            "  timestamp: 1625727811\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 1\n",
            "  trial_id: '83150_00006'\n",
            "  \n",
            "== Status ==\n",
            "Memory usage on this node: 4.3/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=6\n",
            "Bracket: Iter 2.000: 0.6825814247131348 | Iter 1.000: 0.5865452885627747\n",
            "Resources requested: 2.0/2 CPUs, 1.0/1 GPUs, 0.0/7.48 GiB heap, 0.0/3.74 GiB objects (0.0/2.0 CPU_group_7647334c6aa32c88cd055440d9a148be, 0.0/1.0 GPU_group_0_7647334c6aa32c88cd055440d9a148be, 0.0/1.0 accelerator_type:T4, 0.0/1.0 GPU_group_7647334c6aa32c88cd055440d9a148be, 0.0/2.0 CPU_group_0_7647334c6aa32c88cd055440d9a148be)\n",
            "Current best trial: 83150_00001 with auroc=0.8491126894950867 and parameters={'learning_rate': 0.001, 'batch_size': 64}\n",
            "Result logdir: /content/drive/MyDrive/Logs/LensResNet_F\n",
            "Number of trials: 12/12 (5 PENDING, 1 RUNNING, 6 TERMINATED)\n",
            "+----------------------------------------------+------------+-----------------+-----------------+--------------+---------+----------+----------------------+\n",
            "| Trial name                                   | status     | loc             |   learning_rate |   batch_size |    loss |    auroc |   training_iteration |\n",
            "|----------------------------------------------+------------+-----------------+-----------------+--------------+---------+----------+----------------------|\n",
            "| train_LensResnet_tune_checkpoint_83150_00006 | RUNNING    | 172.28.0.2:1811 |          0.0001 |           32 | 1.01216 | 0.666694 |                    1 |\n",
            "| train_LensResnet_tune_checkpoint_83150_00007 | PENDING    |                 |          1e-05  |           32 |         |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00008 | PENDING    |                 |          1e-05  |           32 |         |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00009 | PENDING    |                 |          0.0001 |          128 |         |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00010 | PENDING    |                 |          0.0001 |           32 |         |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00011 | PENDING    |                 |          0.01   |           32 |         |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00000 | TERMINATED |                 |          0.01   |           64 | 9.62847 | 0.673103 |                    3 |\n",
            "| train_LensResnet_tune_checkpoint_83150_00001 | TERMINATED |                 |          0.001  |           64 | 2.21447 | 0.849113 |                    3 |\n",
            "| train_LensResnet_tune_checkpoint_83150_00002 | TERMINATED |                 |          0.001  |          128 | 2.06535 | 0.805194 |                    3 |\n",
            "| train_LensResnet_tune_checkpoint_83150_00003 | TERMINATED |                 |          0.01   |           32 | 1.10137 | 0.495535 |                    1 |\n",
            "| train_LensResnet_tune_checkpoint_83150_00004 | TERMINATED |                 |          1e-05  |          128 | 1.02855 | 0.642269 |                    2 |\n",
            "| train_LensResnet_tune_checkpoint_83150_00005 | TERMINATED |                 |          0.0001 |           64 | 0.80322 | 0.845182 |                    3 |\n",
            "+----------------------------------------------+------------+-----------------+-----------------+--------------+---------+----------+----------------------+\n",
            "\n",
            "\n",
            "Epoch 0: 100%|██████████| 2344/2344 [08:48<00:00,  4.43it/s, loss=1.02, v_num=.]\n",
            "                                                             \u001b[A\n",
            "Epoch 1:   0%|          | 0/2344 [00:00<?, ?it/s, loss=1.02, v_num=.]\n",
            "Epoch 1:   1%|          | 20/2344 [00:04<08:12,  4.72it/s, loss=0.96, v_num=.]\n",
            "Epoch 1:   2%|▏         | 40/2344 [00:08<08:05,  4.75it/s, loss=0.979, v_num=.]\n",
            "Epoch 1:   3%|▎         | 60/2344 [00:12<07:56,  4.79it/s, loss=1.02, v_num=.] \n",
            "Epoch 1:   3%|▎         | 80/2344 [00:17<08:01,  4.70it/s, loss=0.97, v_num=.]\n",
            "Epoch 1:   4%|▍         | 100/2344 [00:21<08:09,  4.59it/s, loss=0.973, v_num=.]\n",
            "Epoch 1:   5%|▌         | 120/2344 [00:26<08:11,  4.52it/s, loss=0.97, v_num=.] \n",
            "Epoch 1:   6%|▌         | 140/2344 [00:31<08:13,  4.46it/s, loss=0.977, v_num=.]\n",
            "Epoch 1:   7%|▋         | 160/2344 [00:36<08:12,  4.43it/s, loss=0.975, v_num=.]\n",
            "Epoch 1:   8%|▊         | 180/2344 [00:40<08:10,  4.41it/s, loss=0.949, v_num=.]\n",
            "Epoch 1:   9%|▊         | 200/2344 [00:45<08:08,  4.39it/s, loss=0.968, v_num=.]\n",
            "Epoch 1:   9%|▉         | 220/2344 [00:50<08:06,  4.36it/s, loss=0.944, v_num=.]\n",
            "Epoch 1:  10%|█         | 240/2344 [00:55<08:02,  4.36it/s, loss=0.995, v_num=.]\n",
            "Epoch 1:  11%|█         | 260/2344 [00:59<07:57,  4.36it/s, loss=0.998, v_num=.]\n",
            "Epoch 1:  12%|█▏        | 280/2344 [01:04<07:53,  4.36it/s, loss=1.01, v_num=.] \n",
            "Epoch 1:  13%|█▎        | 300/2344 [01:08<07:50,  4.35it/s, loss=0.959, v_num=.]\n",
            "Epoch 1:  14%|█▎        | 320/2344 [01:13<07:45,  4.34it/s, loss=0.979, v_num=.]\n",
            "Epoch 1:  15%|█▍        | 340/2344 [01:18<07:41,  4.34it/s, loss=0.946, v_num=.]\n",
            "Epoch 1:  15%|█▌        | 360/2344 [01:23<07:37,  4.34it/s, loss=0.932, v_num=.]\n",
            "Epoch 1:  16%|█▌        | 380/2344 [01:27<07:33,  4.33it/s, loss=0.922, v_num=.]\n",
            "Epoch 1:  17%|█▋        | 400/2344 [01:32<07:29,  4.33it/s, loss=0.927, v_num=.]\n",
            "Epoch 1:  18%|█▊        | 420/2344 [01:37<07:24,  4.32it/s, loss=0.954, v_num=.]\n",
            "Epoch 1:  19%|█▉        | 440/2344 [01:41<07:20,  4.32it/s, loss=0.934, v_num=.]\n",
            "Epoch 1:  20%|█▉        | 460/2344 [01:46<07:15,  4.32it/s, loss=0.931, v_num=.]\n",
            "Epoch 1:  20%|██        | 480/2344 [01:51<07:11,  4.32it/s, loss=0.941, v_num=.]\n",
            "Epoch 1:  21%|██▏       | 500/2344 [01:55<07:06,  4.33it/s, loss=0.929, v_num=.]\n",
            "Epoch 1:  22%|██▏       | 520/2344 [02:00<07:01,  4.33it/s, loss=0.899, v_num=.]\n",
            "Epoch 1:  23%|██▎       | 540/2344 [02:04<06:56,  4.33it/s, loss=0.915, v_num=.]\n",
            "Epoch 1:  24%|██▍       | 560/2344 [02:09<06:51,  4.33it/s, loss=0.979, v_num=.]\n",
            "Epoch 1:  25%|██▍       | 580/2344 [02:13<06:46,  4.33it/s, loss=0.935, v_num=.]\n",
            "Epoch 1:  26%|██▌       | 600/2344 [02:18<06:42,  4.33it/s, loss=0.909, v_num=.]\n",
            "Epoch 1:  26%|██▋       | 620/2344 [02:22<06:37,  4.34it/s, loss=0.903, v_num=.]\n",
            "Epoch 1:  27%|██▋       | 640/2344 [02:27<06:32,  4.34it/s, loss=0.927, v_num=.]\n",
            "Epoch 1:  28%|██▊       | 660/2344 [02:31<06:27,  4.34it/s, loss=0.894, v_num=.]\n",
            "Epoch 1:  29%|██▉       | 680/2344 [02:36<06:22,  4.34it/s, loss=0.899, v_num=.]\n",
            "Epoch 1:  30%|██▉       | 700/2344 [02:41<06:18,  4.35it/s, loss=0.935, v_num=.]\n",
            "Epoch 1:  31%|███       | 720/2344 [02:45<06:13,  4.35it/s, loss=0.892, v_num=.]\n",
            "Epoch 1:  32%|███▏      | 740/2344 [02:50<06:08,  4.35it/s, loss=0.911, v_num=.]\n",
            "Epoch 1:  32%|███▏      | 760/2344 [02:54<06:03,  4.35it/s, loss=0.882, v_num=.]\n",
            "Epoch 1:  33%|███▎      | 780/2344 [02:59<05:59,  4.35it/s, loss=0.883, v_num=.]\n",
            "Epoch 1:  34%|███▍      | 800/2344 [03:03<05:54,  4.36it/s, loss=0.893, v_num=.]\n",
            "Epoch 1:  35%|███▍      | 820/2344 [03:07<05:49,  4.36it/s, loss=0.924, v_num=.]\n",
            "Epoch 1:  36%|███▌      | 840/2344 [03:12<05:44,  4.37it/s, loss=0.87, v_num=.] \n",
            "Epoch 1:  37%|███▋      | 860/2344 [03:17<05:40,  4.36it/s, loss=0.886, v_num=.]\n",
            "Epoch 1:  38%|███▊      | 880/2344 [03:21<05:35,  4.37it/s, loss=0.88, v_num=.] \n",
            "Epoch 1:  38%|███▊      | 900/2344 [03:26<05:30,  4.36it/s, loss=0.842, v_num=.]\n",
            "Epoch 1:  39%|███▉      | 920/2344 [03:30<05:26,  4.37it/s, loss=0.833, v_num=.]\n",
            "Epoch 1:  40%|████      | 940/2344 [03:35<05:21,  4.37it/s, loss=0.861, v_num=.]\n",
            "Epoch 1:  41%|████      | 960/2344 [03:39<05:16,  4.37it/s, loss=0.878, v_num=.]\n",
            "Epoch 1:  42%|████▏     | 980/2344 [03:44<05:12,  4.37it/s, loss=0.878, v_num=.]\n",
            "Epoch 1:  43%|████▎     | 1000/2344 [03:48<05:07,  4.37it/s, loss=0.859, v_num=.]\n",
            "Epoch 1:  44%|████▎     | 1020/2344 [03:53<05:02,  4.37it/s, loss=0.834, v_num=.]\n",
            "Epoch 1:  44%|████▍     | 1040/2344 [03:57<04:58,  4.37it/s, loss=0.854, v_num=.]\n",
            "Epoch 1:  45%|████▌     | 1060/2344 [04:02<04:53,  4.37it/s, loss=0.854, v_num=.]\n",
            "Epoch 1:  46%|████▌     | 1080/2344 [04:06<04:48,  4.38it/s, loss=0.832, v_num=.]\n",
            "Epoch 1:  47%|████▋     | 1100/2344 [04:11<04:44,  4.38it/s, loss=0.804, v_num=.]\n",
            "Epoch 1:  48%|████▊     | 1120/2344 [04:15<04:39,  4.38it/s, loss=0.804, v_num=.]\n",
            "Epoch 1:  49%|████▊     | 1140/2344 [04:20<04:34,  4.38it/s, loss=0.846, v_num=.]\n",
            "Epoch 1:  49%|████▉     | 1160/2344 [04:24<04:29,  4.39it/s, loss=0.866, v_num=.]\n",
            "Epoch 1:  50%|█████     | 1180/2344 [04:28<04:24,  4.39it/s, loss=0.836, v_num=.]\n",
            "Epoch 1:  51%|█████     | 1200/2344 [04:33<04:20,  4.39it/s, loss=0.908, v_num=.]\n",
            "Epoch 1:  52%|█████▏    | 1220/2344 [04:37<04:15,  4.40it/s, loss=0.783, v_num=.]\n",
            "Epoch 1:  53%|█████▎    | 1240/2344 [04:42<04:11,  4.40it/s, loss=0.768, v_num=.]\n",
            "Epoch 1:  54%|█████▍    | 1260/2344 [04:46<04:06,  4.40it/s, loss=0.803, v_num=.]\n",
            "Epoch 1:  55%|█████▍    | 1280/2344 [04:50<04:01,  4.40it/s, loss=0.792, v_num=.]\n",
            "Epoch 1:  55%|█████▌    | 1300/2344 [04:55<03:56,  4.41it/s, loss=0.789, v_num=.]\n",
            "Epoch 1:  56%|█████▋    | 1320/2344 [04:59<03:52,  4.40it/s, loss=0.823, v_num=.]\n",
            "Epoch 1:  57%|█████▋    | 1340/2344 [05:04<03:47,  4.41it/s, loss=0.848, v_num=.]\n",
            "Epoch 1:  58%|█████▊    | 1360/2344 [05:08<03:43,  4.41it/s, loss=0.753, v_num=.]\n",
            "Epoch 1:  59%|█████▉    | 1380/2344 [05:13<03:38,  4.41it/s, loss=0.765, v_num=.]\n",
            "Epoch 1:  60%|█████▉    | 1400/2344 [05:17<03:34,  4.41it/s, loss=0.768, v_num=.]\n",
            "Epoch 1:  61%|██████    | 1420/2344 [05:21<03:29,  4.41it/s, loss=0.798, v_num=.]\n",
            "Epoch 1:  61%|██████▏   | 1440/2344 [05:26<03:24,  4.41it/s, loss=0.752, v_num=.]\n",
            "Epoch 1:  62%|██████▏   | 1460/2344 [05:30<03:20,  4.41it/s, loss=0.759, v_num=.]\n",
            "Epoch 1:  63%|██████▎   | 1480/2344 [05:35<03:15,  4.41it/s, loss=0.807, v_num=.]\n",
            "Epoch 1:  64%|██████▍   | 1500/2344 [05:39<03:11,  4.41it/s, loss=0.785, v_num=.]\n",
            "Epoch 1:  65%|██████▍   | 1520/2344 [05:44<03:06,  4.41it/s, loss=0.764, v_num=.]\n",
            "Epoch 1:  66%|██████▌   | 1540/2344 [05:48<03:02,  4.41it/s, loss=0.761, v_num=.]\n",
            "Epoch 1:  67%|██████▋   | 1560/2344 [05:53<02:57,  4.42it/s, loss=0.779, v_num=.]\n",
            "Epoch 1:  67%|██████▋   | 1580/2344 [05:57<02:52,  4.42it/s, loss=0.738, v_num=.]\n",
            "Epoch 1:  68%|██████▊   | 1600/2344 [06:02<02:48,  4.42it/s, loss=0.759, v_num=.]\n",
            "Epoch 1:  69%|██████▉   | 1620/2344 [06:06<02:43,  4.42it/s, loss=0.738, v_num=.]\n",
            "Epoch 1:  70%|██████▉   | 1640/2344 [06:11<02:39,  4.42it/s, loss=0.779, v_num=.]\n",
            "Epoch 1:  71%|███████   | 1660/2344 [06:15<02:34,  4.42it/s, loss=0.759, v_num=.]\n",
            "Epoch 1:  72%|███████▏  | 1680/2344 [06:19<02:30,  4.42it/s, loss=0.813, v_num=.]\n",
            "Epoch 1:  73%|███████▎  | 1700/2344 [06:24<02:25,  4.42it/s, loss=0.761, v_num=.]\n",
            "Epoch 1:  73%|███████▎  | 1720/2344 [06:28<02:21,  4.42it/s, loss=0.736, v_num=.]\n",
            "Epoch 1:  74%|███████▍  | 1740/2344 [06:33<02:16,  4.42it/s, loss=0.741, v_num=.]\n",
            "Epoch 1:  75%|███████▌  | 1760/2344 [06:37<02:12,  4.42it/s, loss=0.735, v_num=.]\n",
            "Epoch 1:  76%|███████▌  | 1780/2344 [06:42<02:07,  4.42it/s, loss=0.75, v_num=.] \n",
            "Epoch 1:  77%|███████▋  | 1800/2344 [06:46<02:02,  4.43it/s, loss=0.715, v_num=.]\n",
            "Epoch 1:  78%|███████▊  | 1820/2344 [06:51<01:58,  4.43it/s, loss=0.689, v_num=.]\n",
            "Epoch 1:  78%|███████▊  | 1840/2344 [06:55<01:53,  4.43it/s, loss=0.733, v_num=.]\n",
            "Epoch 1:  79%|███████▉  | 1860/2344 [07:00<01:49,  4.43it/s, loss=0.793, v_num=.]\n",
            "Epoch 1:  80%|████████  | 1880/2344 [07:03<01:44,  4.44it/s, loss=0.793, v_num=.]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/469 [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[2m\u001b[36m(pid=1811)\u001b[0m \n",
            "Epoch 1:  81%|████████  | 1900/2344 [07:07<01:39,  4.44it/s, loss=0.793, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1811)\u001b[0m \n",
            "Epoch 1:  82%|████████▏ | 1920/2344 [07:11<01:35,  4.45it/s, loss=0.793, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1811)\u001b[0m \n",
            "Epoch 1:  83%|████████▎ | 1940/2344 [07:16<01:30,  4.45it/s, loss=0.793, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1811)\u001b[0m \n",
            "Epoch 1:  84%|████████▎ | 1960/2344 [07:20<01:26,  4.45it/s, loss=0.793, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1811)\u001b[0m \n",
            "Epoch 1:  84%|████████▍ | 1980/2344 [07:25<01:21,  4.45it/s, loss=0.793, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1811)\u001b[0m \n",
            "Epoch 1:  85%|████████▌ | 2000/2344 [07:29<01:17,  4.45it/s, loss=0.793, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1811)\u001b[0m \n",
            "Epoch 1:  86%|████████▌ | 2020/2344 [07:34<01:12,  4.45it/s, loss=0.793, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1811)\u001b[0m \n",
            "Epoch 1:  87%|████████▋ | 2040/2344 [07:38<01:08,  4.45it/s, loss=0.793, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1811)\u001b[0m \n",
            "Epoch 1:  88%|████████▊ | 2060/2344 [07:43<01:03,  4.45it/s, loss=0.793, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1811)\u001b[0m \n",
            "Epoch 1:  89%|████████▊ | 2080/2344 [07:47<00:59,  4.45it/s, loss=0.793, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1811)\u001b[0m \n",
            "Epoch 1:  90%|████████▉ | 2100/2344 [07:52<00:54,  4.45it/s, loss=0.793, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1811)\u001b[0m \n",
            "Epoch 1:  90%|█████████ | 2120/2344 [07:56<00:50,  4.45it/s, loss=0.793, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1811)\u001b[0m \n",
            "Epoch 1:  91%|█████████▏| 2140/2344 [08:01<00:45,  4.45it/s, loss=0.793, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1811)\u001b[0m \n",
            "Epoch 1:  92%|█████████▏| 2160/2344 [08:05<00:41,  4.45it/s, loss=0.793, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1811)\u001b[0m \n",
            "Epoch 1:  93%|█████████▎| 2180/2344 [08:10<00:36,  4.45it/s, loss=0.793, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1811)\u001b[0m \n",
            "Epoch 1:  94%|█████████▍| 2200/2344 [08:14<00:32,  4.45it/s, loss=0.793, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1811)\u001b[0m \n",
            "Epoch 1:  95%|█████████▍| 2220/2344 [08:18<00:27,  4.45it/s, loss=0.793, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1811)\u001b[0m \n",
            "Epoch 1:  96%|█████████▌| 2240/2344 [08:23<00:23,  4.45it/s, loss=0.793, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1811)\u001b[0m \n",
            "Epoch 1:  96%|█████████▋| 2260/2344 [08:27<00:18,  4.45it/s, loss=0.793, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1811)\u001b[0m \n",
            "Epoch 1:  97%|█████████▋| 2280/2344 [08:32<00:14,  4.45it/s, loss=0.793, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1811)\u001b[0m \n",
            "Epoch 1:  98%|█████████▊| 2300/2344 [08:36<00:09,  4.45it/s, loss=0.793, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1811)\u001b[0m \n",
            "Epoch 1:  99%|█████████▉| 2320/2344 [08:40<00:05,  4.46it/s, loss=0.793, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1811)\u001b[0m \n",
            "Epoch 1: 100%|█████████▉| 2340/2344 [08:45<00:00,  4.46it/s, loss=0.793, v_num=.]\n",
            "\u001b[2m\u001b[36m(pid=1811)\u001b[0m \n",
            "Validating: 100%|██████████| 469/469 [01:43<00:00,  4.53it/s]\u001b[A\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1811)\u001b[0m /usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/deprecated_api.py:152: LightningDeprecationWarning: `Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\n",
            "\u001b[2m\u001b[36m(pid=1811)\u001b[0m   \"`Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for train_LensResnet_tune_checkpoint_83150_00006:\n",
            "  auroc: 0.8242172598838806\n",
            "  date: 2021-07-08_07-12-21\n",
            "  done: false\n",
            "  experiment_id: a314a9233c754abab55f490af4e6b86d\n",
            "  hostname: 0b93bd12a956\n",
            "  iterations_since_restore: 2\n",
            "  loss: 0.8677229285240173\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 1811\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 1073.1987454891205\n",
            "  time_this_iter_s: 530.6550559997559\n",
            "  time_total_s: 1073.1987454891205\n",
            "  timestamp: 1625728341\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 2\n",
            "  trial_id: '83150_00006'\n",
            "  \n",
            "== Status ==\n",
            "Memory usage on this node: 4.3/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=6\n",
            "Bracket: Iter 2.000: 0.698858767747879 | Iter 1.000: 0.5865452885627747\n",
            "Resources requested: 2.0/2 CPUs, 1.0/1 GPUs, 0.0/7.48 GiB heap, 0.0/3.74 GiB objects (0.0/1.0 GPU_group_0_7647334c6aa32c88cd055440d9a148be, 0.0/2.0 CPU_group_7647334c6aa32c88cd055440d9a148be, 0.0/2.0 CPU_group_0_7647334c6aa32c88cd055440d9a148be, 0.0/1.0 GPU_group_7647334c6aa32c88cd055440d9a148be, 0.0/1.0 accelerator_type:T4)\n",
            "Current best trial: 83150_00001 with auroc=0.8491126894950867 and parameters={'learning_rate': 0.001, 'batch_size': 64}\n",
            "Result logdir: /content/drive/MyDrive/Logs/LensResNet_F\n",
            "Number of trials: 12/12 (5 PENDING, 1 RUNNING, 6 TERMINATED)\n",
            "+----------------------------------------------+------------+-----------------+-----------------+--------------+----------+----------+----------------------+\n",
            "| Trial name                                   | status     | loc             |   learning_rate |   batch_size |     loss |    auroc |   training_iteration |\n",
            "|----------------------------------------------+------------+-----------------+-----------------+--------------+----------+----------+----------------------|\n",
            "| train_LensResnet_tune_checkpoint_83150_00006 | RUNNING    | 172.28.0.2:1811 |          0.0001 |           32 | 0.867723 | 0.824217 |                    2 |\n",
            "| train_LensResnet_tune_checkpoint_83150_00007 | PENDING    |                 |          1e-05  |           32 |          |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00008 | PENDING    |                 |          1e-05  |           32 |          |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00009 | PENDING    |                 |          0.0001 |          128 |          |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00010 | PENDING    |                 |          0.0001 |           32 |          |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00011 | PENDING    |                 |          0.01   |           32 |          |          |                      |\n",
            "| train_LensResnet_tune_checkpoint_83150_00000 | TERMINATED |                 |          0.01   |           64 | 9.62847  | 0.673103 |                    3 |\n",
            "| train_LensResnet_tune_checkpoint_83150_00001 | TERMINATED |                 |          0.001  |           64 | 2.21447  | 0.849113 |                    3 |\n",
            "| train_LensResnet_tune_checkpoint_83150_00002 | TERMINATED |                 |          0.001  |          128 | 2.06535  | 0.805194 |                    3 |\n",
            "| train_LensResnet_tune_checkpoint_83150_00003 | TERMINATED |                 |          0.01   |           32 | 1.10137  | 0.495535 |                    1 |\n",
            "| train_LensResnet_tune_checkpoint_83150_00004 | TERMINATED |                 |          1e-05  |          128 | 1.02855  | 0.642269 |                    2 |\n",
            "| train_LensResnet_tune_checkpoint_83150_00005 | TERMINATED |                 |          0.0001 |           64 | 0.80322  | 0.845182 |                    3 |\n",
            "+----------------------------------------------+------------+-----------------+-----------------+--------------+----------+----------+----------------------+\n",
            "\n",
            "\n",
            "Epoch 1: 100%|██████████| 2344/2344 [08:49<00:00,  4.43it/s, loss=0.732, v_num=.]\n",
            "                                                             \u001b[A\n",
            "Epoch 2:   0%|          | 0/2344 [00:00<?, ?it/s, loss=0.732, v_num=.]\n",
            "Epoch 2:   1%|          | 20/2344 [00:04<08:08,  4.76it/s, loss=0.752, v_num=.]\n",
            "Epoch 2:   2%|▏         | 40/2344 [00:08<07:58,  4.81it/s, loss=0.712, v_num=.]\n",
            "Epoch 2:   3%|▎         | 60/2344 [00:12<07:48,  4.88it/s, loss=0.713, v_num=.]\n",
            "Epoch 2:   3%|▎         | 80/2344 [00:16<07:56,  4.75it/s, loss=0.707, v_num=.]\n",
            "Epoch 2:   4%|▍         | 100/2344 [00:21<08:04,  4.63it/s, loss=0.734, v_num=.]\n",
            "Epoch 2:   5%|▌         | 120/2344 [00:26<08:08,  4.56it/s, loss=0.741, v_num=.]\n",
            "Epoch 2:   6%|▌         | 140/2344 [00:31<08:11,  4.48it/s, loss=0.727, v_num=.]\n",
            "Epoch 2:   7%|▋         | 160/2344 [00:36<08:13,  4.43it/s, loss=0.693, v_num=.]\n",
            "Epoch 2:   8%|▊         | 180/2344 [00:40<08:11,  4.41it/s, loss=0.721, v_num=.]\n",
            "Epoch 2:   9%|▊         | 200/2344 [00:45<08:08,  4.39it/s, loss=0.712, v_num=.]\n",
            "Epoch 2:   9%|▉         | 220/2344 [00:50<08:04,  4.38it/s, loss=0.752, v_num=.]\n",
            "Epoch 2:  10%|█         | 240/2344 [00:54<08:00,  4.38it/s, loss=0.606, v_num=.]\n",
            "Epoch 2:  11%|█         | 260/2344 [00:59<07:56,  4.37it/s, loss=0.735, v_num=.]\n",
            "Epoch 2:  12%|█▏        | 280/2344 [01:04<07:52,  4.37it/s, loss=0.693, v_num=.]\n",
            "Epoch 2:  13%|█▎        | 300/2344 [01:08<07:48,  4.36it/s, loss=0.711, v_num=.]\n",
            "Epoch 2:  14%|█▎        | 320/2344 [01:13<07:45,  4.35it/s, loss=0.619, v_num=.]\n",
            "Epoch 2:  15%|█▍        | 340/2344 [01:18<07:40,  4.35it/s, loss=0.692, v_num=.]\n",
            "Epoch 2:  15%|█▌        | 360/2344 [01:22<07:36,  4.34it/s, loss=0.665, v_num=.]\n",
            "Epoch 2:  16%|█▌        | 380/2344 [01:27<07:32,  4.34it/s, loss=0.706, v_num=.]\n",
            "Epoch 2:  17%|█▋        | 400/2344 [01:32<07:27,  4.34it/s, loss=0.748, v_num=.]\n",
            "Epoch 2:  18%|█▊        | 420/2344 [01:36<07:23,  4.34it/s, loss=0.684, v_num=.]\n",
            "Epoch 2:  19%|█▉        | 440/2344 [01:41<07:18,  4.34it/s, loss=0.605, v_num=.]\n",
            "Epoch 2:  20%|█▉        | 460/2344 [01:45<07:14,  4.34it/s, loss=0.638, v_num=.]\n",
            "Epoch 2:  20%|██        | 480/2344 [01:50<07:09,  4.33it/s, loss=0.681, v_num=.]\n",
            "Epoch 2:  21%|██▏       | 500/2344 [01:55<07:05,  4.33it/s, loss=0.651, v_num=.]\n",
            "Epoch 2:  22%|██▏       | 520/2344 [01:59<07:00,  4.33it/s, loss=0.641, v_num=.]\n",
            "Epoch 2:  23%|██▎       | 540/2344 [02:04<06:55,  4.34it/s, loss=0.619, v_num=.]\n",
            "Epoch 2:  24%|██▍       | 560/2344 [02:08<06:50,  4.34it/s, loss=0.636, v_num=.]\n",
            "Epoch 2:  25%|██▍       | 580/2344 [02:13<06:46,  4.34it/s, loss=0.692, v_num=.]\n",
            "Epoch 2:  26%|██▌       | 600/2344 [02:18<06:41,  4.34it/s, loss=0.631, v_num=.]\n",
            "Epoch 2:  26%|██▋       | 620/2344 [02:22<06:36,  4.35it/s, loss=0.62, v_num=.] \n",
            "Epoch 2:  27%|██▋       | 640/2344 [02:27<06:31,  4.35it/s, loss=0.671, v_num=.]\n",
            "Epoch 2:  28%|██▊       | 660/2344 [02:31<06:26,  4.36it/s, loss=0.671, v_num=.]\n",
            "Epoch 2:  28%|██▊       | 660/2344 [02:31<06:26,  4.36it/s, loss=0.656, v_num=.]\n",
            "Epoch 2:  29%|██▉       | 680/2344 [02:36<06:21,  4.36it/s, loss=0.637, v_num=.]\n",
            "Epoch 2:  30%|██▉       | 700/2344 [02:40<06:16,  4.36it/s, loss=0.685, v_num=.]\n",
            "Epoch 2:  31%|███       | 720/2344 [02:44<06:11,  4.37it/s, loss=0.678, v_num=.]\n",
            "Epoch 2:  32%|███▏      | 740/2344 [02:49<06:07,  4.37it/s, loss=0.627, v_num=.]\n",
            "Epoch 2:  32%|███▏      | 760/2344 [02:54<06:02,  4.37it/s, loss=0.688, v_num=.]\n",
            "Epoch 2:  33%|███▎      | 780/2344 [02:58<05:58,  4.37it/s, loss=0.688, v_num=.]\n",
            "Epoch 2:  33%|███▎      | 780/2344 [02:58<05:58,  4.37it/s, loss=0.649, v_num=.]\n",
            "Epoch 2:  34%|███▍      | 800/2344 [03:03<05:53,  4.37it/s, loss=0.662, v_num=.]\n",
            "Epoch 2:  35%|███▍      | 820/2344 [03:07<05:48,  4.37it/s, loss=0.665, v_num=.]\n",
            "Epoch 2:  36%|███▌      | 840/2344 [03:12<05:43,  4.37it/s, loss=0.653, v_num=.]\n",
            "Epoch 2:  37%|███▋      | 860/2344 [03:16<05:39,  4.38it/s, loss=0.691, v_num=.]\n",
            "Epoch 2:  38%|███▊      | 880/2344 [03:20<05:34,  4.38it/s, loss=0.631, v_num=.]\n",
            "Epoch 2:  38%|███▊      | 900/2344 [03:25<05:29,  4.38it/s, loss=0.578, v_num=.]\n",
            "Epoch 2:  39%|███▉      | 920/2344 [03:29<05:24,  4.38it/s, loss=0.676, v_num=.]\n",
            "Epoch 2:  40%|████      | 940/2344 [03:34<05:20,  4.38it/s, loss=0.684, v_num=.]\n",
            "Epoch 2:  41%|████      | 960/2344 [03:38<05:15,  4.39it/s, loss=0.647, v_num=.]\n",
            "Epoch 2:  42%|████▏     | 980/2344 [03:43<05:10,  4.39it/s, loss=0.672, v_num=.]\n",
            "Epoch 2:  43%|████▎     | 1000/2344 [03:47<05:05,  4.39it/s, loss=0.589, v_num=.]\n",
            "Epoch 2:  44%|████▎     | 1020/2344 [03:52<05:01,  4.40it/s, loss=0.662, v_num=.]\n",
            "Epoch 2:  44%|████▍     | 1040/2344 [03:56<04:56,  4.40it/s, loss=0.605, v_num=.]\n",
            "Epoch 2:  45%|████▌     | 1060/2344 [04:00<04:51,  4.40it/s, loss=0.615, v_num=.]\n",
            "Epoch 2:  46%|████▌     | 1080/2344 [04:05<04:47,  4.40it/s, loss=0.586, v_num=.]\n",
            "Epoch 2:  47%|████▋     | 1100/2344 [04:09<04:42,  4.40it/s, loss=0.589, v_num=.]\n",
            "Epoch 2:  48%|████▊     | 1120/2344 [04:14<04:38,  4.40it/s, loss=0.58, v_num=.] \n",
            "Epoch 2:  49%|████▊     | 1140/2344 [04:18<04:33,  4.41it/s, loss=0.651, v_num=.]\n",
            "Epoch 2:  49%|████▉     | 1160/2344 [04:23<04:28,  4.41it/s, loss=0.614, v_num=.]\n",
            "Epoch 2:  50%|█████     | 1180/2344 [04:27<04:24,  4.41it/s, loss=0.538, v_num=.]\n",
            "Epoch 2:  51%|█████     | 1200/2344 [04:32<04:19,  4.41it/s, loss=0.617, v_num=.]\n",
            "Epoch 2:  52%|█████▏    | 1220/2344 [04:36<04:14,  4.41it/s, loss=0.54, v_num=.] \n",
            "Epoch 2:  53%|█████▎    | 1240/2344 [04:40<04:10,  4.42it/s, loss=0.599, v_num=.]\n",
            "Epoch 2:  54%|█████▍    | 1260/2344 [04:45<04:05,  4.42it/s, loss=0.573, v_num=.]\n",
            "Epoch 2:  55%|█████▍    | 1280/2344 [04:49<04:00,  4.42it/s, loss=0.593, v_num=.]\n",
            "Epoch 2:  55%|█████▌    | 1300/2344 [04:54<03:56,  4.42it/s, loss=0.624, v_num=.]\n",
            "Epoch 2:  56%|█████▋    | 1320/2344 [04:58<03:51,  4.42it/s, loss=0.644, v_num=.]\n",
            "Epoch 2:  57%|█████▋    | 1340/2344 [05:03<03:47,  4.42it/s, loss=0.599, v_num=.]\n",
            "Epoch 2:  58%|█████▊    | 1360/2344 [05:07<03:42,  4.42it/s, loss=0.636, v_num=.]\n",
            "Epoch 2:  59%|█████▉    | 1380/2344 [05:12<03:38,  4.42it/s, loss=0.602, v_num=.]\n",
            "Epoch 2:  60%|█████▉    | 1400/2344 [05:16<03:33,  4.42it/s, loss=0.592, v_num=.]\n",
            "Epoch 2:  61%|██████    | 1420/2344 [05:21<03:28,  4.42it/s, loss=0.576, v_num=.]\n",
            "Epoch 2:  61%|██████▏   | 1440/2344 [05:25<03:24,  4.43it/s, loss=0.552, v_num=.]\n",
            "Epoch 2:  62%|██████▏   | 1460/2344 [05:29<03:19,  4.43it/s, loss=0.609, v_num=.]\n",
            "Epoch 2:  63%|██████▎   | 1480/2344 [05:34<03:15,  4.43it/s, loss=0.603, v_num=.]\n",
            "Epoch 2:  64%|██████▍   | 1500/2344 [05:38<03:10,  4.43it/s, loss=0.538, v_num=.]\n",
            "Epoch 2:  65%|██████▍   | 1520/2344 [05:42<03:05,  4.43it/s, loss=0.648, v_num=.]\n",
            "Epoch 2:  66%|██████▌   | 1540/2344 [05:47<03:01,  4.43it/s, loss=0.52, v_num=.] \n",
            "Epoch 2:  67%|██████▋   | 1560/2344 [05:51<02:56,  4.43it/s, loss=0.589, v_num=.]\n",
            "Epoch 2:  67%|██████▋   | 1580/2344 [05:56<02:52,  4.44it/s, loss=0.55, v_num=.] \n",
            "Epoch 2:  68%|██████▊   | 1600/2344 [06:00<02:47,  4.44it/s, loss=0.577, v_num=.]\n",
            "Epoch 2:  69%|██████▉   | 1620/2344 [06:05<02:43,  4.44it/s, loss=0.56, v_num=.] \n",
            "Epoch 2:  70%|██████▉   | 1640/2344 [06:09<02:38,  4.44it/s, loss=0.541, v_num=.]\n",
            "Epoch 2:  71%|███████   | 1660/2344 [06:13<02:34,  4.44it/s, loss=0.572, v_num=.]\n",
            "Epoch 2:  72%|███████▏  | 1680/2344 [06:18<02:29,  4.44it/s, loss=0.579, v_num=.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "DGJC9YVIMHBR",
        "outputId": "263e5898-f0d0-4276-e096-31b5077183b6"
      },
      "source": [
        "# # create a ZipFile object\n",
        "# with ZipFile('/content/sampleDir.zip', 'w') as zipObj:\n",
        "#    # Iterate over all the files in directory\n",
        "#    for folderName, subfolders, filenames in os.walk('drive/MyDrive/Logs/LensResNet_J'):\n",
        "#        for filename in filenames:\n",
        "#            #create complete filepath of file in directory\n",
        "#            filePath = os.path.join(folderName, filename)\n",
        "#            # Add file to zip\n",
        "#            zipObj.write(filePath, basename(filePath))\n",
        "shutil.make_archive('/content/folder', 'zip', 'drive/MyDrive/Logs/LensResNet_J')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/folder.zip'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QB79ExWZ74KH",
        "outputId": "ca8d196b-eb69-4d98-c176-5a662e8b6e1a"
      },
      "source": [
        "!tensorboard dev delete --experiment_id vbghQrP0RWWfyBY3KRip0g\n",
        "# upload --logdir ./drive/MyDrive/Logs/ \\\n",
        "# --name \"My experiments\" \\\n",
        "# # --description \"Lightning_StackGAN\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-07-07 22:48:05.538117: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "Data for the \"text\" plugin is now uploaded to TensorBoard.dev! Note that uploaded data is public. If you do not want to upload data for this plugin, use the \"--plugins\" command line argument.\n",
            "Cannot delete experiment vbghQrP0RWWfyBY3KRip0g because it is owned by a different user.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gM91irMo1uUR"
      },
      "source": [
        "# StackGAN:\n",
        "Here we define the GAN module, that we shall use to generate representative images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMwzBib_zulo"
      },
      "source": [
        "class Generator2(nn.Module):\n",
        "    def __init__(self, image_channels: int = 1, ngf: int = 128,\n",
        "                 ker: int = 4, strd: int = 2,\n",
        "                 res_ker: int = 3, res_strd: int = 1, res_pad: int = 1):\n",
        "        super().__init__()\n",
        "\n",
        "        pad = int((ker - 2)/2)\n",
        "        # 64 -> 32\n",
        "        self.preprocessing = nn.Sequential(\n",
        "            nn.Conv2d(image_channels, ngf, ker, strd, pad, bias=False),\n",
        "            nn.ReLU(True)\n",
        "        )\n",
        "        # residuals\n",
        "        self.residual = nn.Sequential(\n",
        "            BasicBlock(ngf, ngf),\n",
        "            BasicBlock(ngf, ngf),\n",
        "            BasicBlock(ngf, ngf),\n",
        "            BasicBlock(ngf, ngf),\n",
        "            BasicBlock(ngf, ngf),\n",
        "            BasicBlock(ngf, ngf),\n",
        "        )\n",
        "        self.ending_residual = nn.Sequential(\n",
        "            nn.Conv2d(ngf, ngf, res_ker, res_strd, res_pad, bias=False),\n",
        "            nn.BatchNorm2d(ngf),\n",
        "            nn.ReLU(True)\n",
        "        )\n",
        "\n",
        "        # at this part, add the residual inputs from after the preprocessing\n",
        "\n",
        "        image_width = 150 # upscaling should be factor of 2 increase\n",
        "        mode = 'nearest' # upscaling method is nearest-neighbour\n",
        "        self.main = nn.Sequential(\n",
        "            # 32 -> 64\n",
        "            nn.Upsample(image_width//2, mode=mode),\n",
        "            nn.Conv2d(ngf, ngf*4, res_ker, res_strd, res_pad, bias=False),\n",
        "            nn.BatchNorm2d(ngf*4),\n",
        "            nn.ReLU(True),\n",
        "            # 64 -> 128\n",
        "            nn.Upsample(image_width, mode=mode),\n",
        "            nn.Conv2d(ngf*4, image_channels, res_ker, res_strd, res_pad, bias=False),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, in_x):\n",
        "        x_p = self.preprocessing(in_x)\n",
        "        x_r = x_p\n",
        "        x_r = self.residual(x_r)\n",
        "        x_r = self.ending_residual(x_r)\n",
        "        # large residual connections\n",
        "        x_f = x_r + x_p\n",
        "        return self.main(x_f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abXKx0xitvoK"
      },
      "source": [
        "class StackGAN(pl.LightningModule):\n",
        "    def __init__(self, config, noise_size: int = 100, image_width = 64,\n",
        "                    num_classes: int = 3, image_channels: int = 1, b1: float = 0.5, **kwargs):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters(ignore = config)\n",
        "        self.feature_maps = config['feature_maps']\n",
        "        self.lr = config['learning_rate']\n",
        "        # -------------------------------------\n",
        "        # Need to create a subclass because we couldn't simply add/remove a layer;\n",
        "        # there are two inputs of the superclas' forward method.\n",
        "        self.G1 = DCGANGenerator(self.hparams.noise_size, self.feature_maps, self.hparams.image_channels).apply(self._weights_init)\n",
        "        l = list(self.G1.gen[0])\n",
        "        del l[1]\n",
        "        self.G1.gen[0] = nn.Sequential(*l)\n",
        "        self.G1.add_module('label_emb', nn.Embedding(self.hparams.num_classes, self.hparams.noise_size))\n",
        "        # ------------------------------------\n",
        "        self.D1 = DCGANDiscriminator(self.feature_maps, self.hparams.image_channels).apply(self._weights_init)\n",
        "        # -------------------------------------\n",
        "        self.G2 = Generator2(self.hparams.image_channels, self.feature_maps).apply(self._weights_init)\n",
        "        # -------------------------------------\n",
        "        self.D2 = DCGANDiscriminator(self.feature_maps, self.hparams.image_channels)\n",
        "        #  steps to mutate the instance, not the class definition\n",
        "        extra = self.D2._make_disc_block(self.feature_maps * 2, self.feature_maps * 2)\n",
        "        l = list(self.D2.disc)\n",
        "        l.insert(2, extra)\n",
        "        self.D2.disc = nn.Sequential(*l)\n",
        "        self.D2.apply(self._weights_init)\n",
        "        # No need for subclassing as the forward method need not be modified.\n",
        "        # -------------------------------------\n",
        "        self.R = LensResnet(config, num_classes = 4).apply(self._weights_init)\n",
        "        # -------------------------------------\n",
        "        self.pretrained = LensResnet(config)\n",
        "        ckpt = pl_load(os.path.join(\n",
        "            '/content/drive/MyDrive/Logs/tune_LensResnet_asha_model_j/train_LensResnet_tune_checkpoint_e38cb_00000_0_batch_size=128,learning_rate=0.001_2021-07-06_17-52-11/checkpoint_epoch=17-step=1406',\n",
        "            # '/content/drive/MyDrive/Logs/tune_LensResnet_asha_model_f/train_LensResnet_tune_checkpoint_e32ba_00000_0_batch_size=64,learning_rate=0.0001_2021-07-06_03-33-10/checkpoint_epoch=14-step=4689',\n",
        "            \"checkpoint\"),\n",
        "            map_location=lambda storage, loc: storage)\n",
        "        self.pretrained._load_model_state(ckpt)\n",
        "        # -------------------------------------\n",
        "        self.criterion1 = nn.BCELoss()\n",
        "        self.criterion2 = nn.CrossEntropyLoss()\n",
        "\n",
        "    @staticmethod\n",
        "    def _weights_init(m):\n",
        "        classname = m.__class__.__name__\n",
        "        if classname.find(\"Conv\") != -1:\n",
        "            torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
        "        elif classname.find(\"BatchNorm\") != -1:\n",
        "            torch.nn.init.normal_(m.weight, 1.0, 0.02)\n",
        "            torch.nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, noise, labels = None):\n",
        "        if labels is None:\n",
        "            labels = torch.randint(*noise.shape[:-1])                           # last dimension is the hidden dimension\n",
        "        inp = torch.mul(noise, self.G1.label_emb(labels))\n",
        "        out1 = self.G1(inp.view(-1, inp.shape[-1], 1, 1))\n",
        "        out2 = self.G2(out1.detach())\n",
        "        return out2, out1\n",
        "\n",
        "    def training_step(self, batch, batch_idx, optimizer_idx):\n",
        "        imgs, labels = batch\n",
        "        temp2, temp1 = self(torch.randn(labels.shape[0], self.hparams.noise_size).type_as(imgs), labels)\n",
        "\n",
        "        if optimizer_idx == 0:\n",
        "            loss = self.criterion1(self.D1(temp1), torch.ones_like(labels, dtype=torch.float32))\n",
        "            self.log('G1/train/loss/disc', loss)\n",
        "            loss.add_(self.criterion2(self.R.backbone(self.G2(temp1)), labels))\n",
        "            self.log('G1/train/loss/full', loss)\n",
        "\n",
        "        elif optimizer_idx == 1:\n",
        "            real, fake = self.D1(F.interpolate(imgs, self.hparams.image_width, mode='nearest')), self.D1(temp1.detach())\n",
        "            prediction, target = torch.cat((real, fake)), torch.cat((torch.ones_like(real),torch.zeros_like(fake)))\n",
        "            loss = self.criterion1(prediction, target)\n",
        "            self.log('D1/train/loss', loss)\n",
        "\n",
        "        elif optimizer_idx == 2:\n",
        "            loss = self.criterion1(self.D2(temp2), torch.ones_like(labels, dtype=torch.float32))\n",
        "            self.log('G2/train/loss/disc', loss)\n",
        "            loss.add_(self.criterion2(self.R.backbone(temp2), labels))\n",
        "            self.log('G2/train/loss/full', loss)\n",
        "\n",
        "        elif optimizer_idx == 3:\n",
        "            real, fake = self.D2(imgs), self.D2(temp2.detach())\n",
        "            prediction, target = torch.cat((real, fake)), torch.cat((torch.ones_like(real),torch.zeros_like(fake)))\n",
        "            loss = self.criterion1(prediction, target)\n",
        "            self.log('D2/train/loss', loss)\n",
        "\n",
        "        elif optimizer_idx == 4:\n",
        "            real, fake = self.R.backbone(imgs), self.R.backbone(temp2.detach())\n",
        "            prediction, target = torch.cat((real, fake)), torch.cat((labels, self.hparams.num_classes * torch.ones_like(labels)))\n",
        "            loss = self.criterion2(prediction, target)\n",
        "            self.log('R/train/loss', loss)\n",
        "        \n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        opt_g1 = torch.optim.Adam(self.G1.parameters(), self.lr, (self.hparams.b1, 0.999))\n",
        "        opt_d1 = torch.optim.Adam(self.D1.parameters(), self.lr, (self.hparams.b1, 0.999))\n",
        "        opt_g2 = torch.optim.Adam(self.G2.parameters(), self.lr, (self.hparams.b1, 0.999))\n",
        "        opt_d2 = torch.optim.Adam(self.D2.parameters(), self.lr, (self.hparams.b1, 0.999))\n",
        "        opt_r = torch.optim.Adam(self.R.parameters(), self.lr, (self.hparams.b1, 0.999))\n",
        "        return opt_g1, opt_d1, opt_g2, opt_d2, opt_r\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        imgs, labels = batch\n",
        "        temp2, _ = self(torch.randn(labels.shape[0], self.hparams.noise_size).type_as(imgs), labels)\n",
        "        return {'pred': self.pretrained(temp2.detach()), 'target': labels}\n",
        "\n",
        "    def validation_epoch_end(self, listofDicts):\n",
        "        prediction, target = torch.cat([x[\"pred\"] for x in listofDicts]), torch.cat([x[\"target\"] for x in listofDicts])\n",
        "        aurocTensor = tm.functional.auroc(prediction, target, num_classes=self.hparams.num_classes, average=None)\n",
        "        self.log('Pre/val/auroc', aurocTensor.min())\n",
        "        fprList, tprList, _ = tm.functional.roc(prediction, target, num_classes=self.hparams.num_classes)\n",
        "        \n",
        "        f = plt.figure()\n",
        "        colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n",
        "        for i, color in zip(range(self.hparams.num_classes), colors):\n",
        "            plt.plot(fprList[i].cpu(), tprList[i].cpu(), color=color,\n",
        "                    label='ROC curve of class {0} (area = {1:0.2f})'\n",
        "                    ''.format(i, aurocTensor[i].cpu()))\n",
        "        plt.plot([0, 1], [0, 1], 'k--')\n",
        "        plt.xlim([0.0, 1.0])\n",
        "        plt.ylim([0.0, 1.05])\n",
        "        plt.xlabel('False Positive Rate')\n",
        "        plt.ylabel('True Positive Rate')\n",
        "        plt.title('Multi-class ROC')\n",
        "        plt.legend(loc=\"lower right\")\n",
        "\n",
        "        self.logger.experiment.add_figure('StackGAN/val/ROC', f)\n",
        "        f.savefig(str(tune.get_trial_dir())+'ROC_epoch_'+str(self.current_epoch)+'.pdf')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfPgHHMdNOzQ"
      },
      "source": [
        "# Tune StackGAN:\n",
        "Here we tune hyperparameters for generating images that resemble the images from input."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "EdR_s_6zGtp4",
        "outputId": "c82d2867-cd16-4072-9840-c9cd84fbd6cd"
      },
      "source": [
        "# __tune_train_checkpoint_begin\n",
        "def train_StackGAN_tune_checkpoint(config,\n",
        "                                   checkpoint_dir=None,\n",
        "                                   num_epochs=10,\n",
        "                                   num_gpus=1):\n",
        "    data_dir = os.path.expanduser(\"/content/images/\")\n",
        "    trainer = pl.Trainer(\n",
        "        # accumulate_grad_batches=2,\n",
        "        # limit_train_batches=0.20,\n",
        "        # limit_val_batches=0.20,\n",
        "        num_sanity_val_steps=-1,\n",
        "        max_epochs=num_epochs,\n",
        "        prepare_data_per_node = False,\n",
        "        # If fractional GPUs passed in, convert to int.\n",
        "        gpus=math.ceil(num_gpus),\n",
        "        # tpu_cores = 8,\n",
        "        logger=TensorBoardLogger(save_dir=tune.get_trial_dir(), name=\"\", version=\".\"),\n",
        "        # progress_bar_refresh_rate=1,\n",
        "        callbacks=[\n",
        "            TuneReportCheckpointCallback(\n",
        "                metrics={\n",
        "                    \"lossG1\": \"G1/train/loss/full\",\n",
        "                    \"lossG2\": \"G2/train/loss/full\",\n",
        "                    \"lossD1\": \"D1/train/loss\",\n",
        "                    \"lossD2\": \"D2/train/loss\",\n",
        "                    \"lossR\": \"R/train/loss\",\n",
        "                    \"auroc\": \"Pre/val/auroc\",\n",
        "                },\n",
        "                filename=\"checkpoint\",\n",
        "                # on=\"training_end\"\n",
        "            )\n",
        "        ],\n",
        "        # stochastic_weight_avg=True,\n",
        "        # works with only one optimizer\n",
        "        )\n",
        "    dm = NpyDataModule(config, data_dir)\n",
        "    if checkpoint_dir:\n",
        "        # Currently, this leads to errors:\n",
        "        # model = StackGAN.load_from_checkpoint(\n",
        "        #     os.path.join(checkpoint, \"checkpoint\"))\n",
        "        # Workaround:\n",
        "        ckpt = pl_load(\n",
        "            os.path.join(checkpoint_dir, \"checkpoint\"),\n",
        "            map_location=lambda storage, loc: storage)\n",
        "        model = StackGAN._load_model_state(\n",
        "            ckpt, config=config, \n",
        "            # data_dir=data_dir\n",
        "            )\n",
        "        trainer.current_epoch = ckpt[\"epoch\"]\n",
        "    else:\n",
        "        model = StackGAN(config)\n",
        "\n",
        "    trainer.fit(model, dm)\n",
        "# __tune_train_checkpoint_end__\n",
        "\n",
        "\n",
        "# __tune_asha_begin__\n",
        "def tune_StackGAN_asha(num_samples=10, num_epochs=10, gpus_per_trial=1):\n",
        "    config = {\n",
        "        \"learning_rate\": tune.choice([1e-4]),\n",
        "        \"feature_maps\": tune.choice([64]),\n",
        "        \"batch_size\": tune.choice([128, 64]),\n",
        "    }\n",
        "\n",
        "    scheduler = ASHAScheduler(\n",
        "        max_t=num_epochs,\n",
        "        grace_period=1,\n",
        "        reduction_factor=2)\n",
        "\n",
        "    reporter = CLIReporter(\n",
        "        # overwrite=True,\n",
        "        parameter_columns=[\"learning_rate\", \"feature_maps\", \"batch_size\"],\n",
        "        metric_columns=[\"lossG1\", \"lossG2\", \"lossD1\", \"lossD2\", \"lossR\", \"auroc\", \"training_iteration\"],\n",
        "        )\n",
        "\n",
        "    analysis = tune.run(\n",
        "        tune.with_parameters(\n",
        "            train_StackGAN_tune_checkpoint,\n",
        "            num_epochs=num_epochs,\n",
        "            num_gpus=gpus_per_trial),\n",
        "        name=\"tune_StackGAN_asha_model_j\",\n",
        "        metric=\"auroc\",\n",
        "        mode=\"max\",\n",
        "        config=config,\n",
        "        resources_per_trial={\n",
        "            \"cpu\": 2,\n",
        "            \"gpu\": gpus_per_trial,\n",
        "            # \"tpu\": 8,\n",
        "        },\n",
        "        num_samples=num_samples,\n",
        "        local_dir='./drive/MyDrive/Logs',\n",
        "        scheduler=scheduler,\n",
        "        progress_reporter=reporter,\n",
        "        # restore='/content/drive/MyDrive/Logs/tune_StackGAN_1_asha_model_j/train_StackGAN_tune_checkpoint_fa25b_00000_0_batch_size=64,feature_maps=64,learning_rate=0.0001_2021-07-06_20-23-13/checkpoint_epoch=0-step=937',\n",
        "        fail_fast = True,\n",
        "        resume='PROMPT',\n",
        "        )\n",
        "\n",
        "    print(\"Best hyperparameters found were: \", analysis.best_config)\n",
        "\n",
        "# __tune_asha_end__\n",
        "\n",
        "\n",
        "# __tune_pbt_begin__\n",
        "def tune_StackGAN_pbt(num_samples=10, num_epochs=10, gpus_per_trial=1):\n",
        "    config = {\n",
        "        \"learning_rate\": 1e-4,\n",
        "        \"feature_maps\": 64,\n",
        "        \"batch_size\": 64,\n",
        "    }\n",
        "\n",
        "    scheduler = PopulationBasedTraining(\n",
        "        perturbation_interval=4,\n",
        "        hyperparam_mutations={\n",
        "            \"learning_rate\": [1e-4, 1e-3],\n",
        "            \"feature_maps\": [64, 128],\n",
        "            \"batch_size\": [32, 64, 128]\n",
        "        })\n",
        "\n",
        "    reporter = CLIReporter(\n",
        "        # overwrite=True,\n",
        "        parameter_columns=[\"learning_rate\", \"feature_maps\", \"batch_size\"],\n",
        "        metric_columns=[\"lossG1\", \"lossG2\", \"lossD1\", \"lossD2\", \"lossR\", \"auroc\", \"training_iteration\"],\n",
        "        )\n",
        "\n",
        "    analysis = tune.run(\n",
        "        # resume=True,\n",
        "        tune.with_parameters(\n",
        "            train_StackGAN_tune_checkpoint,\n",
        "            num_epochs=num_epochs,\n",
        "            num_gpus=gpus_per_trial),\n",
        "        name=\"tune_StackGAN_pbt_model_j\",\n",
        "        metric=\"auroc\",\n",
        "        mode=\"max\",\n",
        "        resources_per_trial={\n",
        "            \"cpu\": 2,\n",
        "            \"gpu\": gpus_per_trial,\n",
        "            # \"tpu\": 8,\n",
        "        },\n",
        "        config=config,\n",
        "        num_samples=num_samples,\n",
        "        scheduler=scheduler,\n",
        "        progress_reporter=reporter,\n",
        "        local_dir='./drive/MyDrive/Logs',\n",
        "        # restore='/content/drive/MyDrive/Logs/tune_StackGAN_1_asha_model_j/train_StackGAN_tune_checkpoint_fa25b_00000_0_batch_size=64,feature_maps=64,learning_rate=0.0001_2021-07-06_20-23-13/checkpoint_epoch=0-step=937',\n",
        "        fail_fast = True,\n",
        "        # resume='PROMPT',\n",
        "        )\n",
        "\n",
        "    print(\"Best hyperparameters found were: \", analysis.best_config)\n",
        "\n",
        "# __tune_pbt_end__\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import argparse\n",
        "\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\n",
        "        \"--smoke-test\", action=\"store_true\", help=\"Finish quickly for testing\")\n",
        "    args, _ = parser.parse_known_args()\n",
        "\n",
        "    if args.smoke_test:\n",
        "        tune_StackGAN_asha(num_samples=1, num_epochs=6, gpus_per_trial=1)\n",
        "        tune_StackGAN_pbt(num_samples=1, num_epochs=6, gpus_per_trial=1)\n",
        "    else:\n",
        "        # ASHA scheduler\n",
        "        tune_StackGAN_asha(num_samples=2, num_epochs=1, gpus_per_trial=1)\n",
        "        # Population based training\n",
        "        # tune_StackGAN_pbt(num_samples=8, num_epochs=5, gpus_per_trial=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Resume from local directory? [y/N]: y\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2021-07-07 15:06:21,224\tWARNING trial_runner.py:449 -- Attempting to resume experiment from /content/drive/MyDrive/Logs/tune_StackGAN_asha_model_j. This will ignore any new changes to the specification.\n",
            "2021-07-07 15:06:21,237\tINFO tune.py:467 -- TrialRunner resumed, ignoring new add_experiment.\n",
            "\u001b[2m\u001b[36m(pid=2877)\u001b[0m 2021-07-07 15:06:21,249\tERROR worker.py:418 -- SystemExit was raised from the worker\n",
            "\u001b[2m\u001b[36m(pid=2877)\u001b[0m Traceback (most recent call last):\n",
            "\u001b[2m\u001b[36m(pid=2877)\u001b[0m   File \"python/ray/_raylet.pyx\", line 595, in ray._raylet.task_execution_handler\n",
            "\u001b[2m\u001b[36m(pid=2877)\u001b[0m   File \"python/ray/_raylet.pyx\", line 453, in ray._raylet.execute_task\n",
            "\u001b[2m\u001b[36m(pid=2877)\u001b[0m   File \"python/ray/_raylet.pyx\", line 490, in ray._raylet.execute_task\n",
            "\u001b[2m\u001b[36m(pid=2877)\u001b[0m   File \"python/ray/_raylet.pyx\", line 497, in ray._raylet.execute_task\n",
            "\u001b[2m\u001b[36m(pid=2877)\u001b[0m   File \"python/ray/_raylet.pyx\", line 501, in ray._raylet.execute_task\n",
            "\u001b[2m\u001b[36m(pid=2877)\u001b[0m   File \"python/ray/_raylet.pyx\", line 451, in ray._raylet.execute_task.function_executor\n",
            "\u001b[2m\u001b[36m(pid=2877)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/ray/_private/function_manager.py\", line 563, in actor_method_executor\n",
            "\u001b[2m\u001b[36m(pid=2877)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
            "\u001b[2m\u001b[36m(pid=2877)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/ray/tune/trainable.py\", line 173, in train_buffered\n",
            "\u001b[2m\u001b[36m(pid=2877)\u001b[0m     result = self.train()\n",
            "\u001b[2m\u001b[36m(pid=2877)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/ray/tune/trainable.py\", line 232, in train\n",
            "\u001b[2m\u001b[36m(pid=2877)\u001b[0m     result = self.step()\n",
            "\u001b[2m\u001b[36m(pid=2877)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/ray/tune/function_runner.py\", line 349, in step\n",
            "\u001b[2m\u001b[36m(pid=2877)\u001b[0m     block=True, timeout=RESULT_FETCH_TIMEOUT)\n",
            "\u001b[2m\u001b[36m(pid=2877)\u001b[0m   File \"/usr/lib/python3.7/queue.py\", line 179, in get\n",
            "\u001b[2m\u001b[36m(pid=2877)\u001b[0m     self.not_empty.wait(remaining)\n",
            "\u001b[2m\u001b[36m(pid=2877)\u001b[0m   File \"/usr/lib/python3.7/threading.py\", line 300, in wait\n",
            "\u001b[2m\u001b[36m(pid=2877)\u001b[0m     gotit = waiter.acquire(True, timeout)\n",
            "\u001b[2m\u001b[36m(pid=2877)\u001b[0m   File \"/usr/local/lib/python3.7/dist-packages/ray/worker.py\", line 415, in sigterm_handler\n",
            "\u001b[2m\u001b[36m(pid=2877)\u001b[0m     sys.exit(1)\n",
            "\u001b[2m\u001b[36m(pid=2877)\u001b[0m SystemExit: 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "== Status ==\n",
            "Memory usage on this node: 3.2/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 1.000: None\n",
            "Resources requested: 0/2 CPUs, 0/1 GPUs, 0.0/7.54 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 GPU_group_989fc9f1de0f0c7a947b8c51120efc30, 0.0/2.0 CPU_group_989fc9f1de0f0c7a947b8c51120efc30, 0.0/2.0 CPU_group_0_989fc9f1de0f0c7a947b8c51120efc30, 0.0/1.0 accelerator_type:T4, 0.0/1.0 GPU_group_0_989fc9f1de0f0c7a947b8c51120efc30)\n",
            "Result logdir: /content/drive/MyDrive/Logs/tune_StackGAN_asha_model_j\n",
            "Number of trials: 2/2 (2 PENDING)\n",
            "+--------------------------------------------+----------+-------+-----------------+----------------+--------------+\n",
            "| Trial name                                 | status   | loc   |   learning_rate |   feature_maps |   batch_size |\n",
            "|--------------------------------------------+----------+-------+-----------------+----------------+--------------|\n",
            "| train_StackGAN_tune_checkpoint_1f244_00000 | PENDING  |       |          0.0001 |             64 |          128 |\n",
            "| train_StackGAN_tune_checkpoint_1f244_00001 | PENDING  |       |          0.0001 |             64 |           64 |\n",
            "+--------------------------------------------+----------+-------+-----------------+----------------+--------------+\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=2878)\u001b[0m GPU available: True, used: True\n",
            "\u001b[2m\u001b[36m(pid=2878)\u001b[0m TPU available: False, using: 0 TPU cores\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "== Status ==\n",
            "Memory usage on this node: 1.9/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 1.000: None\n",
            "Resources requested: 2.0/2 CPUs, 1.0/1 GPUs, 0.0/7.54 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 GPU_group_989fc9f1de0f0c7a947b8c51120efc30, 0.0/2.0 CPU_group_989fc9f1de0f0c7a947b8c51120efc30, 0.0/2.0 CPU_group_0_989fc9f1de0f0c7a947b8c51120efc30, 0.0/1.0 accelerator_type:T4, 0.0/1.0 GPU_group_0_989fc9f1de0f0c7a947b8c51120efc30)\n",
            "Result logdir: /content/drive/MyDrive/Logs/tune_StackGAN_asha_model_j\n",
            "Number of trials: 2/2 (1 PENDING, 1 RUNNING)\n",
            "+--------------------------------------------+----------+-------+-----------------+----------------+--------------+\n",
            "| Trial name                                 | status   | loc   |   learning_rate |   feature_maps |   batch_size |\n",
            "|--------------------------------------------+----------+-------+-----------------+----------------+--------------|\n",
            "| train_StackGAN_tune_checkpoint_1f244_00000 | RUNNING  |       |          0.0001 |             64 |          128 |\n",
            "| train_StackGAN_tune_checkpoint_1f244_00001 | PENDING  |       |          0.0001 |             64 |           64 |\n",
            "+--------------------------------------------+----------+-------+-----------------+----------------+--------------+\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=2878)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "== Status ==\n",
            "Memory usage on this node: 3.1/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 1.000: None\n",
            "Resources requested: 2.0/2 CPUs, 1.0/1 GPUs, 0.0/7.54 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 GPU_group_11925c938e061af52a49e839fa9f7d2e, 0.0/1.0 accelerator_type:T4, 0.0/2.0 CPU_group_0_11925c938e061af52a49e839fa9f7d2e, 0.0/2.0 CPU_group_11925c938e061af52a49e839fa9f7d2e, 0.0/1.0 GPU_group_0_11925c938e061af52a49e839fa9f7d2e)\n",
            "Result logdir: /content/drive/MyDrive/Logs/tune_StackGAN_asha_model_j\n",
            "Number of trials: 2/2 (1 PENDING, 1 RUNNING)\n",
            "+--------------------------------------------+----------+-------+-----------------+----------------+--------------+\n",
            "| Trial name                                 | status   | loc   |   learning_rate |   feature_maps |   batch_size |\n",
            "|--------------------------------------------+----------+-------+-----------------+----------------+--------------|\n",
            "| train_StackGAN_tune_checkpoint_1f244_00000 | RUNNING  |       |          0.0001 |             64 |          128 |\n",
            "| train_StackGAN_tune_checkpoint_1f244_00001 | PENDING  |       |          0.0001 |             64 |           64 |\n",
            "+--------------------------------------------+----------+-------+-----------------+----------------+--------------+\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=2878)\u001b[0m 2021-07-07 15:06:32.107077: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "\u001b[2m\u001b[36m(pid=2878)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=2878)\u001b[0m   | Name       | Type               | Params\n",
            "\u001b[2m\u001b[36m(pid=2878)\u001b[0m --------------------------------------------------\n",
            "\u001b[2m\u001b[36m(pid=2878)\u001b[0m 0 | G1         | DCGANGenerator     | 3.6 M \n",
            "\u001b[2m\u001b[36m(pid=2878)\u001b[0m 1 | D1         | DCGANDiscriminator | 2.8 M \n",
            "\u001b[2m\u001b[36m(pid=2878)\u001b[0m 2 | G2         | Generator2         | 632 K \n",
            "\u001b[2m\u001b[36m(pid=2878)\u001b[0m 3 | D2         | DCGANDiscriminator | 3.0 M \n",
            "\u001b[2m\u001b[36m(pid=2878)\u001b[0m 4 | R          | LensResnet         | 11.2 M\n",
            "\u001b[2m\u001b[36m(pid=2878)\u001b[0m 5 | pretrained | LensResnet         | 11.2 M\n",
            "\u001b[2m\u001b[36m(pid=2878)\u001b[0m 6 | criterion1 | BCELoss            | 0     \n",
            "\u001b[2m\u001b[36m(pid=2878)\u001b[0m 7 | criterion2 | CrossEntropyLoss   | 0     \n",
            "\u001b[2m\u001b[36m(pid=2878)\u001b[0m --------------------------------------------------\n",
            "\u001b[2m\u001b[36m(pid=2878)\u001b[0m 32.3 M    Trainable params\n",
            "\u001b[2m\u001b[36m(pid=2878)\u001b[0m 0         Non-trainable params\n",
            "\u001b[2m\u001b[36m(pid=2878)\u001b[0m 32.3 M    Total params\n",
            "\u001b[2m\u001b[36m(pid=2878)\u001b[0m 129.359   Total estimated model params size (MB)\n",
            "\u001b[2m\u001b[36m(pid=2878)\u001b[0m /usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/data_loading.py:349: UserWarning: Your val_dataloader has `shuffle=True`, it is best practice to turn this off for val/test/predict dataloaders.\n",
            "\u001b[2m\u001b[36m(pid=2878)\u001b[0m   f'Your {mode}_dataloader has `shuffle=True`, it is best practice to turn'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=2878)\u001b[0m \rValidation sanity check: 0it [00:00, ?it/s]\rValidation sanity check:   0%|          | 0/118 [00:00<?, ?it/s]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=2878)\u001b[0m /usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "\u001b[2m\u001b[36m(pid=2878)\u001b[0m   return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation sanity check:  17%|█▋        | 20/118 [00:11<00:54,  1.80it/s]\n",
            "Validation sanity check:  34%|███▍      | 40/118 [00:22<00:43,  1.80it/s]\n",
            "Validation sanity check:  51%|█████     | 60/118 [00:33<00:32,  1.78it/s]\n",
            "Validation sanity check:  68%|██████▊   | 80/118 [00:45<00:21,  1.75it/s]\n",
            "Validation sanity check:  85%|████████▍ | 100/118 [01:02<00:11,  1.54it/s]\n",
            "Validation sanity check: 100%|██████████| 118/118 [01:17<00:00,  1.42it/s]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=2878)\u001b[0m /usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/deprecated_api.py:152: LightningDeprecationWarning: `Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\n",
            "\u001b[2m\u001b[36m(pid=2878)\u001b[0m   \"`Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=2878)\u001b[0m \r                                                                          \r\rTraining: 0it [00:00, ?it/s]\rTraining:   0%|          | 0/587 [00:00<?, ?it/s]\rEpoch 0:   0%|          | 0/587 [00:00<?, ?it/s] \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2021-07-07 15:08:40,499\tWARNING tune.py:507 -- SIGINT received (e.g. via Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C one more time (or send SIGINT/SIGKILL/SIGTERM) to skip. \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-0baa71c773de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0;31m# ASHA scheduler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m         \u001b[0mtune_StackGAN_asha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpus_per_trial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m         \u001b[0;31m# Population based training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0;31m# tune_StackGAN_pbt(num_samples=8, num_epochs=5, gpus_per_trial=1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-0baa71c773de>\u001b[0m in \u001b[0;36mtune_StackGAN_asha\u001b[0;34m(num_samples, num_epochs, gpus_per_trial)\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;31m# restore='/content/drive/MyDrive/Logs/tune_StackGAN_1_asha_model_j/train_StackGAN_tune_checkpoint_fa25b_00000_0_batch_size=64,feature_maps=64,learning_rate=0.0001_2021-07-06_20-23-13/checkpoint_epoch=0-step=937',\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mfail_fast\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0mresume\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'PROMPT'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m         )\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ray/tune/tune.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(run_or_experiment, name, metric, mode, stop, time_budget_s, config, resources_per_trial, num_samples, local_dir, search_alg, scheduler, keep_checkpoints_num, checkpoint_score_attr, checkpoint_freq, checkpoint_at_end, verbose, progress_reporter, log_to_file, trial_name_creator, trial_dirname_creator, sync_config, export_formats, max_failures, fail_fast, restore, server_port, resume, queue_trials, reuse_actors, trial_executor, raise_on_failed_trial, callbacks, loggers, ray_auto_init, run_errored_only, global_checkpoint_period, with_server, upload_dir, sync_to_cloud, sync_to_driver, sync_on_checkpoint, _remote)\u001b[0m\n\u001b[1;32m    518\u001b[0m     \u001b[0mtune_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_finished\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msignal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSIGINT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 520\u001b[0;31m         \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    521\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhas_verbosity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVerbosity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mV1_EXPERIMENT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m             \u001b[0m_report_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrunner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogress_reporter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ray/tune/trial_runner.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    539\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrial_executor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_staging_grace_period\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m                     \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# blocking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrial_executor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_no_available_trials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ray/tune/trial_runner.py\u001b[0m in \u001b[0;36m_process_events\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    652\u001b[0m             \u001b[0;31m#  fetch_result functionality so that we don't timeout on fetch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m             trial = self.trial_executor.get_next_available_trial(\n\u001b[0;32m--> 654\u001b[0;31m                 timeout=timeout)  # blocking\n\u001b[0m\u001b[1;32m    655\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    656\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ray/tune/ray_trial_executor.py\u001b[0m in \u001b[0;36mget_next_available_trial\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    658\u001b[0m         \u001b[0;31m# See https://github.com/ray-project/ray/issues/4211 for details.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    659\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 660\u001b[0;31m         \u001b[0mready\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshuffled_results\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    661\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    662\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ray/_private/client_mode_hook.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mclient_mode_should_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ray/worker.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_refs, num_returns, timeout, fetch_local)\u001b[0m\n\u001b[1;32m   1635\u001b[0m             \u001b[0mtimeout_milliseconds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1636\u001b[0m             \u001b[0mworker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_task_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1637\u001b[0;31m             \u001b[0mfetch_local\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1638\u001b[0m         )\n\u001b[1;32m   1639\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mready_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremaining_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpython/ray/_raylet.pyx\u001b[0m in \u001b[0;36mray._raylet.CoreWorker.wait\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpython/ray/_raylet.pyx\u001b[0m in \u001b[0;36mray._raylet.check_status\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}